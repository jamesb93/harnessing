<script>
	import NextSection from "$lib/components/NextSection.svelte"
    import {metadata as m} from "./directory.svx"
</script>

# Conclusion

This written text has presented five creative projects that outline the development of my computer-aided compositional practice with content-aware programs over the last four years. For each project, I have documented the compositional process and traced the evolution of technological tools and creative coding practice, as well as the growth of musical ideas specific to those projects. From the tools presented alongside the varying musical outcomes, I hope to have answered the question: *How can I use content-aware programs to compose with digital samples in a studio-based computer-aided compositional practice?*

Two sub-questions emerged from my initial question: *What compositional procedures do I delegate to either human or computer and what is the balance of these two?* and *What procedures do I defer to the computer and how do those procedures work?*. Reflecting on the portfolio works as a collection, I think there is a clear delineation between the first three projects, [Stitch/Strata]({m.ss}), [Annealing Strategies]({m.as}), [Refracted Touch]({m.rt}) and the last two, [Reconstruction Error]({m.re}) and [Interferences]({m.em}) and how they grapple with these questions. With the benefit of hindsight, I consider those first three projects as almost "blind" experimentation and an attempt to consolidate what capabilities of the computer I deemed valuable in computer-aided composition as well as what forms of content-awareness were interesting and useful to me. As a result, each of these initial projects explored a different approach entirely, and imbued the computer with different decision making capabilities without necessarily working towards the consolidation of technological forces in my practice.

[Stitch/Strata]({m.ss}) revealed to me that interacting with the computer by creating automated procedures for organising atomic units of sounds, is incompatible with the level of intervention and control that I want to have over those processes. Furthermore, high-level structures are complex to generate in this manner, because for me they are predicated on the existence of a knowable model that can describe the interaction between multiple, or all formal levels in a composition. My initial goal for [Stitch/Strata]({m.ss}) was to make the computer responsible for many fundamental aspects of the work through procedural generation involving such a model. Frustrated with the lack of satisfying results emerging from this almost complete deference to the computer, I reigned its agency back, forming a workflow in which I was offered solutions to ad-hoc musical questions and tasks. This was transformative, and accelerated the compositional process once I was able to "have my say" with the aid of the computer. Another important facet of this, was the dialogical nature of my interactions with the computer from that point on. These were situated in manoeuvres between listening, coding and responding to the computer's perceptually relevant organisations of the sound materials.

On the other hand, [Annealing Strategies]({m.as}) involved almost complete cognitive off-load of compositional decision making to the computer. I had few opportunities to intervene in neither the generative process undertaken by the simulated annealing algorithm, nor (as a result) the outcome of the music it produced. Instead, my control was communicated through the management of constraints and parameters as well as the curatorial control over which "iteration" of this process would become the final work. This balance of agency was successful to an extent, despite requiring me to give away my ability to intervene almost entirely. However, I did not feel that it was necessarily a generalisable workflow, nor was it one that could be applied to many different future compositional projects.

[Refracted Touch]({m.rt}) explored agent behaviours in terms of machine listening and the ability for the computer to interact and react in real-time inside a set of musical and parametric constraints. In the reflection of this project I outline a number of issues pertaining to my workflow within the paradigm of live electronics, illustrating the complexities of managing improvisation, programming and clarifying my musical intentions at the same time. Giving up agency to the computer in this way offered little in return, and thus I view it as the least successful in this regard in the PhD.

The last two projects in this portfolio are characterised by a much more refined and focused technological approach using [FTIS]({m.ftis}), [ReaCoMa]({m.reacoma}) and the DAW as creative interfaces to the computer. Thus, the technological efforts of those works were invested less so in creating new specific tools and instead were put into incrementally improving an interconnected framework of systems for analysis ([FTIS]({m.ftis})/Python), manipulation ([ReaCoMa]({m.reacoma})) and representation (REAPER).

This combination of technologies influenced the procedures and compositional thinking that I deferred to the computer â€” centring its function on producing organisation of sample-based materials according to perceptual similarity. Through this, the computer evolved to become a co-listener and facilitator of my engagement with these materials and structured different pathways to the solution of ad-hoc compositional questions. This helped higher level musical thinking reach internal consistency through *feedback* and experimentation with the computational outputs.

Rather than being responsible for the generation of certain compositional aspects itself, the form of the works and the organisation of materials within these forms *emerged* through a dialogue between myself and the computer. In this configuration I retained ultimate control over decision making and used the computer as a cognitive resource to influence me. In comparison to projects at the start of this PhD research, such as [Annealing Strategies]({m.as}), the computer became more limited in what it could concretely contribute to the sonic aspects of a composition. In these last two projects it never created fixed sections of the music, nor was it designed to perform low-level musical control. Instead it *suggested* what could be done with materials by proposing them in different relationships, arrangements and structures through machine-listening. Thus, the last two projects embody a human-computer configuration based on the notion of *querying*, as a way of both retrieving answers to compositional questions and helping to clarify and shape new and connected queries.
 
## Contribution To Knowledge

This PhD demonstrates a bespoke workflow using content-aware programs to inform and guide compositional decision making in a computer-aided practice. The focus of the research has been on the mechanisms by which a content-aware machine can function as a heuristic for compositional ideas at various stages of conceptual and aesthetic development. In my engagement with listening machines, I create a dialogue between workflow, creative coding, listening, taste and composition decision making in a computer-aided, studio-based practice.

The primary contribution of this research are my workflows, which are novel in the current landscape of computer-aided composition. Currently there is little research which involves using the DAW alongside bespoke lower-level technologies to support computer-led exploration with sample-based materials. I asserted in [chapter 2, "Preoccupations"]({m.preoc}), that much contemporary computer-aided research is influenced by the models implicit in instrumental score writing. This PhD research diverges from this, and exposes some of the ways in which human-computer authored compositions can be mediated through the hybridisation of listening and machine listening, rather than through generative or procedural strategies.

I also demonstrate how machine learning and listening can be synthesised into a creative practice on a technical level. A large portion of my compositional process is devoted toward building combinations of processes so that the computer can produce representations of corpora and their internal relationships, based on their perceptual similarity. While I am mostly concerned with the discovery of sounds by their commonalities, there is naturally room to explore the opposite of this, as well as the spectrum between. As such, the techniques and combinations of technologies I have employed for sound browsing, searching, analysis and machine learning might present a model by which other practitioners who share similar conceptual and aesthetic goals or desire more specialised ways of curating and selecting sample-based materials can base their own research from.

Elements of my creative coding can be assimilated into other research and creative practice. The tools and technology that I have made are open source and can be hacked and extended by other techno-fluent musicians. [ReaCoMa]({m.reacoma}) in particular has gained much public popularity and use, bridging ways of thinking about computer-led sound decomposition into the approachable and familiar environment of the DAW. Amongst several "high traffic" internet communities, [ReaCoMa]({m.reacoma}) has been received well and its adoption into other artists' set of tools is evident in the questions and discussions spawned by the shared engagement. Such discussions can be found at [this FluCoMa thread](https://discourse.flucoma.org/t/using-flucoma-tools-with-reaper/311/7), [this lines forum discourse thread](https://llllllll.co/t/reacoma-fluid-corpus-manipulation-in-reaper/34500) and on the [REAPER forums too](https://forum.cockos.com/showthread.php?t=239824).

[FTIS]({m.ftis}) also contributes to the wider landscape of computer-aided composition software, offering combined and integrated implementations of machine listening, machine learning and analysis technologies. A major benefit of this unification of technologies is how it allows the outputs of these processes to be readily incorporated into well established paradigms of composition. In this thesis I demonstrate how FTIS can be used to programmatically construct REAPER sessions in response to data, or how its JSON outputs can be consumed  in Max for audition processes. This interoperability of FTIS is novel, and supports merging the complexities of machine listening and learning with my compositional method, as well as potentially for others. Given the open architecture, as well as the low-level scripting and command line interface, FTIS could be embedded in other environments or languages, as well as be extended to incorporate other ways of working and types of meaningful output. Other machine listening algorithms could be added as analysers or additional environmental adapters could be created to facilitate FTIS operating fluently with other software or being embedded into existing practices.

At the time of submitting this thesis, there are several groups of artists and creative coders interested in and researching the artistic affordances of similar technologies presented in my practice. [MIMIC](https://mimicproject.com/about), [FluCoMa](https://www.flucoma.org/) and [AI Music Creativity](https://aimusiccreativity.org/about/) (the joining of [MuMe](https://musicalmetacreation.org/) and CSMC) are some key examples where my research closely aligns with theirs. My writing, code and creative works have been disseminated to this interconnected community of researchers, including published journal articles with the International Computer Music Conference and AI Music Conference, as well as formal presentations at key research events such as the FluCoMa plenaries. This demonstrates the contribution that my research has already made and will continue to make within established institutional bodies.

## Further Research And The Future

Over the course of this PhD developing various technological tools has furnished me with a more sophisticated creative coding practice which is intertwined deeply with my musical thinking and compositional decision making. At the start of the PhD I was limited by my programming abilities to Max. As I became interested in [machine learning]({m.ca}#machine-learning) and [machine listening]({m.ca}#machine-listening), I naturally had to expand my knowledge of other languages, libraries and ecosystems such as Python and [scikit-learn](https://scikit-learn.org/stable/) to meet the demands of my changing and evolving interests. This ultimately gave rise to the necessary skills and conditions in which [FTIS]({m.ftis}) could be built, which draws on several technologies.

In the future developing [FTIS]({m.ftis}) will be essential to the evolution of my overall artistic practice as opposed to the creation of new tools on a project-by-project basis. While the start of this PhD saw several technological shifts between projects, I have now found an extensible framework and methodology for drawing technology into my artistic thinking that does not require me to "reinvent the wheel" in response to new musical ideas and aims. I have already put in place the necessary architectural work allowing [FTIS]({m.ftis}) to be extended programmatically and with new analysers, without having to modify the underlying system and framework itself. This is described in [5.3.3 Architecture]({m.ftis}#architecture). Referring back to the graph of connected compositional actions presented in [2.1.2.3 The Feedback Loop]({m.preoc}#bricolage), it seems restrictive to bound this network of interactions that characterise the human-computer dialogue to individual projects. Moving forward, my creative coding practice will cross pollinate different aesthetic and compositional ideas as projects are undertaken, opening dialogues between pieces and forming a web of interconnected works situated around [FTIS]({m.ftis}).

One development to [FTIS]({m.ftis}) that is not explored in depth in this research is visualisation and visual interfaces, which could potentially allow for novel ways of interacting with processing pipelines and corpora for composition. As it currently stands, the only way to interact with the outputs of [FTIS]({m.ftis}) is to draw them into other environments such as Max or REAPER in order to aurally inspect the results. This has been an effective workflow so far as it allows me to situate my engagement with complex data within the realm of listening and soundful experimentation. That said, I have in some instances visualised data, such as with the UMAP results described in [[4.4.2.4 Dimension Reduction]]({m.re}#dimension-reduction) and this has been a useful process in dealing with the scale and complexity of audio descriptor analysis and machine learning processes. I anticipate that this could be more thoroughly explored, perhaps by creating a "frontend" interface to FTIS which allows both the data and representation of a corpus to be visually portrayed and even operated on through a set of intuitive tactile metaphors such as cutting, drawing or reshaping as a physical material. Such an interface would allow me to physically transform and curate samples from a corpus within the immediate frame of visual feedback, allowing for a more rapid and intuitive response to the outputs of machine learning and listening. 

In addition to this, I anticipate that a frontend interface could support more rapid transportation of computer outputs between different creative coding and composition environments. Instead of having to rely on scripts, such as when I would import cluster data into REAPER, I imagine a visual workflow based on rapidly moving data between FTIS and REAPER for example. A potential innovation could be to have FTIS automatically update a REAPER or Max session interactively, thus providing a tighter feedback loop between scripting, visualisation and audition.

Another 

<NextSection 
next="References"
link={m.ref}
/>

<style>
	h1 {counter-reset: h2}
    h2 {counter-reset: h3}
    h3 {counter-reset: h4}
    h4 {counter-reset: h5}

    h1:before {content: "6." " "}

    h2:before {
        content: "6." counter(h2, decimal) " ";
        counter-increment: h2;
    }
    h3:before {
        content: "6." counter(h2, decimal) "." counter(h3, decimal) " ";
        counter-increment: h3;
    }
    h4:before {
        content: "6." counter(h2, decimal) "." counter(h3, decimal) "." counter(h4, decimal) " ";
        counter-increment: h4;
    }

    h2.nocount:before, h3.nocount:before, h4.nocount:before {
        content : "";
        counter-increment: none;
    }
</style>

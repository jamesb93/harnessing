<script>
    import NextSection from "$lib/components/NextSection.svelte"
    import Waveform from "$lib/components/Waveform.svelte"
    import YouTube from "$lib/components/YouTube.svelte"
    import ImageMedia2 from "$lib/components/ImageMedia2.svelte"
    import {metadata as m} from "../directory.svx"
    import X86 from '$lib/demos/reacoma/X86.svelte';
    import MaxwaveStart from '$lib/demos/reacoma/MaxwaveStart.svelte';
    import MaxwaveEnd from '$lib/demos/reacoma/MaxwaveEnd.svelte';
    import Sys from '$lib/demos/reacoma/Sys.svelte';
</script>

# ReaCoMa
ReaCoMa is a collection of [Lua](http://www.lua.org/) scripts that facilitate using the [Fluid Corpus Manipulation](https://www.flucoma.org) (FluCoMa) "First Toolbox" algorithms within the [REAPER digital audio workstation](https://www.reaper.fm). This suite of algorithms are designed to facilitate signal decomposition, audio descriptor analysis and automatic segmentation. My main motivation for creating ReaCoMa was to consolidate my compositional workflow within REAPER by extending it so that I did not have to switch between various pieces of software such as Max or the command line. Prior to the existence of ReaCoMa there was no way to directly use FluCoMa's tools within REAPER. There were and are Max objects, but for me, having to alternate between two distinct pieces of software, with vastly different interfaces and setups is a suboptimal compositional workflow, especially for tasks which benefit from a tight feedback loop between processing and listening. For example, processing audio files in Max and accessing the results within REAPER requires outputting those results to disk and making them a part of the REAPER session. This workflow is potentially time consuming if there are lots of audio files and requires several stages of manual file management. My imagined ideal workflow was to instead perform that processing directly within REAPER on media items and to be able to observe the results immediately after processing had completed. ReaCoMa solves this problem, and makes FluCoMa's first toolbox algorithms feel like a first class citizen in REAPER, enabling possibilities within that environment for sound design, sound decomposition, analysis and segmentation that are otherwise not readily available. From ReaCoMa's nascent stages of development I liberally used it for processing sound materials in [Reconstruction Error]({m.re}). Since then it has found its place as a fixture of my practice and a key tool for processing, manipulating and organising sounds such as in [Interferences]({m.em}).

ReaCoMa matured over a number of iterations as I incorporated it into my compositional practice. Nonetheless, even in its earliest versions it facilitated computer-led decomposition of sounds which had a large impact on how I composed the works and structured my engagement with audio-based corpora.

## Implementation
There is not a native [REAPER extension](https://www.reaper.fm/sdk/plugin/plugin.php) or audio plug-in that allows one to use the FluCoMa first toolbox algorithms directly within the REAPER environment. As such, ReaCoMa functions as a scripting bridge between the command line executables and the REAPER session where media items are positioned, selected and manipulated on a timeline. Thus, ReaCoMa offers a suite of scripts, where each script is responsible for implementing a single algorithm and orchestrating the necessary processing and changes to the session. For example, `fluid-hpss.lua` is used to execute the `fluid.hpss` algorithm, the same as what is found in Max as the object `fluid.hpss~`. 

To start, the user runs one of the Lua scripts while a media item or group of items are selected. This provides an interface for configuring parameters. This is shown in [IMAGE 1]({m.reacoma}#img1).

<ImageMedia2 
url="/tech-reacoma/step1.jpg" 
figure='IMAGE 1'
caption='User interface for parameter selection is shown after selecting and running one of the Lua scripts.'
id='img1'
/>

Once the user has configured the parameters they can execute the algorithm by clicking the "Okay" button. Depending on the type of algorithm, an update will be made to the REAPER session. In this case, `fluid-hpss.lua` returns two new takes and appends them to the source media item. This is depicted in [IMAGE 2]({m.reacoma}#img2). One take will contain the harmonic component while the other take will contain the percussive component. This facilitates rapid auditioning of the decomposition results by toggling which take is active. I have found this interface particularly potent in my compositional process as the source material is always stored and attached to the decomposition results, allowing me to follow the development of sonic materials directly in the REAPER session.

<ImageMedia2 
url="/tech-reacoma/step2.jpg"  
caption='New takes containing the harmonic and percussive components are appended to the source media item.'
figure='IMAGE 2'
id='img2'
/>

For the segmentation algorithms (`fluid-onsetslice.lua`, `fluid-noveltyslice.lua`, `fluid-ampslice.lua`, `fluid-ampgate.lua`, `fluid-transientslice.lua`) the original source media item is divided at the slice time points returned by the algorithm. This does not create new audio files on disk, rather, it creates new takes with the original source audio file and adjusts the start and length of those takes. [IMAGE 3]({m.reacoma}#img3) shows a file segmented with `fluid-noveltyslice.lua`.

<ImageMedia2 
url="/tech-reacoma/step3.jpg"  
caption='fluid-noveltyslice.lua is used with default parameters, rendering new takes at each slice point.'
figure='IMAGE 3'
id='img3'
/>

## Experimental Scripts
The initial aim of ReaCoMa was to process media items on the REAPER timeline. Achieving this required significant research to understand the ReaScript API. New possibilities became apparent in this research, such as how I could attach additional metadata to items through notes, organise media items according to audio descriptor data and enhance the segmentation workflow through automatic parameter selection. From this, three categories of experimental scripts emerged.

### Sorters
These scripts sort media items in the REAPER session timeline by ordering them according to audio descriptor analysis. There are two variants, one that sorts by spectral centroid and one that sorts by loudness. To produce a descriptor for each media item , the script orchestrations a frame-by-frame analysis and then averages the per-frame values. Each segment is then sorted along the timeline according to this summary.

### Auto-Slicing
Performing segmentation with the computerÂ often requires specifying a number of parameters to achieve the intended results. Sometimes this process is not intuitive or it can be difficult to find a combination of parameters that achieves the desired segmentation. The goal of the "auto-slicing" scripts is to automatically find the optimal settings for the `fluid-noveltyslice` algorithm so that it produces a predetermined number of slices. It solves this problem through brute force, incrementally adjusting the threshold and measuring how many slices were produced. Based on the results of this process it will increase or decrease the parameter to achieve the desired result. There is also a variant of this script which will keep the threshold the same while altering the `kernelsize` parameter. With the same amount of desired slices these two variants produce different results.

### Tagging
For the purposes of aiding my mixing, I wanted to be able to annotate media items with descriptor analysis data. The tagging scripts, `tag-centroid.lua`, `tag-loudness.lua` and `tag-pitch.lua`, use the relevant descriptor algorithm from the toolbox (`fluid.spectralshape`, `fluid.loudness`, `fluid pitch`) to calculate statistics (average, minimum, maximum, median) of the relevant audio descriptor across all the frames or values of a media item. This analysis is appended as a note to the media item and can be quickly viewed by hovering over the note icon. Instead of having to alternate between REAPER and another environment, I could make quick assumptions about the perceived loudness, brightness or pitch of samples to inform decisions involved in selecting sounds or their organisation. This also supported rectifying differences in my intuitive perception of sounds and how they would be processed by analysis in [FTIS]({m.ftis}) for example. [IMAGE 4]({m.reacoma}#img4) demonstrates the output of the tagging script and how it can be viewed on a media item.

<ImageMedia2 
url="/tech-reacoma/tagging.png" 
caption='Each item has been tagging using tag-loudness.lua. The results can be quickly viewed by hovering over the note.'
figure='IMAGE 4'
id='img4'
/>

## The Role In My Practice and Case Studies
ReaCoMa supports the phase of my compositional workflow by which I intuitively operate on materials within the frame of decomposition and deconstruction. [FTIS]({m.ftis}) supports querying that is operating on corpora, while ReaCoMa is important when I enter the more tactile phase of my practice in which I compose intuitively and manipulate materials through trial and error as well as experimentation. ReaCoMa has proven to facilitate different forms of querying to [FTIS]({m.ftis}) too, especially when I work with individual sounds. The following case studies demonstrate these. 

### Transient Extraction

In [sys.ji_]({m.re}) from [Reconstruction Error]({m.re}), a combination of `fluid.noveltyslice.lua` and `fluid.transients.lua` scripts are used to take a long sample and reshape it into a passage of music. I first deconstructed the sample into short segments which are randomly reshuffled to obfuscate the original morphology. Progressively, these random shuffles are "undone" so that the morphology of the original sample by the end of this process is reconstructed. I then decomposed those segments into a transient and residual component (everything except the transients). A fade in envelope is applied to the residual component and both components are summed together. This modifies the morphological and textural properties of the sound as the residual component is gradually introduced while the transients are exposed.  This functioned to also generate various colours of impulse-based material, and as a way of temporarily suppressing the noisier aspects of the sound. The affordances of being able to treat the sound as two distinct components catalysed the formal shape, thus positioning ReaCoMa in another form of computer-led compositional decision making.  [DEMO 1]({m.reacoma}) allows you to explore this decomposition process interactively.

<!-- WIDGET: sys widget -->
<Sys />

In [X86Desc.a]({m.re}#x86desca), a section spanning 1:20 to 1:37 is built from several interlocking streams of impulse-based materials or clicks. These were not manually created by my own manual methods, rather, they were discovered by extracting the transients from spectrally complex noise-based source material. With this approach, I was able to find intricate and varied patterns of this type of material by exploring with the extraction algorithm decomposition processes.  This is demonstrated in [DEMO 2]({m.reacoma}#demo2).

<X86 
title='Transient Extraction in X86Desc.a from Reconstruction Error' 
caption="DEMO 2" 
id="demo2"
/>

### Harmonic Percussive Source Separation
In [_.dotmaxwave]({m.re}#_dotmaxwave), `fluid.hpss.lua` is used to separate the harmonic component and percussive component from source material possessing a dominant pitch component and a subtle shade of noise.  I achieved a level of control over shaping the sound that would be otherwise difficult without such decomposition techniques using this approach. It also guided me towards structuring the piece around manipulating the relationship between the two extracted components. In this way, the decomposition process situated within a tight feedback loop of listening and processing facilitated computer-led sound discovery and conceptual development. [DEMO 3]({m.reacoma}#demo3) allows you to isolate the harmonic and percussive components to observe the decomposition results in this piece.
<MaxwaveStart />

### Non-Negative Matrix Factorisation
<!-- WIDGET: NMF widget -->
When working with thick and texturally dense sounds, non-negative matrix factorisation (NMF) processing became useful as a means to to *unpack* those sounds into distinguished components. An example can be found in [Interferences]({m.em}#manchors) at 1:35 and 3:39, in which a sample is split into two channels through NMF decomposition. This disentangles prominent perceptual qualities which are superimposed in the source. The overall morphology and structure of the sound remains intact, but can be manipulated and reconfigured to situate better with other materials. [DEMO 4]({m.reacoma}#demo4) allows you to audition the source material before and after it was processed in this way.

### Chaining Processes
<!-- WIDGET: chaining video -->
As I became more fluent with ReaCoMa in my workflow I began to use it in more sophisticated ways, such as by chaining together processes to deconstruct sounds. In [{m.anchors}]({m.em}#manchors) of [Interferences]({m.em}), one particular texture I created serves as a pertinent demonstration for how chaining allowed me to retrieve specific and nested segments from a composite sound and to compose with it directly in REAPER.

The resulting texture can be heard at 8:02 in [{m.anchors}]({m.em}#manchors) and was extracted through a multi-step process. I began by decomposing the sound into transient and residual components. Because ReaCoMa produces new takes in this scenario, I could easily toggle between which component I wanted to operate on afterwards. I then used the novelty slicing algorithm on the transient component, which produced segmentations that were perceptually aligned with moments of rapid changes in the signal. Using a ReaCoMa sorting script, I arranged the segments on the timeline by their spectral centroid. This allowed me to then manually audition and select a group of samples with delicate and fragile qualities by listening to sounds only arranged at the right edge of the group of samples. I then constructed the final texture by intuitively arranging those samples which I carved off from the main section into blocks. [VIDEO 1]({m.reacoma}#vid1) shows how this process worked in practice, although it is a recreation of the steps after the fact.

This process was not designed in a step-by-step plan to begin with and at each stage I would attempt different combinations of processes to try and extract the delicate and fragile sounds from within the block of source material I began with. Being able to audition, test, undo and rapidly weave between different strategies allowed me to find a specific moment of sound across a whole sound file relatively easily.

## Related Links
I produced a series of video tutorials that demonstrate how to install and use ReaCoMa. This video series is useful as a resource that documents how the user facing parts of ReaCoMa work and shows some use cases related to my own practice. The [ReaCoMa Source Code](https://www.github.com/jamesb93/reacoma) is available as well as an external site with additional documentation at https://www.reacoma.xyz.

<YouTube 
url="https://www.youtube.com/embed/r3uHMXmlPRo"
title='ReaCoMa Tutorial Video Series'
figure=''
id=''
/>

<NextSection 
next="Finding Things in Stuff"
link={m.ftis}
/>

<style>
	h1 {counter-reset: h2}
    h2 {counter-reset: h3}
    h3 {counter-reset: h4}
    h4 {counter-reset: h5}

    h1:before {content: "5.2." " "}

    h2:before {
        content: "5.2." counter(h2, decimal) " ";
        counter-increment: h2;
    }
    h3:before {
        content: "5.2." counter(h2, decimal) "." counter(h3, decimal) " ";
        counter-increment: h3;
    }
    h4:before {
        content: "5.2." counter(h2, decimal) "." counter(h3, decimal) "." counter(h4, decimal) " ";
        counter-increment: h4;
    }

    h2.nocount:before, h3.nocount:before, h4.nocount:before {
        content : "";
        counter-increment: none;
    }
</style>
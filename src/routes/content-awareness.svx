# Content-Awareness in my Sampled-Based Practice
This section provides an overview of content-awareness in relation to my compositional practice. To preface this, I outline conceptual and pragmatic areas of machine listening and machine learning, two subject areas that are fundamental to the design of my own content-aware programs. I then provide a technology review by discussing other practices, software and projects where machine listening and content-awareness is pivotal. I conclude this section by situating technology within my practice as "augmentative", in comparison to "semi-supervised" and "unsupervised" approaches.

## Machine Listening 
Machine listening is a field of study that uses signal processing, machine learning and [audio descriptors]({m.ca}#audio-descriptors) in order to enable computers to "listen" to sound. The model of listening constructed from these technologies may endeavour to replicate the way that humans interpret sounds, or it could be based on creating a computational system that is nothing like a human and produces results beyond what humans are capable of.

Machine Listening plays an important role in a variety of applications, such as automatic sound recognition (what sound is *that*?), creating a condensed summary of a sound, also known as "fingerprinting" and automatic organisation and classification of sounds. A comprehensive overview of machine-listening applications can be found in Wang (2010). In my practice, I leverage machine listening for automatically deriving collections of sounds from corpora of sample-based materials, performing descriptor-driven concatenative synthesis and for building software that can respond in real-time to audio input. These capabilities aid me in composition by enhancing manual processes such as sample selection and arrangement . This was pivotal in [Stitch/Strata]({m.ss}), [Reconstruction Error]({m.re}) and [Electromagnetic]({m.em}). I also harness machine listening to shape generative processes where materials are constructed through synthesis, such as in [Annealing Strategies]({m.as}). Finally, I use machine-listening in real-time systems where the interaction between an instrumentalist and a listening machine's behaviour structures the music, for example in [Refracted Touch]({m.rt}).

### Machine Listening and Compositional Thinking
Unlike human listening, machine listening is not embodied and unless specifically designed to, has no implicit life experience, memory or framework for interpreting sound. Humans however are apt at understanding the context of a heard sound by relating it to their lived experience and collective understanding for how different sounds can relate. A human listening to a previously unheard sound might be able to quickly discern its source, or project how it relates to a another memorised sound, or imagine several uses within composition. Although machines do not listen in this relational and context driven manner without specifically being programmed to do so, they have the benefit of "listening " rapidly and can create a quantifiable and numerical type of response to sound. They are also more capable than humans are at listening to large collections of sounds quickly and can "memorise" many different sounds simultaneously, allowing a kind of referential and context that is outside of the human listening and psychological experience.

While machine listening is often used in an effort to humanise machines - such as with speech recognition to enable humans to communicate more naturally with them  - the harsh and digital character of machine listening can be used for producing understanding of sounds that are outside of what a human would normally do. My practice embraces this and positions machine listening as an enhancement to my own listening. For example, I use [audio segmentation]({m.ca}#audio-segmentation) to segment collections of sounds into smaller units of sound that I can then compose with. This process never produces results that perfectly align with how I would perform this task without the aid of the computer. By doing this I have to engage with the machine listening algorithm by adjusting its parameters, observing the result and deciding if the result is satisfactory. These results might feel quite close to how I would perform the task manually or they could diverge in a way that is still satisfactory but in an unintended manner. In this particular example, the differences between segmentation configurations can affect the material I am given to work with. Through this process, I am allowing myself to be challenged and for serendipitous results to emerge through experimentation with machine listening. This type of workflow propagates across my practice more widely and machine listening is a way that I become familiar with sound materials as well as invent schemes for their use in composition. Machine listening allows me to test compositional hypothesis which can inform future modifications to the underlying technical implementation or it could prompt reflection on the sound material that I was concerned with in the first place. 

As such, machine listening presents two key features in my practice. Firstly, it can reveal the internal structure of sounds in ways that humans cannot or in situations where human listening is inadequate. Secondly, I can encode my listening, processing of materials and engagement with sounds in machine listening. This allows me to use the computer to test those ideas and to experiment by reconfiguring the underlying technology rather than relying solely on intuition and manual investigation. This second feature manifests as a significant aspect of computer-assistance in my practice, and is the basis on which many compositional decisions are made. There are numerous occasions documented in the [project]({m.proj}) section that describe situations in which initial plans and aims were challenged through experimentation and how those challenges have inflected the final result.

## Key Technologies of Machine Listening
There are three key areas of machine listening technology that I work with in my practice. While each of these components has a defined role, their design and implementation is entangled and in many cases they perform calculations and use the results produced by each other . This section will describe these three technologies: [Audio Descriptors]({m.ca}#audio-descriptors), [Audio Segmentation]({m.ca}#audio-segmentation) and [Signal Decomposition]({m.ca}#signal-decomposition). 

### Audio Descriptors
Audio descriptors are values or labels that represent characteristics of a sound (or features). Audio descriptors can be computed automatically with machine listening or manually assigned by a human. Audio descriptors can be used to describe a perceptual quality of a sound such as its “loudness” or “brightness” or may represent high-level semantic information such as the genre or when a sound-file stored on a computer disk was produced. Audio descriptors are used for both technical applications and by creative coders in musical practice. Artists who use audio descriptors include [Andrea Valle](https://andreavalle.org), [Alex Harker](https://www.alexanderjharker.co.uk), [Alice Eldridge](http://www.ecila.org), [Owen Green](http://owengreen.net) or [Diemo Schwarz](http://diemo.concatenative.net) for example.

In my practice I use audio descriptors to describe and differentiate digital samples based on timbral, textural or morphological characteristics. There are many descriptors available and are documented in both Herrera et al., (1999), Malt & Jourdan (2008) and Peeters (2004). However, I only use a small subset of the descriptors described in such literature. Curating and selecting descriptors is a part of my compositional process, and by selecting certain descriptors I guide the computer to listen in a way that is sympathetic to my [aesthetic preferences]({m.preoc}#aesthetics). Descriptors that I often find useful include spectral moments [(centroid, spread, skewness, kurtosis)](https://en.wikipedia.org/wiki/Moment_(mathematics)), other statistics calculated from spectra ([rolloff](https://librosa.org/doc/0.8.0/generated/librosa.feature.spectral_rolloff.html), [flatness](https://en.wikipedia.org/wiki/Spectral_flatness), [crest](https://uk.mathworks.com/help/audio/ref/spectralcrest.html?requestedDomain=)), [mel-frequency cepstrum coefficients (MFCC)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum), and statistical calculations applied to this data (mean, standard deviation, median, percentiles).

As part of my understanding and application of audio descriptors I consider them in three categories; low, mid or high-level. These categories relate to an increasing level of abstraction from the sound that is analysed. This classification system is borrowed in part from [Daniele Ghisi's thesis (Ghisi, 2017)](https://www.danieleghisi.com/phd/PHDThesis_20180118.pdf)  who remarks that such a subdivision is not necessarily agreed upon. As such, I propose this classification as a strategy for understanding the different levels that audio descriptors occupy in my practice and what that means for their creative affordances, rather than as some objectively valuable classification system. The most economical way to outline this system is to use a real-world example. For this, an onset detection algorithm designed to extract onsets from the amplitude of a signal will be the example. This onset detector has a sensitivity control modified with a single threshold specified in decibels which, when exceeded by the signal, triggers an onset. For this onset-detection to work, several stages of analysis and audio descriptors form part of a "processing pipeline" where data is transformed and passed between those stages. 

#### Low-level
Firstly, the digital representation of the source signal is analysed sample-by-sample to derive the amplitude by [rectifying the signal](https://www.sfu.ca/sonic-studio-webdav/handbook/Rectification.html) and converting the normalised value from linear amplitude to decibels. Calculating these amplitudes from the raw samples is a low-level descriptor, and processes such as frame-by-frame, sample-by-sample or windowed calculations fall under this category. Low-level descriptors often match the dimensionality of the analysed sound. For this onset-detection algorithm a new point of data is produced for each sample. As such, low-level descriptors can be useful for creative applications where the derived values immediately used. For example, these amplitude values could be mapped to a parameter of a sound-producing module like distortion or compression.

#### Mid-level
Secondly, after deriving the amplitude values, a threshold is applied to determine at what points in time that threshold is exceeded. It could also be recorded how far the threshold is surpassed to determine the *intensity* of an onset. This new set of data is increasingly abstract from the original source sound and tends to be a sumation of a feature of that signal. In this case, this descriptor describes when a certain level of change has occurred which is not an inherent characteristic of the signal itself. In this particular onset-detection example, the onsets could be used in real-time to trigger the playback of other sounds or to activate pre-composed events. Furthermore, one might use these onsets to [segment]({m.ca}#segmentation) a signal offline, based on the assumption that meaningful separations of the source material are to be found in the differences in amplitude.

#### High-level
Lastly, with a collection of onset times and intensities the relationship between them could be analysed. Doing so might engender additional semantic information about the signal or provide a singular summary of the entire source. As a hypothetical example, there may be a pattern between the onset times that can be derived through analysis which could be be useful for describing how perceptually rhythmic the source sound is. The concept of a sound being more or less rhythmic might be hard to formalise computationally. However, other similar high-level descriptors exist such as[danceability](https://essentia.upf.edu/reference/streaming_Danceability.html). Such a hypothetical "rhythmicity" high-level descriptor departs from analysing the inner structural elements of the signal and moves toward a kind of *musical listening*.

As my practice has developed over this research project, I have increasingly relied on using high-level descriptors, while earlier in my practice low-level descriptors were my primary tool for exploring with machine listening. This trajectory was not intentional or predetermined, rather, it came about from developing my creative coding skills and experimenting with increasingly complex technologies in the service of changing sets of questions and interests. This is an important aspect of my practice that will be traced in each [project]({m.proj}) section more thoroughly.

#### Issues with Audio Descriptors
Given that high-level descriptors often use data produced by low-level or mid-level descriptors, a number of issues can arise if these dependent audio descriptors are poorly selected or managed. This has consequences when compositional choices are made from this information or if the computer is working further with that data. As such, the curation, implementation and application of descriptors is an entangled process that requires reflection and thought within the context of their application.  I consider the application of audio descriptors in my practice as important as orchestration is to a composer of acoustic music. Grappling with these types of issues became increasingly prevalent in the final projects of this research and my attempt to make these types of issues more manageable can be seen in the development of ["finding things in stuff" (FTIS)]({m.ftis}).

#### Implementations
Many software tools can be used to calculate audio descriptors especially for creative coding and composition. For Max there is [Alex Harker](https://www.alexanderjharker.co.uk)'s' [descriptors~](https://github.com/AlexHarker/AHarker_Externals/tree/master/descriptors/descriptors%7E) object, the [zsa.descriptors](http://www.e--j.com/WordPress/wp-content/SBCM09_Malt_Jourdan_v1.2.pdf), the [MuBu/PiPo/ircamdescriptor~ ecosystem](https://forum.ircam.fr/projects/detail/mubu/) and several of the [Fluid Corpus Manipulation (FluCoMa)](https://www.flucoma.org) objects. Among text-based languages such as [Python](https://www.python.org) and [C++](https://www.cplusplus.com) there are packages and projects including: [Essentia](https://repositori.upf.edu/handle/10230/32252), [libxtract](http://hdl.handle.net/2027/spo.bbp2372.2007.116), [Surfboard](https://github.com/novoic/surfboard) and [aubio](https://aubio.org). It is worth noting that even for different implementations of the same base algorithm there are disagreements in design, and "flavours" of algorithms that better suit the biases and preferences of the creator. For example, a discussion on the [FluCoMa discourse](https://discourse.flucoma.org/t/mfcc-comparison/484/31?) highlights how seemingly subtle engineering choices such as liftering on MFCCs can drastically alter the results of an algorithm that attempts to match two samples by proximity in euclidian space. A video accompanies this, where [Rodrigo Constanzo](http://www.rodrigoconstanzo.com) demonstrates such differences in a Max patch.

<YouTube 
    url="https://www.youtube.com/embed/DQ_hPkrbgzQ"
    title="FluCoMa vs Orchidea MFCC matching comparison"
    figure="VIDEO 1"
/>

### Audio Segmentation
According to Ajmera, McCowan and Bourlard (2004) audio segmentation is "the task of segmenting a continuous audio stream in terms of acoustically homogenous regions, where the rule of homogeneity depends on the task". Homogeneity is a broad term, and can refer to a perceptual grouping process but is not necessarily the case. Segmentation can be performed *manually* or *automatically*. In *manual segmentation* divisions are created by a human through intuition or listening. *Automatic segmentation* is performed by a computer, allowing the process to be applied to collections of samples programmatically.

Automatic segmentation is an essential component of machine listening in my compositional practice and supports a creative workflow in which granular compositional units can be collected without having to exhaustively produce them from the ground up. This is performed by taking several whole sound files, often collected through additional automatic procedures, and designing an appropriate automatic segmentation algorithm to separate these into smaller parts. I can then employ technologies such as audio descriptors, statistical analysis, dimension reduction and clustering to organise these collections into groups. Such groups might be predicated on perceptual similarity, or to satisfy constraint-based organisation (such as in descriptor-driven concatenative synthesis). The results of these processes inspire and lead many facets of composition, such as sample selection and phrase construction. This way of working is particularly prevalent in [Stitch/Strata]({m.ss}), [Reconstruction Error]({m.re}#segmentation) and [ElectroMagnetic]({m.em}#exploring-with-ftis) and I discuss the challenges in selecting appropriate segmentation tools for dividing large corpora in these sections.

My understanding and engagement with segmentation techniques developed throughout this research, particularly as my demands for more sophisticated automatic algorithms increased. To this end, I developed [ReaCoMa]({m.reacoma}), a set of [ReaScripts](http://reaper.fm/sdk/reascript/reascript.php) for [REAPER](http://reaper.fm/index.php) that among other things, facilitate rapid and interactive automatic segmentation in a DAW. This package utilises the command line from the [Fluid Corpus Manipulation](https://www.flucoma.org) to perform the actual segmentation, allowing a variety of different types of segmentation algorithms within a single composition environment.

#### Onset Detection
While segmentation is not strictly the same as onset detection, there is a significant overlap between these two tasks. Both segmentation and onset detection are essentially approaches to detecting events based on meaningful points of difference in a signal. I used onset detection in [Refracted Touch]({m.rt}) as a way of triggering events that would progress through pre-composed states. Works that utilise onset detection and have influenced mine include [Rodrigo Constanzo's *Kaizo Snare*](https://rodrigoconstanzo.com/2020/08/kaizo-snare/), [Martin Parker's *gruntCount*](https://www.tinpark.com/gruntcount-release) and [Christos Michalakos' *Augmented Drum*](https://rke.abertay.ac.uk/ws/files/15874958/Michalakos_Augmented_drum_kit_Published_2012.pdf). 

#### Implementations
In my practice and particularly in early projects, I have tended towards using the ["dynamic split tool"](https://wiki.cockos.com/wiki/index.php/Dynamic_split) that is part of REAPER. This segmentation tool is highly configurable and I have used it to extract smaller samples from recordings, such as in [Stitch/Strata]({m.ss}) for isolating phonetics sounds. With the release of the [FluCoMa](https://www.flucoma.org) "first toolbox" their suite of segmentation algorithms became a large source of experimentation for me and provided several different models that each possess unique strengths and weaknesses. These tools have been creatively stimulating, by offering segmentation algorithms that are better suited to materials where amplitude-based slicing is not suitable. This situates the FluCoMa tools in my practice well, because I am mostly concerned with textural materials that do not respond well to amplitude-based segmentation.

### Signal Decomposition
Signal decomposition is the "extraction and separation of signal components from composite signals, which should preferably be related to semantic units" (Jens-Rainer Ohm, 2004, p. 317). In music audio and music composition, signal decomposition can be used to unmix composite digital audio, such as a recording containing several instruments. It can also be used with monophonic sounds, where the superimposed internal layers in a sound can be isolated from each other. 

Signal decomposition techniques are often modelled on human listening and may attempt to reproduce computationally the types of filtering and separation that humans can perform. [Harmonic percussive source separation (HPSS)](https://www.researchgate.net/profile/Derry_Fitzgerald/publication/254583990_HarmonicPercussive_Separation_using_Median_Filtering/links/00b495396ef03235af000000.pdf), for example, attempts to isolate harmonic and percussive components from within a single sound. This can be useful for reducing the noisier components of a signal, or for shaping a harmonic aspect of a sound without affecting the noise.

<HPSS />

Other techniques such as [non-negative matrix factorisation (NMF)](https://www.researchgate.net/profile/Derry_Fitzgerald/publication/224633812_Shifted_non-negative_matrix_factorisation_for_sound_source_separation/links/00463521492bddc0dc000000/Shifted-non-negative-matrix-factorisation-for-sound-source-separation.pdf) may not be designed with specific decomposition tasks in mind and instead provide a generalised algorithm for extracting "components" from a source. Such techniques could form the basis for a specific task, such as separating out distinct instrumental parts from an ensemble recording, or might be used more speculatively to decompose sounds into arbitrary parts.

<!-- WIDGET: Widget with NMF -->

Ultimately, there are a variety of signal decomposition approaches and techniques that can support different creative intentions . Decomposition tasks can be approached with a known separation in mind and its success can be improved by working with an algorithm, learning how it operates and tuning its parameters. On the other hand, an algorithm might support a speculative computer-led approach where value is found in unexpected results . In such a scenario, the emphasis is on seeing what emerges from the decomposition process rather than getting closer to a solution which satisfies pre-determined goals.

As my practice has developed, signal decomposition has become a prominent part of my approach to composing with samples. In later projects, particularly [Reconstruction Error]({m.re}) and [ElectroMagnetic]({m.em}), decomposing sounds is used widely to process samples for creative effect as well as for enhancing the capabilities of machine listening by extracting specific characteristics from a sound and analysing those. Decomposition also has a place in my practice for speculative deconstruction of textural materials and can be creatively fruitful even when the outcomes are unpredictable.

#### Implementations
Musical applications of signal decomposition are an emerging commercial market and can be found in tools such as  [Izotope RX](https://www.izotope.com/en/products/rx.html). These tools are expensive, closed-source and relatively hard to gather information on how they work under-the-hood. More accessible and open implementations of signal decomposition algorithms are available for a wider scope of purposes but often require knowledge of a programming language such as Python, and are generally are not designed for real-time applications. In this sphere, there are projects such as Northwestern University's [nussl](https://github.com/nussl/nussl), and the University of Surrey's [untwist](https://github.com/IoSR-Surrey/untwist), [The Bregmann Toolkit](https://github.com/bregmanstudio/SoundscapeEcology) (see [Eldridge, Casey, Moscoso and Peck's work (2016)](https://peerj.com/articles/2108/) and [Spleeter](https://github.com/deezer/spleeter). A relatively emerging set of tools have been produced by the [Fluid Corpus Manipulation project](https://www.flucoma.org), which specifically focus on the creative applications of signal decomposition. These algorithms have supported much of my experimentation and incorporation of decomposition techniques into my practice.

<!-- DELETE: do we need this? -->
### Conclusion
There are three aspects of machine learning that are important in my practice. Audio segmentation helps me to automatically derive compositional materials by segmenting long sounds into smaller ones. Signal decomposition can assist me in accessing the inner components of sounds or for finding unforeseen ones. Audio descriptors are the backbone for how the computer listens, by producing data that can describe the content of a signal. The next section [Machine Learning]({m.ca}#machine-learning), gives an overview for how I  machine learning fits into my practice and approach to composition.

## Machine Learning
Machine learning is a branch of artificial intelligence that focuses on building algorithms that enable machines to learn from data in order to improve their results (Mitchell, 1997). Machine learning algorithms create models from training data, which is often supplied in large amounts. Using this model, machines can make predictions, find patterns or generate new data based on what has been "learned" through training. Because of this, machine learning algorithms can be used in a variety of applications assuming that there is data that can be learned from in order to solve the task at hand. This can be especially useful where conventional algorithmic approaches are hard to conceptualise or develop. Instead, the computer can learn *how* to solve problem itself. 

In music and audio, machine learning has been used toward a number of technical and creative goals. For example,  [Tone Transfer](https://sites.research.google/tonetransfer) made by [Google Research](https://ai.google) allows users to perform "style transfer" between two digital audio recordings. The result is a hybridisation of the two source recordings, where the timbre from one is transplanted onto the dynamics and expression of the other.  Machine learning has also been used to analyse the structure of music and to generate new music based on what has been derived through that analysis. Such research has been conducted using different types of training data, including folk music (Sturm et al., 2016) and the music of Johann Sebastian Bach with DeepBach (Hadjeres et al., 2017).

<!-- !!! -->
<!-- 
It just has to make sense!
If you write about machine learning, its applications and then flesh out the 'speculative bit' youre in a good spot.
Talking about agency and LCGI sets up the final paragraph well where you can link back to it.
 -->
In my practice I use machine learning in conjunction machine learning for a number of purposes.
    Reduce dimensionality of data
        - This makes searching for samples simpler
        - Computer can figure out things for me
        - Mapping (which is discussed later)
    Improve analysis
    Classify audio files
    - Machine listening can produce a lot of data especially given how many samples are present in even short digital audio samples.
        - Machine learning can help to make make sense of this data automatically. 
    - Take many data points and compress them to few

    - Clustering
        - Grouping together samples. Depending on the audio descriptor that is used this can produce clustering results that create groupings based on different features.
        - Removing redundancy from data
    - Speculative...
    - The design of new learning algorithms for musical applications might thus be motivated by a desire to support new useful affordances rather than only by more conventional computational goals such as accurate and efficient modelling

    Furthermore, creators of new music technoologies are often engaged with what design theorist Horst Rittel described as "wicked" design prbolems: ill-defined problems wherein a problem "definition" is found only by arriving at a solution. (Page 18) #process

    By affording people the ability to communicate their goals for the system through user-supplied data, it can be more efficient to create prototypes that satisfy design criteria that are subjective, taciet, embodied, or otherwise hard to speciy in code. (Page 18) #process #tacitknowledge 

- Agency and "Black Boxes"
    - Interactive: change the data, change the output
    - Machine Learning often encapsulates its functionality inside itself, hidden 

### Black Boxes
In many of these examples, and particularly because contemporary machine learning technologies are incorporated into works, it can be difficult to probe inside these programs to understand exactly what they are doing. For example, while neural networks can in theory approximate any function, studying the internal structure will not give one any insights on the structure of the function being approximated. Instead, one has to observe how the curation and refinement of training data, and modification to[hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)), influences the output, perhaps turning to a different kind of implementation to shape that process. Even then, state-of-the-art approaches boast that their programs **only** have millions of parameters, such as in [Rethinking the Inception Architecture for Computer Vision (2016)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf), which is a scale of control that goes far beyond what we can grapple with meaningfully, particularly in a compositional context. The established parametric approach to composing with digital tools is relinquished in favour of allowing a conceptual, abstract and complex machine to deal with a task with humans as observers and tenders to its execution.

Part of my music practice is influenced and formed by my exposure to artwork in the computational visual art. This is a rich territory where in some cases cutting edge technology, particularly those involving "black boxes" is being explored for much more *unsupervised* and *autonomous* creative ends. Examples of this can be found hosted and displayed on the internet, such as ["art42"](https://art42.net), a program that is trained on cubist paintings and endlessly generates new ones, a GAN that is trained on children's drawings and generates new ones by [@erinbeess](https://twitter.com/erinbeess/status/1317871709870063616) or a program that [creates new symbols and shapes](https://www.fluate.net/en/travaux/vectoglyph) from a training set of existing glyphs.

Similarly, [Damien Henry](http://dh7.free.fr) has created [a program that attempts to construct sequential video frames by training a neural network on past video frames](https://magenta.tensorflow.org/nfp_p2p). The result is approximate in such a way that the outputs are believably real, however, the smeared textures and warped objects contradict this, producing a feeling that the computer is somewhat able to understand what we might expect from the next frame, but not entirely.

<YouTube 
    url="https://www.youtube.com/embed/lr59AhOPgWQ"
    title="Damien Henry's neural network video frame predictor" 
/>

As is the case in Ghisi's work, these examples from the visual arts demonstrate an approach in which much of the creative work is constructing machine learning *black boxes* that consume vast amounts of input data to generate outputs. Presenting these examples contextualises the edges of artistic practice with computer-aid and where *unsupervised* machines are being used with cutting edge technology. At the beginning of this research these approaches influenced my practice and research aims greatly and I believed using similar techniques could generate music for my own practice, perhaps almost entirely without my intervention after design and development processes had finished. Almost immediately in the arduous process of composing [Stitch/Strata]({m.ss}) I arrived at a compositional workflow situated between the *unsupervised* extreme and a completely *supervised* level of self-imposed control. This was the initial context for my content-aware CAC practice which I now classify as *augmentative*. This concept is explored in more detail in the next section.

### Lose Control Gain Influence
A framework for conceptualising the range of computer autonomy in these practices is "Lose Control, Gain Influence" (LCGI) put forward by Alberto De Campo (2014). The thrust of the article in which this concept is proposed, addresses considerations of parametric control for devices and digital instruments with multiple complex and interconnected parameters. De Campo (2014, p. 1) describes LCGI as "gracefully relinquishing full control of the process involved, in order to gain higher-order forms of influence". By giving up direct control one can unlock new modes of creativity working with a computer that are based on influencing the machine as opposed to manipulating it directly. De Campo argues that exploring the territory in between the *unsupervised* and *supervised* extremes is where new possibilities for interacting with computers emerge. Despite the intended application of this framework toward interface design and instrument control, LCGI is useful in conceptualising the spectrum of computer autonomy in computer-aided composition with content-aware programs. 

LCGI as a concept foregrounds a set of contemporary uses of computers in art in which computational decision making sits at the core of practice. Deferring critical processes of selection, curation or realisation is not just to make producing artworks easier or faster, it assists in developing a creative methodology where ideas are explored computationally, letting the computer inflect a process or even be fully responsible for it. While the earliest approaches to computing were primarily for problem solving and intense number crunching, we are now at a level where technology, such as artificial intelligence and its close neighbour machine learning are becoming readily available from both the perspective of usability and personal computing power. These affordances align with [Douglas Engelbart's position on computing in 1962](https://apps.dtic.mil/dtic/tr/fulltext/u2/289565.pdf), its potential laying in *augmenting human intellect*, where "humans could work with [computers] to support and expand their own problem-solving process" (Carter & Nielson, 2017, p. 1). 

## Compositional Applications of Machine Listening and Learning
Machine listening has expanded the possibilities of computer-aided composition by enabling the computer to perform sophisticated analytical and machine listening techniques. Below I detail various artists' work that uses machine listening and reflect on how their approach is relevant to my own practice and interests.

### Embracing The Machine In Machine Listening
Modelling human listening computationally is a complex problem and is often a approximation of the deeply entangled psychological processes by which humans interpret sound. Some artists embrace this problem and foreground the digital, inorganic computer behaviours produced by machine listening algorithms as a central aesthetic of their work. The work of [Owen Green](http://owengreen.net) happily confronts and exacerbates the tendencies of the computer, pushing bespoke machine listening software to its limits by designing algorithms that teeter on the edge of breaking or that are applied toward difficult (perhaps impossible) listening tasks. 

<YouTube 
    url="https://www.youtube.com/embed/zeOm9w3ZJVc"
    title="Owen Green performing 'Race to the Bottom'"
    figure="VIDEO 2"
/>

In *Race To The Bottom* (see VIDEO 2), Green creates several machine listening modules that work together to form a simultaneously cooperative and antagonistic improvisational partner that responds to him bowing a cardboard box. In VIDEO 3, Green discusses both musical and technical aims for this work. For him, the use of machine listening in this piece is a strategy for managing form while avoiding the explicit modelling of musical time. At the core of the machine listening system is a series of onset detectors that are prone to over detecting onsets from a live input signal. This results in an ensemble of rapid impulse generators which are connected to other sound-producing and manipulating modules. Sound-producing modules that are driven by this rapid onset detection impulse generator include a harmonic percussive decomposition algorithm which Green uses to processes each layer in isolation from each other.  His goal in doing so, is to be able to morph each component into its counterpart. The harmonic component is ring modulated, using the interval between impulses to control the frequency of the carrier signal. This results in a distortion to the input signal, adding side bands and artefacts to the sound. The percussive layer is smeared in time using a sample and hold, again driven by the timings generated by the impulse trains. This produces something more "harmonic" for Green, based on his personalised notion that harmonic sounds are things that repeat in time.

In addition to Green's motivations for managing form without specifically modelling time, he presents this piece as a setup where the machine listening program is tasked with interpreting amplitude onsets from the complex textural sounds of bowed cardboard. By doing this, he creates an unpredictable situation - the machine listening implementation is likely to fail - however, the failure is embraced aesthetically and introduces indeterminacy to the listening machine's behaviour and spontaneity to the performance. This approach foregrounds the aesthetics of machine listening and its behaviour. The machine will accurately and rapidly do what it is instructed to do, but the output is not guaranteed to be something that is understandable or perhaps coherent in relation to human listening. Instead of trying to mitigate this, Green embraces and lets it inflect the shape of the improvisation.

There are several aspects of Green's practice with machine listening that do not resonate with my compositional method:

1. I have no performative practice with an instrument. Many instrumental performers who work with live electronics, for example [Mari Kimura](http://www.marikimura.com), [Ben Carey](https://bencarey.net), [Anne La Berge](https://annelaberge.com), [John Eckhardt](https://www.johneckhardt.de) possess a level of virtuosity and training that is beyond my own and I consider this to be an essential. Green playfully subverts this notion of instrumental virtuosity by playing a cardboard box. However, there is a level of knowledge that he has acquired in coercing sounds from and controlling it which is virtusic and skillful in its own right.

2. I prefer a level of control over unpredictability and indeterminism that is not dependent on a single iteration or performance. Part of my compositional process is selecting outputs made by the computer, such as rendered audio or generated REAPER sessions. I want to be able to do this "out of time" in the studio, rather than experiencing machine listening behaviours in real-time and responding to this.

That said, Green's work influences me in the way that machine listening algorithms can be pushed outside conventional operation to render creative results. I rarely subvert machine listening algorithms to the extent that Green does but there is a resonance between our practices in how machine listening is used less like a tool and more like a lens for exploring ideas. The application of machine listening does not always have to be for producing results that are commensurate with human listening rather, as a composer it can be valuable to observe how a set of constraints or behaviours on machine listening can be explored to reach unexpected creative outcomes. This can be found prominently throughout [Annealing Strategies]({m.as}) and to an extent in [Refracted Touch]({m.rt}).This type of exploring with machine listening is foregrounded with [FTIS]({m.ftis}) in [Electromagnetic]({m.em}).

<YouTube 
    url="https://www.youtube.com/embed/cekfFC9eZhY"
    title="Owen Green presentation on 'Race To The Bottom'"
    figure="VIDEO 3"
/>

### A Spectrum of Perceptual Congruency
Machine listening does not always have to produce perceptually congruent results with human listening to be creatively useful. Working along a spectrum of congruency can be a fruitful territory where machines produce novel outcomes. With these interests in mind, composers and creative coders have used audio mosaicing and descriptor-driven concatenative synthesis techniques such as that found in [CataRT](https://github.com/Ircam-RnD/catart-mubu). The central aim of these techniques is to take a source sound and match it to a target using a measure of distance between two feature vectors. Given enough diverse source segments, one can ideally produce a new output that is perceptually similar to the target. This process is not perfect, and may result in a sound that does not necessarily resemble the target perceptually. Between perfectly this process perfectly recreating the target and the computer producing results that are totally unrelated is a spectrum that services a wide-range of aesthetic exploration.

In Constanzo's [C-C-Combine](https://rodrigoconstanzo.com/combine/) (see VIDEO 4), the process of source-to-target matching is performed with grains of audio as small as 40 milliseconds from the source to form the target.

<YouTube 
    url="https://www.youtube.com/embed/K5AgH1leBUU"
    title="C-C-Combine"
    figure="VIDEO 4"
/>

A range of controls are available for weighting audio features in the matching process. This becomes an expressive control because discrepancies inherently found in matching two sounds can be compensated to improve the congruency. If, for example, the pitch is one semitone away in the source from the target, one can ask C-C-Combine to re-pitch the source grain by that difference to match the target better. The same process can be applied to the loudness of the source. Manipulating these compensation constraints can increase the *tightness* of the matching process under circumstances where the sounds differ perceptually. As the source sounds are compensated, the behaviour of the machine is foregrounded aesthetically and the artifice of that process becomes more evident in the output. Weighting features and controlling the compensation intensity transforms C-C-Combine from an audio effect to an interface for exploring the behaviour of a machine listening algorithm. As Constanzo says in the VIDEO 5, "we specifically *want* quantisation error, and the 'art' is in how much".

<YouTube 
    url="https://www.youtube.com/embed/Ne3wAEzd3rU?start=2304"
    title="Rodrigo Constanzo Creative Coding Lab Symposium Presentation" 
    figure="VIDEO 5"
/>

Because C-C-Combine exists primarily for real-time use I do not use it in my own work. However, the concepts in its design and implementation resonate with other aspects of my creative coding practice. For example, his work on selecting and curating audio descriptors for analysis has both strengthened my knowledge in this area and informed my preferences. Furthermore, reading and engaging with the code for C-C-Combine has taught me how the treatment, sanitisation and compensation processes applied to data can affect further processes such as feature vector matching. In principle, much of this technical research and design can be applied to off-line processing such as in [FTIS]({m.ftis}).

In a similar vein to C-C-Combine, [Ben Hackbarth's](http://www.benhackbarth.com) [AudioGuide](http://www.benhackbarth.com/audioGuide/) is a program for concatenative synthesis that works by matching a selection of source grains to a target sound. While C-C-Combine or CataRT work in real-time, AudioGuide functions in non-realtime allowing more complex matching schemas and exhaustive computational constraints to be used that are not required to be rapidly scheduled ahead of time. Hackbarth (2010) proposes AudioGuide as a solution to his own compositional problems including the labour intensive process of constructing gestural material using samples, and the ability to layer sounds densely such that "evocative" sounds reminiscent of "time-varying" acoustic morphologies can be made (Hackbarth, 2010).

AudioGuide offers an extensive interface for shaping this process, allowing corpus filtering, data transposition, normalisation and deeply customisable constraints for sample matching. This design reflects Hackbarth's vision for how he can resolve his own compositional problems and affords the program a role in constructing long and complex continuous sections of musical material based on such configurations. Hackbarth demonstrates how this interface was used to great effect in the video below in reference to his piece [*Volleys of Light and Shadow* (2014)](http://www.benhackbarth.com/vls/). While AudioGuide requires the user to specify matching and concatenation constraints in an abstract configuration syntax (see CODE 1), this provides a powerful interface where the complexity of superimposition and time-varying matches can be defined and expressed. In this sense, the machine listening component, however complex embedded behind layers of configuration it is, becomes an interface where one can *show* the computer an example and then "guide" the pathway it takes to reach that. In the video below, Hackbarth posits that a program like AudioGuide is not necessarily a new form of tool; rather it resembles a composer testing out harmonic or melodic structures at the piano and then orchestrating them fully later.

```python
TARGET = tsf('cage.aiff', thresh=-25, offsetRise=1.5)

CORPUS = [csf('lachenmann.aiff')]

SEARCH = [
    spass('closest_percent', d('effDur-seg', norm=1), d('power-seg', norm=1), percent=25),
    spass('closest', d('mfccs'))
]

SUPERIMPOSE = si(maxSegment=6)
```

<Caption>
    <p slot="text">
    CODE 1: An example of the configuration interface for AudioGuide
    </p>
</Caption>

<!-- TODO: This is important... (See as above (from Alex)) -->
AudioGuide has directly influenced the technical aspects of my practice and by extension the creative possibilities. This is evident by comparing the architecture of software I programmed at the start of this PhD research, which was Max and real-time based and the later technical developments which are text-based and mostly use the Python programming language in conjunction with a DAW. A significant influence that shaped this trajectory from AudioGuide is the separation of audio segmentation, machine listening analysis and synthesis (concatenation) into discrete stages of a processing pipeline. Dividing each of these stages is not only pragmatic for development and maintenance, it is an advantageous choice that allows the user to hone each aspect in isolation from each other. This is important because each component in this pipeline has an impact on the following component's results. For example, segmentation is often the first step in my practice, and the granularity at which this is performed will affect how recognisable the source material is when composing with those segmented units. As such, separation of different processes becomes a powerful interface for sculpting the overall process and for engaging with sound materials with the aid of the computer.

Additionally, software that operates in non-realtime is conducive for my [iterative development cycle]({m.preoc}#iterative-development) between moulding the computer's behaviour through configuration files and auditioning or experimenting with rendered outputs.  As I developed my own tools such as [FTIS]({m.ftis}) and its nameless Python-based predecessor, this iterative compositional workflow became prominent and opened up possibilities for composition with sample-based material that was difficult or arduous in other environments such as Max. Storing configuration data in files separated from the sonic materials creates a workflow that allows me to step between the mindset of massaging analytical processes and evaluating sonic outputs. This workflow became an important aspect of how [Refracted Touch]({m.rt}) and [Electromagnetic]({m.em}) were composed as well as the workflow that [FTIS]({m.ftis}) aimed to engender.

<YouTube 
    url="https://www.youtube.com/embed/BEwTCZxx9r0?start=414"
    title="Ben Hackbarth presenting AudioGuide"
    figure="VIDEO 6"
/>

### Mapping
Mapping is the process of spatially representing data or for constructing relationships between sets of inputs and outputs such as in digital musical instrument (DMI) design (see Kiefer (2014), De Campo (2014), Hunt et. al (2003) and Tanaka (2010)). Machine listening combined with machine learning can be useful for producing such *maps* automatically.

In [Sam Pluta's](http://www.sampluta.com/biographyText.html) more recent work, machine listening is used to transform non-linear multidimensional parameter spaces of chaotic synths to lower dimensional spaces. Using an [auto-encoder](https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726) that is trained on synthesiser parameters at the input and audio descriptor features at the output, Pluta employs the computer to discover how these two sets of data are related and if there is a simpler representation in terms of perceptual data that can describe the complex and co-dependent connections between the input parameters. This produces an intermediary *map* between the method of control for the synthesiser and the perception of the sound that is made.

If one imagines that all the possible states of the synthesiser are entangled by co-dependent parameters, the combination of machine learning and listening "untangles that mess" and projects it onto a flat surface in which perceptually similar states are positioned around each other and onto a continuous palette. This palette can be traversed linearly, while the internal state remains non-linear. In this way, the listening machine solves a problem that would involve a lengthy period of manual trial and error or familiarisation through practice. It also allows Pluta to explore a highly dimensional synthesis space with a lower number of controls that then are *mapped* onto them.

Pluta presents this research in the VIDEO 7. There is also a [relevant thread in the Fluid Corpus Manipulation discourse](https://discourse.flucoma.org/t/nnsize-question/450/27) that to an extent documents the development of these ideas as they were emerging and opens up some of the questions to a wider internet community for discussion.

<!-- LINK: you need to point to an actual chapter where the maps from reconstruction error are discussed -->
I have used a combination of machine listening and machine learning for similar purposes in my work. In [Annealing Strategies]({m.as}), [simulated annealing (SA)](https://en.wikipedia.org/wiki/Simulated_annealing) is used to discover a configuration of parameters that will cause a [Fourses synthesiser](http://www.ciat-lonbarde.net/fyrall/) to produce the quietest output possible within a given subset of possibilities. As such, SA is used as a mapping technique between the parameters and an audio descriptor that measures the loudness.

<VideoMedia>
    <iframe slot="media" title="Sam Pluta webinar on neural networks, machine listening and chaotic synthesisers."src="https://player.vimeo.com/video/470864627" width="640" height="360" frameborder="0" allow="autoplay; fullscreen" allowfullscreen></iframe>
    <p slot="caption">VIDEO 7: Sam Pluta discussing his work on synthesis parameter mapping using neural networks.</p>
</VideoMedia>

[Thomas Grill](https://grrrr.org) has investigated how digital audio samples can be projected into two dimensional navigable spaces based on audio descriptor features. Two papers related to this are [Grill, T. (2012)](https://grrrr.org/pub/grill-2012-drthesis.pdf) and [(Grill, T., & Flexer, A., 2012)](https://grrrr.org/pub/grill-2012-icmc.pdf). In this work, Grill researched strategies for portraying diverse collections of sounds on visually explorable two dimensional maps. Using custom high-level descriptors such as "high-low", "bright-dull", "ordered-chaotic", "coherent-erratic", "natural-artificial", "analog-digital" or "smooth-coarse", a projection is created that attempts to balance these qualities of the sound and to make the map as continuous and perceptually grounded as possible. While much creative coding work with machine listening is tuned according to the listening perception of the composer, Grill performed surveys and listening tests with subjects to arrive at certain assumptions about the influence that each of these custom descriptors should have on the final projection. Exploring one localised area of the projected space renders perceptually similar sounds together in space. Increasingly different sounds will be found by moving outward in either direction from a single local point. Grill provides an [interactive timbre map](https://grrrr.org/data/research/texmap/) which lets a user navigate across a projection of some pre-prepared sounds.

<ImageMedia2 
    url="https://jbphd-pub.s3.us-west-000.backblazeb2.com/content-awareness/texmap.jpg"
    caption="A preview of the texture map found at https://grrrr.org/data/research/texmap/"
    figure="IMAGE 1"
/>

Thomas Grill's work, in particular his interactive timbre map,inspired compositional processes where sample-based corpora are analysed, mapped and explored. In [Reconstruction Error]({m.re}) and [Electromagnetic]({m.em}) dimensionality reduction techniques are used to project audio samples to a two-dimensional space. The function of this is to discard audio features that do not explain or account for the differences and similarities between the sounds. In a similar vein to Pluta's work, the output can be used to simplify the exploration of possibilities, albeit in the form of samples rather than synthesis parameters.

## Content-Awareness and Augmentative Practice
When I incorporate a computer program that uses machine listening into my practice I am using it to discover something related to the *content* of a sound, whether that sound is sample-based (such as in [Stitch/Strata]({m.ss}), [Reconstruction Error]({m.re}) or [ElectroMagnetic]({m.em})), uses live electronics ([Refracted Touch]({m.rt})) or employs digital synthesis ([Annealing Strategies]({m.as})). Using content-aware programs to *listen* is a process where I alternate between experimenting "hands on" with materials produced by the computer, and devising new listening tasks for the computer. This process is described as the [iterative development cycle]({m.preoc}#iterative-development). This positions the computer in my practice as an explorative and speculative companion rather than a tool to arrive more quickly at a set of prior knowns. It involves a process of question and answer, or "querying" which sits as a central tenet of how I make compositional decisions.

While computers are excellent at solving problems that are well defined, compositional practice often requires solving issues with shifting criteria for success. As I approach a satisfactory solution to a creative idea, aim or goal, it is likely that the answer might change or I might realise the initial question was fundamentally shaped by a perspective that has been challenged through this experience. Therefore, the role of a content-aware program is to foster this compositional process by offering ad-hoc solutions to questions as they arise. Through this process, content-aware programs act as a sounding board and heuristic where the artistic questions that underpin a work can be developed and honed.

<!-- WIDGET: workflow diagram -->
<!--
Idea >> Listening Task >> Results (audio/data) >> evaluation >> incorporation
                ^                                     |
                |                                     |
                ---------------------------------------
-->

I consider this incorporation of content-aware programs in computer-aided composition as *augmentative* in comparison to practices that are *semi-supervised* or *unsupervised*. The next section, ["A Spectrum of Autonomy"]({m.ca}#a-spectrum-of-autonomy) will outline a variety of projects and pieces that have influenced me technologically and musically and that function to position my practice as *augmentative* amongst a body of related work that occupies this spectrum of machine agency in creative practice. While these terms are commonly used in the area of machine learning, in this case I am appropriating to explain the differences between levels of computer agency and autonomy in creative practice.

### A Spectrum Of Autonomy

#### Fake Fish Distribution
An example of a *semi-supervised* approach is [*Fake Fish Distribution*](https://www.icarus.nu/FFD/) by [Icarus](https://www.icarus.nu/wp/). This album "in 1000 variations" is created algorithmically, and each copy of the album differs slightly while sharing some structural features across variants of tracks. Using [Max for Live (M4L)](https://www.ableton.com/en/live/max-for-live/) the standard timeline environment of [Ableton Live](https://www.ableton.com/en/) is extended so that fixed "keyframes" (as explained [in this interview with Icarus](https://www.soundonsound.com/news/icarus-fake-fish-distribution)) are represented as edges of breakpoint functions. More detail is provided in [another interview between Icarus and Marsha Vdovin](https://cycling74.com/articles/an-interview-with-icarus).  See IMAGE 2 for a depiction of the breakpoint function module they used to interpolate between keyframes. Furthermore, see VIDEO 8 for an explanation of how this system was implemented in Ableton Live.

<ImageMedia2
url="http://1cyjknyddcx62agyb002-web-assets.s3.amazonaws.com/interviews/icarus/BreakpointFunctionEditor.jpg"
figure="IMAGE 2"
caption="An example of a breakpoint interpolation scheme between keyframes"
/>

Interpolating between these keyframes provides variation in between known points, so a limited amount of material can be manually composed and further iterations of that material are derived computationally by interpolating along the breakpoint functions between the fixed points. As an external observer, it is not completely transparent to what extent the computer is responsible for deciding how far to interpolate and when to do this. However, the important facet of this generative strategy is that some immediate control over the shape and structure of the music is given away to the computer to produce. In the first interview, they describe a process of drawing shapes into the breakpoint editor in order to guide the result of that process. As such, this approach is *semi-supervised* as they control the computer and its agency is manifest in a process which is ultimately humanly managed.

<VideoMedia>
<iframe slot="media" src="https://player.vimeo.com/video/38404732" width="640" height="360" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p><a href="https://vimeo.com/38404732">Icarus - Fake Fish Distribution</a> from <a href="https://vimeo.com/n074ppl1c48l3">not applicable</a> on <a href="https://vimeo.com">Vimeo</a>.</p>
<p slot="caption">VIDEO 8: Icarus (Ollie Bown and Sam Britton) in a brief interview about Fake Fish Distribution</p>
</VideoMedia>
<!-- Designing Variation -->

#### S.709 - Iannis Xenakis
Another *semi-supervised* approach can be found in the compositional method for [Iannis Xenakis' piece *S.709*](https://www.iannis-xenakis.org/xen/works/work_135.html). 

<YouTube 
    url="https://www.youtube.com/embed/Zmi2EVb7kQU"
    title="Iannis Xenakis - S.709" 
    figure="VIDEO 9"
/>

This piece followed a long series of works that experimented with "dynamic stochastic synthesis" (to which a chapter is devoted in [Formalised Music](https://monoskop.org/images/7/74/Xenakis_Iannis_Formalized_Music_Thought_and_Mathematics_in_Composition.pdf) (Xenakis, 1992), including pieces such as *"La Légende d'Eer"* (1977-1978) and *GENDY3* (1991). *S.709* uses a combination of two programs, *GENDY* and *PARAG* to construct the work with a blend of human and machine agency. GENDY generates waveforms sample-by-sample using stochastically bound random walks. These waveforms are algorithmically determined with little preconception of their sonic result from the information and constraints alone. Despite this, GENDY is a strategy for "encompassing all possible forms from a square wave to white noise" (1992, p. 289). For him the challenge is to create music "from a minimum number of premises but which would be of 'interest' from a contemporary aesthetical sensitivity" (1992, p.295). It is not as if there is imagined sonic outcome from this approach beforehand; in fact conforming to a "contemporary aesthetical sensitivity" perhaps drives the extremely exhaustive computational implementation itself. PARAG works alongside GENDY by concatenating several GENDY outputs to build a larger formal structure. Xenakis (1992, p. 296) states that S.709 is named after a selected sequence output from PARAG - a single rendering from the system among many possible ones.

For *S.709*, the computer is engendered with a relatively autonomous role of artistic contribution to the work. Xenakis' own responsibility is to compose the program, encoding his aesthetic desires in a textual format, only being able to interfere by altering the algorithm, the seed of all things built from the sample level, and perhaps to a greater extent through the arrangements made by the PARAG program. A work such as this is highly parametric like Fake Fish Distribution and even more so tends towards a fully autonomous or *unsupervised* mode of composition in which many aspects of composition are given away to the computer to be executed beyond the actions of the composer, only being available for modification from a distanced perspective or through curation of artefacts resulting from the process. Xenakis retains the final say in this work, choosing the "best" output from the computer rather than perfecting a single master rendition that is unaltered after it has been generated. I engaged with a similar process of curation in [Annealing Strategies]({m.as}), where the computer was tasked with making many variants from which I selected the most suitable output. Xenakis reflects on this choice to curate the machine output in his conversations with Balint Andras Varga: 

<div class="bigquote">

*Other composers, like Barbaud, have acted differently. He did some programs using serial principles and declared: 'The machine gave me that so I have to respect it'. This is totally wrong, because it  was he who gave the machine the rule!* (Varga, 1996, p. 201 )

</div>

#### David Cope and Experiments in Musical Intelligence
At the furthest end of the *unsupervised* scale artists prefer to leave computer generated audio rendering untouched, deferring all responsibility to the computer and perhaps even attributing the artistic merit to the program itself. In this area, David Cope's work on "Experiments in Musical Intelligence" (EMI) affectionately known as "Emmy", is well known in the literature. This program was designed initially as a compositional aid to Cope, helping him break through creative blocks by generating material and allowing him to select outputs interactively. Cope soon realised that for a program to operate like this there had to be a model of his own compositional style which in the earliest stages of EMI was not conceptualised. Instead of modelling his own style, he started to analyse and codify existing music, particularly that constructed through well researched sets of rules and patterns. As a result, Emmy become a program for analysing MIDI representations of music from the baroque to romantic era and generating new works modelled on the styles of several composers from that period. 

<!-- TODO: redo the summative staement for this piece or get rid of it -->
EMI represents a remarkable compositional and artistic approach, in that much of the attribution of the works is to "Emmy", an anthropomorphised machine, Cope often referencing her ["compositional abilities"](http://artsites.ucsc.edu/faculty/cope/5000.html) in descriptions of the work. Once the program is designed and the outputs of the machine are satisfactory, Cope transitions into the role of a career manager for the program, choosing what to officially release and tending to the portrayal of that program as an artist rather than constantly delving back into the studio or into the code to work in a creative capacity. 

#### Daniele Ghisi - La Fabrique des Monstres
A contemporary example of an *unsupervised* approach can be found in [*La Fabrique des Monstres*](https://www.danieleghisi.com/works/la-fabrique-des-monstres/), a suite of computationally generated by Daniele Ghisi. These pieces use [SampleRNN](https://github.com/Unisound/SampleRNN) (see Mehri, S (2016)), a recurrent neural network that is trained to model the dependencies between samples in digital audio.  This can be used generate new audio samples based on the assumption that the digital samples can be represented probabilistically. This process is relatively uncontrollable. One cannot intervene with the intermediary compuational processes to understand what the computer is doing or what it is specially modelling from the input. Unfortunately Ghisi gives few details on his implementation such as what the training corpus was. Nonetheless, we can garner an appreciation and feeling of what the computer has learned from the results it has generated. 

For example in *FuenfteLetzeLied* and *PasVraimentHumaine*, vocal gestures are artifically extended and elongated by the computer to the point where in some cases no human could sing these passages. My interpretation from listening to these works is that part of the training data must be song cycles or operatic material. What the computer has "learned" from this material is not necessarily stylistic or contextual, rather, it appears to have latched onto the most prominent surface elements of the music and decided to proliferate that. In this case it is the dominating solo voice. At times in *Recitation*,  structures and shapes similar to comprehensible sentences emerge  from  rapid phonetics that are shuffled and somewhat random. Throughout other passages seemingly sophisticated, although not understandable phrases are strung together without any breaks. This gives a sense that there is some order and patterning to the individual grammatical elements of the machine-generated language being spoken which is simultaneously subverted by the lack of overall structure imposed on to them.

Throughout these works it seems as if the computer learns *some* significant aspects from the source material. It is clear when a vocalist has been present in that training data, but often what the computer learns from this is purely statistic and fails to engage with the aspects of a work which are delicate, nuanced and central to our understanding of them as human . This produces results that embody a "machine understanding" of the source material that is then aestheticised in the final work. Ghisi exposes this which positions the computer as an *unsupervised* creative force.

### Situating My Practice
A range of practices have been described in ["Content-Awareness and Augmentative Practice"]({m.ca}#content-awareness-and-augmentative-practice) demonstrating different degrees of computer autonomy exercised in creative practice. What binds these diverse approaches together is the way that computation *leads* artistic investigation and a dialogue is created between the human and the machine.

For example, Fake Fish Distribution is densely parametric, requiring human input for auditioning, tweaking the breakpoint functions and keyframes and further auditioning to get the computer to produce acceptable outputs. Throughout this process the underlying mechanisms are accessed directly, allowing the composer to interface with the generative aspect by interpolation. Much of the compositional work is *embedded* into that process when deciding on ranges for parameters, trajectories of automation and mapping parameter values to musical controls. In comparing to Ghisi's practice, compositional effort and work is invested into training sampleRNN model that will generate musical sound without much ability to go backwards and to adjust any set of dials, knobs, sliders or constraints that would otherwise affect the output. Cope's and Xenakis' work could be placed close to Ghisi's and toward the "relinquishing of full control" and of the spectrum, however they retain curatorial powers over the technological aspect of what they are doing.

<!-- TODO: reference back to LCGI, wherever that ends up being mentioned -->
As such, I define my own practice as *augmentative* computer-aided composition using content-aware programs. The computer is a co-pilot in the creative process and not just a tool that executes tasks for me. It has the ability to help me in ways that are not possible with my faculties alone and it is not just a way of making existing processes more efficient. Content-awareness in my programming practice shapes the role of this process in a novel way, placing the emphasis of their function on *listening* and producing outputs related to that listening. By harnessing content-aware machines for computer-aided composition, my aims are not to investigate formalist approaches where entire works emerge from algorithmic, rule-based or procedural; rather, the computer has a mediating role between me and creative action: it is a lens that is focused on a set of compositional ideas, materials and strategies that are then acted out cooperatively. My practice is a fusion of human and machine in composition, grounded in machines that listen in order to compose *with* me.

## Concluding Remarks

In the previous two sections, ["Preoccupations"]({m.preoc}) and ["Content-Awareness"]({m.ca}) a number of contextualising concepts have been put forward that situate my practice within the wider field of computer-aided composition and highlight the ways that content-aware technology is a significant component of my practice. The next section, ["Projects"]({m.proj}) is comprised of five subsections. These subsections discuss five compositional works that were made throughout this PhD research and that explore the notion of content-aware computer-aided composition in practice.

Before moving on to the discussion of these individual projects I will posit a number of "postures" as concluding remarks of the previous two sections and as a way of framing the discussion of the next five subsections.

1. My practice is computer-aided composition and uses content-aware programs as a *interface for expression*. This interface is embedded in an [iterative development]({m.preoc}#iterative-development) process that helps initially seeded concepts reach internal consistency by engaging with sound materials through machine listening and computer-aid.

2. Compositional materials are discovered through deconstructing pre-existing sound objects (recordings, synthesis). My practice leverages the strengths of content-aware programs to sieve, filter and search collections of pre-existing materials or to have the computer propose notions of "material groups" or formed sections of music derived through algorithmic and parametric means. Content-aware programs *structure listening* and engagement with materials, providing a strategy for meaningfully discovering, understanding and then incorporating those materials into composition. 

3. As such, my practice is initialised in a process of *extraction*, one that I assert as oppositional to *constructing* which comes much later once sufficient materials have been extracted. Metaphorically the difference might described as that between going for a walk and digging for oil. The walk has a notion of a prepared path but diversions can be made along the way depending on what is discovered or observed in the moment. For the walk there are both "known knowns" and "known unknowns". and this process invites both. Digging for oil on the other hand tends towards the observation of "known knowns" and represents a process of refinement to achieve a more tangible goal. This is ultimately speculative. However, there is a well defined goal to work toward.

<NextSection 
next="Projects"
link={m.proj}
/>

<script>
    import Caption from "$lib/components/Caption.svelte";
    import VideoMedia from "$lib/components/VideoMedia.svelte"
    import ImageMedia2 from "$lib/components/ImageMedia2.svelte"
    import YouTube from "$lib/components/YouTube.svelte"
    import HPSS from "$lib/demos/HPSS.svelte"
    import NextSection from "$lib/components/NextSection.svelte"
    import {metadata as m} from "./directory.svx"
</script>

<style>
    h1 {counter-reset: h2}
    h2 {counter-reset: h3}
    h3 {counter-reset: h4}

    h1:before {content: "3." " "}

    h2:before {
        content: "3." counter(h2, decimal) " ";
        counter-increment: h2;
    }
    h3:before {
        content: "3." counter(h2, decimal) "." counter(h3, decimal) " ";
        counter-increment: h3;
    }

    h2.nocount:before, h3.nocount:before {
        content : "";
        counter-increment: none;
    }
</style>

<!-- Where does the conceptual 'hop' occur from machine listening to content-awareness? The difference is not huge nor is it mutually exclusive, it does not involve some complex machine learning that  that somehow makes the computer *magical* or able to identify the *content* of a signal through black-boxed and mysterious means. A content-awareness machine can be as simple as a compressor that only switches on above a certain loudness threshold. There are several patents - such as ["Content aware audio ducking"](https://patents.google.com/patent/US9536541B2/en) ["Content aware audio modes"](https://patents.google.com/patent/US9578436B2/en) - for digital signal processing schematics that describe themselves as "content-aware" and have little to do with the kind of compositional applications described above or represent even a fraction of the complexity involved with the machine listening designs found in those works. There are directly relevant examples such as a content-aware granular synthesiser found in ["Smooth Granular Sound Texture Synthesis by Control of Timbral Similarity"](https://hal.archives-ouvertes.fr/hal-01182793/document). While granular synthesis is often implemented as a stochastic process that selects grains of audio with a degree of controlled randomness, the approach detailed here proposes a grain selection procedure that is based on the content of the grains. [MUSESCAPE](https://www.researchgate.net/profile/George_Tzanetakis/publication/2884094_Musescape_An_Interactive_Content-Aware_Music_Browser/links/0912f50f72395c4cef000000.pdf) describes itself as a 'content-aware music browser', that exceeds browsing possibilities using titles, genres, artists and provides mechanisms for exploring sound based on their content. -->

<!-- Content-awareness is a capability or state that is achieved when a system is able to capture, interpret or do something useful with the content of a signal. This supports a paradigm where the computer might contextually switch between modes of operation, support and hone other analysis with additional awareness of content information and be adaptable based on what it is receiving as an input. For me, a program is content-aware when it can help me perform analysis or compositionally meaningful tasks with a degree of sensitivity to the what is in the digital audio samples and can somehow mirror my musical listening which is primarily oriented around their perception as textural sounds. This means content-awareness is about engendering the computer with listening capabilities that are centred around *my* human listening which is an integral part of how I compose. -->

<!-- Content analysis for classification and segmentation https://ieeexplore.ieee.org/abstract/document/1045282 -->
<!-- CONTENT-BASED AUDIO SEGMENTATION USING SUPPORT VECTOR MACHINES | http://www.cbsr.ia.ac.cn/Li%20Group/papers/ACM-03.pdf -->
<!-- https://www.sciencedirect.com/journal/design-studies/vol/41/part/PA | Computational Making -->
<!-- When making becomes diviation | https://www.sciencedirect.com/science/article/abs/pii/S0142694X15000587?casa_token=Lso0O_4zaMcAAAAA:P3cBH8NRp6GGyy5hIf2iWYj84KuUpUEy93ZqAVutpYK8hgLxZM6y0HiXSzPknfIQSphNujNN -->

<!-- Content Based Segmentation | http://www.cbsr.ia.ac.cn/Li%20Group/papers/ACM-03.pdf -->

<!-- AUTOMATIC SEGMENTATION, LABELLING, AND CHARACTERISATION OF AUDIO STREAMS http://www.ofai.at/research/impml/projects/audiostreams.html -->

<!-- Content-aware programs aspire to produce meaningful interpretations of sounds through the production of data, such as that from audio descriptors, and the analysis of that data, through techniques such as machine learning or statistical analysis. Content-aware software for sound is technically supported by machine listening or 'computer audition' (CA), an area of research that aims to design and implement methods for engendering machines with 'listening' faculties.Real-world applications of this are evidenced in technology such as voice recognition, [enhancement in cochlear-implants](https://ieeexplore.ieee.org/abstract/document/7472933?casa_token=l2pItnm716EAAAAA:QzkuGOcePL5jrEjyBKtdXudPmJASduflGoNViDuKXSosySxkLN9xluV4B3TKV5OTfiJoHmicbI4) or [quantifying the biodiversity of dense natural environments](https://peerj.com/articles/2108/). Since the idea of a computer 'hearing' is relatively murky, machine listening draws on various algorithms and approaches, sometimes attempting to replicate the way that humans analyse and organise sound implicitly while also stepping into more abstract territory removed from a biomemetic frame around listening.

Content-aware programs draw on various strategies from CA for making sense of sounds computationally, and ultimately the design of these programs is inflected by the user, the content they are analysing, their biases and their preferences for what is analytically meaningful and how that information will be accessed and used for further purposes. While content-aware programs draw on technology and algorithms from machine listening, 'content-awareness' goes beyond producing analytics and aims to provide insight to the user, sensitive to the perception of the content from a human perspective, whatever that perspective may be. As such, it is highly concerned with involving a 'human in the loop', producing meaningful analysis and outputs that are integrated with the way that an individual perceives the materials that are analysed and what they are trying to find through the faculties of the computer. -->

<!-- /////////// QUESTIONS THAT MAY GO IN THE COMPOSITIONAL PREOCCUPATIONS -->

<!-- 
In practice, this content-awareness revolves around the idea of "querying", a concept I propose which positions my application content-aware systems as one where I ask listening machines different questions or propose it scenarios where I require an interpretation or answer. Using these "responses" as compositional stimulus I am then able to either initialise or develop creative ideas. These "queries" differ in nature and can be asked at different time points in the compositional process. Overall though, they characterise a process where I bounce back and forth between experimenting, "hands on", with digital sonic materials, and proposing listening task expecting a new output. Examples of broad queries and propositions are:

- **Organise** a group of samples linearly in time by a descriptor
- **Decompose** this sound into the most distinct set of components
- **Find** the closest sound to this sound
- **Project** this group of sounds into a two dimensional visual space for exploration
- **Segment** this sound into *n* discrete parts
- **Discover** how *this* parameter space maps to *this* sonic feature space, i.e learn by examples. 
-->

<script>
    import {metadata as m} from '../directory.svx'
    import NextSection from '$lib/components/NextSection.svelte';
    import VideoMedia from '$lib/components/VideoMedia.svelte';
    import VideoMedia2 from '$lib/components/VideoMedia2.svelte';
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
    import YouTube from '$lib/components/YouTube.svelte';
    import Album from '$lib/components/Album.svelte';
    import Code from '$lib/components/Code.svelte';
    import Waveform from '$lib/components/Waveform.svelte';
    import Points from '$lib/demos/dimension-reduction/Points.svelte';
    import Agglom from '$lib/demos/clustering/Agglom.svelte';

    import { 
        reAlbum, ibuffer, generation, 
        libllvmSlices, noveltySlices, segmPoints, sysPoints, 
        isnSegments, isnPoints, descPoints,
        maxwaveSegs, maxwave
    } from '$lib/data/reconstruction-error.js'
</script>

# Reconstruction Error

<Album 
tracks={reAlbum}
title="Reconstruction Error"
id="audio1"
figure="AUDIO 1"
/>

Reconstruction Error is the fourth project in this portfolio. It is a set of five pieces, presented in an EP format.

There is also a git repository which contains all of the Python code and Max patches which were used. This can be found hosted on github at: https://github.com/jamesb93/reconstruction_error.

## Motivations and Influences
In the previous three projects, I explored a number of techniques, approaches and workflows for computer-aided composition with content-aware programs. Each of these projects were motivated by a combination of factors both musical and extra-musical, such as opportunities to compose for visiting artists or by responding to immediate influences in my creative coding practice and musical listening. Reflecting on the previous projects up to this point, the most successful workflow was developed in [Stitch/Strata]({m.ss}). Despite being a difficult compositional process that involved many failed attempts to reach the finished state of the piece - using the computer to sieve through a corpus of sounds and *offer* up small material groups was a fluid and expressive workflow. Furthermore, agency was balanced between myself and the computer. I decided to move forward with this in mind and aimed to create a piece by using the computer in a similar fashion.

### Databending
Amongst this reflection on my practice up to this point, I had been listening to artists such as [Alva Noto](http://www.alvanoto.com/), [Autechre](https://en.wikipedia.org/wiki/Autechre), [Opiate](https://en.wikipedia.org/wiki/Thomas_Knak) and [Ryoji Ikeda](https://www.ryojiikeda.com/). Their work draws on hyper-digital, surgical and sometimes ultra-minimalist sonic materials associated with post-digital aesthetics. The raw digital sounds found in this sphere of music has always been appealing to me, and a part of my compositional vocabulary and aesthetic. [Stitch/Strata]({m.ss}), while using entirely organically generated vocal sounds, engenders a strong machine and digital aesthetic by the way that sounds are arranged and processed. Similarly, the signal processing modules in [Refracted Touch]({m.rt}) at times conceal the "human" elements of Daryl's playing, and transform sound materials that he generates into synthetically altered versions of it.

I was also becoming interested in databending, a process in which raw data is used to construct visual and sound materials. This can be achieved several ways, and the nature of this process itself can have a significant impact on the results. A number of existing tools facilitate databending either intentionally or through divergent and unintended application. Examples include [SoundHack](https://www.soundhack.com/) which allows one to convert any file on a computer into a valid WAVE audio file, [Audacity](https://www.audacityteam.org/) which supports the reading of raw bytes as audio data, or [PhotoShop](https://digital-photography-school.com/make-abstract-glitch-art-photographs/) which can open non-image files as if they were images. 

A number of artists have experimented with databending in their work and this guided my research into this digital practice. [Nick Briz](http://nickbriz.com/), a "new-media artist"  explores various "attacks" on video data to degrade and produce distortions in *Binary Self-Portraits* (see [VIDEO 1]({m.re}#vid1)). Another Briz's works, *From The Group Up In Order, Embrace* (see [VIDEO 2]({m.re}#vid2)), explores the creation of visual material through the interface of the binary code that represents itself. Briz states: 

<div class="bigquote">

*Anything that is digital contains beneath it layers of code and at the base is an immense series of ones and zeroes known as binary code. Like the atom in the natural world, the ones and zeroes of binary code are the smallest component of the digital medium. In my efforts to strengthen my relationship with the digital medium I set out to create a piece where I dealt directly with the binary code of a video file. The result was this video as well as an obsession with the digital medium, and a first in a series of binary video pieces.* (Briz, n.d )

</div>

<VideoMedia id='vid1'>
<iframe slot='media' src="https://player.vimeo.com/video/4510943" width="640" height="524" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot="caption">VIDEO 1: Binary Self-Portraits by Nick Briz</p>
</VideoMedia>

<VideoMedia id='vid2'>
<iframe slot='media' src="https://player.vimeo.com/video/4506517" width="640" height="524" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot="caption">VIDEO 2: From The Ground Up In Order, Embrace by Nick Briz</p>
</VideoMedia>

*Unitxt Code (Data to AIFF)* (see [VIDEO 3]({m.re}#vid3)) by [Alva Noto](http://www.alvanoto.com/) although not explicitly stated in liner notes or interviews, likely uses data converted to audio (as suggested by the name) for the core compositional material of that work. The harsh and degraded noise sounds are commonly produced when converting text files into audio. *[Datamatics](https://www.ryojiikeda.com/project/datamatics/)*  (see [VIDEO 4]({m.re}#vid4)), by[Ryoji Ikeda's](https://www.ryojiikeda.com)  incorporates raw data as source material which drives both the visual and audio of this installation work. [Pimmon's] album *Electronic Tax Return*  (see [VIDEO 5]({m.re}#vid5)) has several works based on synthesising sound by converting dynamic link libraries (.dll files) into files for playback and manipulation.

<YouTube 
url="https://www.youtube.com/embed/vN9FrqrPptg"
figure="VIDEO 3"
caption="Unitxt Code (Data to AIFF) by Alva Noto"
id='vid3'
/>

<YouTube 
url="https://www.youtube.com/embed/eaIrSZIyxxk" 
figure="VIDEO 4"
caption="Datamatics (Limited Section) by Ryoji Ikeda"
id='vid4'
/>

<YouTube 
url="https://www.youtube.com/embed/AbF8NFanUdQ"
figure="VIDEO 5"
caption="Electronic Tax Return by Pimmon"
id='vid5'
/>

These works of sound art and composition were part of my exposure and research into databending. In particular, artist [Michael Chinen's](https://michaelchinen.com/) practical experiments were instrumental in drawing me into a "hackers" approach through the creation of custom software. Chinen's work almost exclusively focuses on the sonification of  data from random-access memory (RAM). One of this works, [FuckingAudacity](https://michaelchinen.com/music/fuckingaudacity/) is a fork of the popular audio application Audacity. The sound output from his fork is generated by using the memory state of the program as audio data for the system output buffer at regular intervals. Interactions with the user interface and user-driven processes such as importing files or enabling effects themselves become ways of manipulating this the sound output. [VIDEO 6]({m.re}#vid6) is a demonstration of Chinen playing with the program.

<VideoMedia id='vid6'>
<iframe slot='media' src="https://player.vimeo.com/video/8144372?byline=0&portrait=0" width="640" height="480" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot='caption'>VIDEO 6: FuckingAudacity demonstration by Michael Chinen.</p>
</VideoMedia>

[FuckingWebBrowser](https://michaelchinen.com/music/fuckingwebbrowser/) (see [VIDEO 7]({m.re}#vid7)) follows in suit with its Audacity counterpart. The application presents as a standard web browser. However, the memory state of the programÂ is used to generate audio data which is then played back as the user interacts with the software. Navigating to or from a website or interacting with a page generates sounds in unpredictable ways. Even doing nothing at times reveals the hidden internal workings as sounds. Making http requests, polling for data or caching files in the background result in sounds that are not tightly coupled to the user's interaction with the application.

<VideoMedia id='vid7'>
<iframe slot='media' src="https://player.vimeo.com/video/9635993?byline=0&portrait=0" width="640" height="480" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot='caption'>VIDEO 7: FuckingWebBrowser demonstration by Michael Chinen.</p>
</VideoMedia>

The logical conclusion to this suite of custom databending applications is one that can be attached to any program. [lstn](https://michaelchinen.com/2010/12/09/listen-to-your-computer-lstn/) performs this function, and can observe and sonify the opcodes, callstack and active memory of any program on the computer. [VIDEO 8]({m.re}#vid8) is a demonstration of this in practice where it is attached to several different running programs in real-time.

<VideoMedia id='vid8'>
<iframe slot='media' src="https://player.vimeo.com/video/17617200?byline=0&portrait=0" width="640" height="480" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot='caption'>VIDEO 8: lstn demonstration by Michael Chinen.</p>
</VideoMedia>

### Employing The Computer To Structure A Corpus
Coming into contact with Chinen's work was influential because there is a mixture of predictable and unpredictable behaviours exhibited by the computer through his various databending programs. The overarching character of the generated sound is digital, harsh and noise-based but occasionally these processes produce silence as well as sounds which are fragile and delicate. This notion of there being "needles in the haystack" was a tension that I wanted to explore myself. How could I generate a large body of material and search for the "needles"? Part of my motivation for this project was to create a set of conditions in which I could explore this, and have that investigation be mediated with the computer in the compositional process.

As part of my preliminary research toward these motivations, I came into contact with the outputs of the Fluid Corpus Manipulation project. I had up to this point been loosely involved with some of their research activities and privy to internal workshops with their commissioned composers as a helper. As such, I was able to view a pre-release video demonstration for *[FluidCorpusMap](https://github.com/flucoma/FluidCorpusMap)* (see [VIDEO 9]({m.re}#vid9)).  Neither the paper (see Roma et. al. (2019)) or source code for this program were released at the time which hindered my ability to understand the software in a detailed way. However, the video broadly points out that it is using  mel-cepstrum frequency coefficients (MFCCs) audio descriptor analysis, statistics summaries and dimensionality reduction algorithm to create a two dimensional latent space for exploration and navigation. I found two aspects of this process fascinating. Firstly,  the *global* visual aspect of the dimension reduction were novel and possessed unique characteristics depending on the algorithm. For me each layout was evocative, in that the computer seemed to be creating its own machine-like understanding of the corpus. Secondly, the *local* distribution of samples was perceptually smooth and coherent. Movements between adjacent points in the demonstration returned sounds which were texturally similar. Even aspects such as morphology seemed to be captured (this can be seen around 1:19 where sounds with upwards glissandi are grouped together) giving the sense that the computer could listen to texture *and* morphology. This to me, was far more sophisticated than my previous experiments using instantaneous statistical summaries such as in [Stitch/Strata]({m.ss}).

<VideoMedia id='vid9'>
    <iframe slot='media' src="https://player.vimeo.com/video/314301724?byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
    <p slot='caption'>VIDEO 9: FluidCorpusMap demonstration.</p>
</VideoMedia>

In my previous projects, searching for sounds within collections has been of interest to me, albeit using different technologies and analytical approaches than *FluidCorpusMap*.  For example, the compositional building blocks of [Stitch/Strata]({m.ss}) were produced by selecting samples from a corpus by audio descriptor query. Similarly, the simulated annealing algorithm searches through a complex latent space in which a single parametric input is coupled to an audio descriptor output. Both of these approaches are based on calculating audio features with descriptors and then using those features as constraints or selection criteria in order to filter samples from a corpus. 

*FluidCorpusMap* presents its own solution to such a problem, positioning the interactive relationship between user and computer as one where the human interpret the machines portrayal and decomposition of the latent space. Rather than having to find a suitable way of representing the relationship between sounds through several audio descriptors, the dimension reduction process attempts to figure this out automatically and offers it back to the user in a compressed and visual form. To me, this seemed like it might be more fruitful in speculatively navigating through unknown sound collections and then having the computer structure my engagement with sonic materials. With that in mind, I moved forward into the practical part of this project. This is discussed in ["Initial Technological Experimentation"]({m.re}#initial-technological-experimentation).

## Initial Technological Experimentation
This section outlines the technology and development  of software situated at the start of the composition process. It outlines five key stages of a processing pipeline, beginning with the generation of a corpus using databending techniques.

### Generation
I began composing [Reconstruction Error]({m.re}) by generating a corpus of materials using databending techniques. Ultimately, I wanted to create a large corpus of sounds that I would initially have little understanding of. From that point, I intended to use machine listening to structure my investigation of those materials, formulating the piece as a hybrid of my own and the computer's agency.

I began databending by modifying the code of Alex Harker's [`ibuffer~`](https://www.alexanderjharker.co.uk/Software.html) so that it could load *any* file, not just those intended to store audio data for playback. Reading audio files such as WAVE and AIFF typically involves parsing a section of bytes at the start of the file, the "header", which tells the program how to interpret the structure of another chunk of data which is the audio itself. My forked version of `ibuffer~` operated by adding a new method to the object, `readraw` which could read a file without looking for the header, and instead assumed all the data of that file was 8-bit encoded 44.1kHz sample-rate mono audio. Some of the conversions from this process can be found in [AUDIO 2]({m.re}#aud2). Note that these samples have been reduced in volume by 90% as the originals were excessively loud.

<Album 
tracks={ibuffer}
title="ibuffer~ databending"
id="aud2"
figure="AUDIO 2"
/>

Using the fork of `ibuffer~` rendered results that gave me confidence in the databending approach. Many of the outputs in my opinion were unremarkable noise (*AEC.pdf*, *carl.png*), while some presented novel morphological and textural properties. For example, *clickers.rpp* and *crash.rtf*,  although containing a noticeable background noise element were ornamented with complex and *intentional* sounding sounds. Despite this mild initial success, there were two issues which hindered my progress. Firstly, my fork of `ibuffer~` would occasionally crash, causing work to be lost. In addition to this. the conversion process was not deterministic and could be slightly different between consecutive executions with the same input file. This was due to my lack of familiarity in C programming. Secondly, the workflow of converting a file and listening to it was slow because I had to manually load the file and play it back to evaluate the results. Combined with the having to seek out more and more files to test, the repetitiveness of this process was time consuming. These two problems together created a lot of friction when trying to explore the results of databending. 

In response to this,  I created a command line script using a combination of Python and the [SoX (Sound eXchange )](http://sox.sourceforge.net) command-line tool. This combination of Python and Sox formed a powerful command line scripting tool for automatically converting batches of files into audio files. Instead of having to manually select files, load them into Max, audition the output and then save the result, I could point this script to a starting directory from which it would "walk", converting any file it encountered into audio. The script for this can be found in the git repository at: [scrape_files.py](https://github.com/jamesb93/reconstruction_error/blob/master/python_scripts/scraping/scrape_files.py). 

<!-- DELETE and just skip to mosh? -->
While this scripting workflow was rapid and could produce numerous files almost instantaneously, I wanted to understand how the conversion process worked at a lower-level and attach parameters to this process which would let me alter the data stored in the output audio's header section. This would allow me to manipulate the perceptual qualities in some fixed ways for the databending process by modifying the bit-depth, sampling rate and number of channels. SoX allows the user to supply parameters such as this. However, I wanted to take ownership of the technology behind this and program it myself. To meet these demands, I built *[mosh]({m.mosh})*, a command line tool for converting any file into an audio file. More specific information regarding implementation and design can be found in the ["Technical Implementation"]({m.ti}) section. 

The development of [mosh]({m.mosh}) provided me with an opportunity to learn more about the structure of data in audio files. From this, I began to grasp the effect that the header could have on the perception of the sound output. For example, different integer bit-depth representations can drastically alter the textural and dynamic properties. There is no predictable way to anticipate these either, so trial and error is required to determine the results. Single precision floating-point 32bit encodings in particular can produce a mixture of results, often tending towards outputs comprised of audio samples that numerically reach theoretical limits. While integer representations equally distribute the values between -1 and 1, part of the bits which make up a floating-point number determine the exponent. In normal operation, these exponents would be very low for audio because the range is typically constrained between -1 and 1. However, because exponents are exponential there is a high degree of chance that the exponent value will create a chunk of data representing a large number. Furthermore, because multichannel audio data in WAV files is interleaved, databending raw bytes into a stereo audio file can produce unanticipated stereoscopic perceptual effects.

<!-- REVIEW the colloquail speak here - needs tigthening  -->
This was my initial engagement with databending and I soon realised with this tool I had made I would be able to generate massive amounts of material quickly and with little effort. I decided to constrain my use of parameters in the generation stage and only used [mosh]({m.mosh}) to create 8bit unsigned integer encoded files. To me these sounds we're the least prone to being "unremarkable noise", and did not suffer the issues of high resolution encodings. I started running it on directories of files on my computer. I put it into practice by converting 5 gigabytes of raw files by walking from the root of my laptop's file system.This produced 8517 audio files which I ambitiously began auditioning manually. Given the sheer quantity, I only managed to audition around 100 audio files before I realised that I was not able to remember what the first one sounded like, or what characteristics of it in I may have found satisfactory. Amongst the typically noisy sounds there were those with diverse morphologies and textures - precisely the type that I wanted to discover more of in the first place. A small selection of manually auditioned samples were kept aside and be found below in [AUDIO 3]({m.re}#aud3). Clearly, there was going to be a problem navigating through this collection though due to its scale, and at this stage I had produced my "haystack" and began to think about ways in which I could find the "needles" within it.

<Album 
tracks={generation}
figure="AUDIO 3"
caption=" hmm "
id="aud3"
title='Samples generated using the mosh command line tool'
/>

### Segmentation
Samples generated with [mosh]({m.mosh}) ranged in duration from 1 second to 53 minutes. The longer sounds especially were comprised of several discrete musical ideas, gestures or even what may have been considered sections of whole music.  A sample that exemplifies this finding is  *libLLVMAMDGPUDecs.a.wav* (see [AUDIO 4]({m.re}#aud4)). I have manually segmented this sample to demonstrate where I perceived such separable divisions.

<Waveform 
title='libLLVMAMDGPUDesc.a'
caption='AUDIO 4'
file='/re/generation/libLLVMAMDGPUDesc.a.mp3'
peaks = '/re/generation/libLLVMAMDGPUDesc.a.dat'
id='aud4'
segments={libllvmSlices}
/>

---
These longer sounds, comprised of many internal sounds, were not compositionally useful to me. I wanted access to the segments within each sample in order to operate on small homogenous units rather than long composite blocks of material. 

In response to this I devised a strategy to automatically segment the entire corpus based on replicating my intuitive segmenting of *libLLVMAMDGPUDesc.a*. After experimenting with several algorithms from Librosa (McFee et al. 2015), such as [Laplacian Segmentation](https://librosa.org/doc/latest/auto_examples/plot_segmentation.html) I came to the conclusion that the demarcations I had made were influenced by my sensitivity to differences in the spectrum over time.

There are algorithms for performing automatic segmentation based on this principle, such as measuring the *spectral flux* between windows of contiguous spectral frames. I experimented with this using the FluCoMa `fluid.bufonsetslice~` object. However, I found that while this algorithm was intuitive to understand it was highly sensitive to parameter changes and often would be too detailed in the result with a low threshold or miss significant changes with a high threshold.

Similarly, the Laplacian Segmentation algorithm was beyond my understanding to use effectively, despite showing promising results in other applications. While these attempts seemed like dead ends at the time, they were encouraging me towards more fruitful results. The documentation of the Laplacian Segmentation algorithm would often refer to self-similarity or recurrence matrices. I used part of the code from this documentation to create one for *libLLVMAMDGPUDesc.a*, and interpreted the structure of this matrix visually to be reflective of the demarcations that I had made manually before. The recurrence matrix plot be seen in [IMAGE 1]({m.re}#img1). 

<ImageMedia2 
url='/re/recurrence.png'
figure="IMAGE 1"
caption='Recurrence matrix visualisation for libLLVMAMDGPUDesc.a.wav.'
id='img1'
/>

I followed this line of inquiry and made a thread on the [FluCoMa discourse](https://discourse.flucoma.org/) asking how I might approach segmentation based on these types of visual representations, or if there were any existing tools that were based on a similar principle. The thread can be found titled ["Difficult segmentation"](https://discourse.flucoma.org/t/difficult-segmentation/222). Owen Green responded (see [IMAGE 2]({m.re}#img2)), elucidating that the`fluid.bufnoveltyslice~` object was based on Andrew Foote's novelty slicing algorithm (Foote, 2000), an algorithm which at its core uses the notion of self-similarity represented through recurrence matrices to perform segmentation. It facilitates this by first calculating the recurrence matrix by comparing analysis windows to each other recursively. This essential piece of information intends to describe the differences between each point in time to every point in time. The algorithm then slides a window over this data, calculating from one moment to the next the summation of the differences. Together, these summaries form a one dimensional time series, a *novelty curve*. From this, one can estimate times of significant comparative distance, by looking for peaks. An example of a *novelty curve* can be found in [IMAGE 3]({m.re}#im3), extracted from Foote (2000, p. 454). 

A powerful aspect of novelty slicing is that the sliding window (the *kernel*), can be expanded or contracted in order to service the measurement of change across larger or smaller scales of time. Furthermore, the algorithm is relatively indifferent to the nature of the data, and thus what might indicate novelty or salience. This makes it a popular technique for analysing the structure of music, based on the notion that points of change are not necessarily demarcated by localised features, but by those which are accrued over meso- and macro-scales. Given these qualities, novelty slicing seemed like the ideal algorithm for slicing these samples containing highly complex, but nonetheless clearly structured musical ideas. 

<ImageMedia2 
url='/re/owen.jpeg'
figure='IMAGE 2'
caption='Owen Green answering my question in the FluCoMa discourse'
id='img2'
/>

<ImageMedia2
url='/re/noveltycurve.jpg'
figure='IMAGE 3'
caption='Example of a novelty curve, taken from Foote (2000, p. 454)'
/>

Using a *threshold* of 0.61, *kernelsize* of 3, *filtersize* of 1, *fftsettings* of 1024 512 1024 and a *minslicelength* of 2, I produced the segmentation which can be seen in [AUDIO 5]({m.re}#aud5). To me, this segmentation result was commensurate with my initial intuitive approach seen in [AUDIO 4]({m.re}#aud4). While at some points the results of `fluid.bufnoveltyslice~` were over-segmented (slices one to five) it seemed like it could discern the changes I perceived as salient, and was able to extract the longer musical phrases such as slice seven and twelve without suffering from oversensitivity issues found in other algorithms.

<Waveform 
title='libLLVMAMDGPUDesc.a segmented with the fluid.noveltyslice~ algorithm'
caption='AUDIO 5'
file='/re/generation/libLLVMAMDGPUDesc.a.mp3'
peaks = '/re/generation/libLLVMAMDGPUDesc.a.dat'
id='aud5'
segments={noveltySlices}
/>

I then segmented every corpus item with these settings accepting that some files might be segmented poorly as I did not want to spend an indefinite amount of time working on just segmentation. This produced 21551 samples in total after the sounds had been divided.

### Analysis
The next stage involved analysing the segmented samples. I planned to use audio descriptors that I was comfortable with and had used before, such as spectral centroid, loudness, pitch and spectral flatness. However, I also wanted to focus on the textural qualities of the corpus items and capture the morphological properties through analysis. This led me to use MFCCs as they are robust against differences in loudness and are capable of differentiating the textural characteristics of sounds. For each corpus item, an MFCC analysis was performed  using `fluid.bufmfcc~` with an *fftsize* of 2048 1024 2048, *numbands* of 40 and *numcoeffs* 13. I then calculated seven statistics- mean, standard deviation, skewness, kurtosis, minimum, median and the maximum and up to the third derivative for each coefficients.  I anticipated that calculating the derivates of the statistics would capture the morphology of the sound by describing both the change between analysis windows as well as the change in that change. These statistical summaries were flattened to a single dimension and each column of these vectors containing 273 values were standardised.

The analysis script can be found here: [`6_mfcc.py`](https://github.com/jamesb93/reconstruction_error/blob/master/python_scripts/descriptor_analysis/6_mfccs.py).

### Dimension Reduction

In other sound searching and corpus exploration research, dimension reduction has been employed to transform complex data such as MFCCs into simpler and more compressed representations. Examples include the work of Stefano Fascinai (Fasciani, 2015), *Flow Synthesizer* (Esling, Masuda, Bardet, Despres, & Chemla-Romeu-Santos, 2019), *[AudioStellar](https://audiostellar.xyz/)* (Garber et. al. 2020), Thomas Grill (Grill & Flexer, 2012), 
[LjudMap](https://github.com/victorwegeborn/LjudMAP) (Vegeborn, 2020) and the previously mentioned *FluidCorpusMap*. 

Using this research as a basis for my own work, I used the Uniform Manifold Approximation and Projection (UMAP) algorithm and reduced the 273 points in each vector for each sample to 2 points. This way, I could visualise the results as a two-dimensional *map* where each sample would be represented as a singular point. A strength of UMAP is its potency for capturing non-linear features in data compared to algorithms such as Principal Component Analysis (PCA). Due to the noise-based and complex spectral profile of the sounds I generated from databending, being able to capture the non-linear features of their MFCC analysis was an important consideration. Furthermore, UMAP can be coerced to favour the either the local or global structure of the untransformed data by altering the *minimum distance* and *number of neighbours parameters*. A number of projections which were made with various parameters can be seen in [DEMO 1]({m.re}#demo1). You can interact with this demo by moving the slider to interpolate between projections where different parameters for the UMAP algorithm have been used. Each point in this space represents a sample, and the interpolation is between the same sample. This shows how changing the parameters can drastically alter the representation and grouping of sounds together. I found that this was an expressive interface for changing how the computer represented the corpus analysis data. 

<Points 
title='UMAP parameters effect on the projection.'
caption='DEMO 1'
id={'demo1'}
 />

For me, the topology of the projections became a source of inspiration compositionally by observing the location, shape and relationships between clusters of samples. Some projections favoured spread out and even distributions, while others reflected the underlying manifold more strongly and were comprised of curves and warped shapes. As well as observing these outputs visually, I explored them using a Max patch to physically move through the space, playing back samples which were closest to the mouse pointer. Most importantly I found that the projections performed well in positioning perceptually similar samples close to each other in the space. 

This success was dependent on the parameters of the UMAP algorithm, and I trialled several different configurations. Such configurations were used to display [DEMO 1]({m.re}#demo1). I found that configurations with small minimum distances and a relatively high number of neigbours created projections where individual samples in close proximity to each other were perceptually similar, but nearby clusters were not necessarily as coherent. I favoured this approach because the structure of the projection would often contain many tight clusters distributed far away surrounding a central mass. I was not interested in how *all* of the samples were related, rather, what perceptually relevant groups existed within this large corpus. That said, these tigther projections still functioned well as a way of rapidly discerning the overall nature of the corpus. Up to this point I had not listened deeply to the sounds in the corpus and was operating in good faith that there was a collection of rich sounds in the corpus.  

<!-- !!! -->

The computer pre-loaded my engagement with the sound. I imagined that databending would produce loads of different sounds, but actually what I found was variation on similar ideas.
I was encouraged by this initial form of soundful exploration

Future processes all were calculated on a single projection with a minimum distance of 0.1 and 7 neighbours. The dimension reduction script can be found here: [_dimensionality_reduction](https://github.com/jamesb93/reconstruction_error/blob/master/python_scripts/dimensionality_reduction/_reduction.py).

### Clustering
Part of my positive response to the dimension reduction analysis was the robustness in how samples with similar perceptual qualities were drawn spatially. I wanted to take this further, and have the computer perform clustering on this data in order to return groups of samples without me having to explore the space through manual audition. I imagined this type of output would be useful as a starting point for composition.

To do this I experimented by running several clustering algorithms on the projection data, including [k-means](https://en.wikipedia.org/wiki/K-means_clustering), [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) and [Agglomerative Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html). Each of these algorithms offers a different interface and will cluster input data based on a different set of assumptions. K-means, for example, operates by partioning data into a fixed number set by the user. Individual points are then drawn into these partitions to create clusters. This process assumes that the data is spherically distributed and that each cluster will have approximately the same number of clusters. This can be a problematic assumption to make and a number of conclusions can be falsesly drawn if these limitations are not accepted and acknowledged. In particular, I found [this discussion on the drawbacks of k-means](https://stats.stackexchange.com/a/133841) useful when researching different clustering methods. HDBSCAN, on the other hand determines an appropriate number of clusters automatically depending on a set of constraints that calculate the *linkage* between data points. It uses a minimum spanning tree to hierarchically divide the space into clusters and so it can often respect the presence of nested structures within a larger dataset. It can also be more suitable to data that is not spherically distributed around barycentres.

Between such algorithms their differences in interface and underlying algorithm invite different questions to be asked and enable the computer to discern different features from data.  I found the notion of asking for a specific number of clusters from k-means valuable because it meant that I could roughly determine the density of each cluster. If I wanted to create very specific groupings, for example, I could just ask for more clusters. However, the minimum spanning tree data structure intriguied me as a computational output that might be able to inform my inspection and understanding of the corpus as well as a way of structuring material in time potentially. Instead of just assigning a cluster membership to each sample, the tree itself projects a web of "leaves" throughout the data which indicate connections between hierarchical sections of it. An example of this can be found in [IMAGE 4]({m.re}#img4).

<ImageMedia2 
url='/re/hdbscan.png'
caption='HDBSCAN minimum spanning tree visualisation. Data points are represented by circular nodes in the space with connections of the tree indicated by lines connecting them. Taken from https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html.'
figure='IMAGE 4'
id='img4'
htmlCap={true}
/>

To get the benefits of both of these features, I settled on using another algorithm, *Agglomerative Clustering*,  which allows the user to specify a desired number of clusters while also working in the same fashion to HDBSCAN and creating a minimum spanning tree. Despite my best efforts to parse and understand the minimum spanning tree though, I found it difficult to reason around programatically and to draw any value from. Nonetheless, my exposure to the minimum spanning tree concept informed my further application of clustering. I used the agglomerative clustering algorithm to calculate four results on the projections with increasing levels of cluster density. This returned a collection of clustering outputs with different granularities and specifities. The first output produced 250 clusters, then 500, then 1600 and finally 3200. These were chosen based on the number's relationship to the total number of corpus items. [DEMO 2]({m.re}#demo2) uses a small synthetic dataset to demonstrate how this clustering approach operates. The colour of each point in the visual space represents its membership to a cluster. By toggling between the three different levels of granularity (2, 4, 8 clusters) one can observe how certain points remain together or are separated into new groups.

<Agglom />


I then performed a meta-analysis discerning the shared membership of each sample to clusters across the various levels. Because each analysis level is producing a higher number of clusters, they always return increasingly smaller clusters. As such, a cluster containing 500 samples in the 250 clusters analysis might have the same samples spread across two different clusters. There is no guarantee that a sample will remain in the same groups though and changing the level of specificity in the hierarchical divisions can cause samples to be reclustered in different ways. This means that while clusters often share samples between levels, new samples can also be introduced. By doing this, it allowed me to "peek" outside of a cluster and to see where the edges of it start to become blurry as the constraints for analysis change. In a sense it allows me to reason around the project space with a complex venn diagram, where different portions overlap and intersect. 

The result of this is a set of JSON files that describes clusters which share samples and how many samples they share. In [CODE 1]({m.re}#code1), a portion of the analysis describing the shared membership of the 250 and 500 analysis is shown. The information in this example expresses that cluster 36 in the 250 clusters analysis is comprised of clusters 317 *and* 327 in the 500 clusters analysis.  

<Code
figure='CODE 1'
caption='Meta-analysis data describing the shared membership of clusters. This portion describes the shared membership of clusters between the 250 and 500 clusters output.'
>

```json
{
    "36": {
        "317": 8,
        "327": 16
    },
    "89": {
        "277": 52,
        "105": 82,
        "491": 34
    },
    "70": {
        "39": 63,
        "214": 75,
        "425": 15
    },
}
```

</Code>

The clustering code can be found here: [`_clustering.py`](https://github.com/jamesb93/reconstruction_error/blob/master/python_scripts/clustering/_clustering.py).

### Moving Forward
At this stage I moved into *soundful* play with the various computer outputs which led the development of the pieces. The next section [Responding To Computer Outputs]({m.re}#responding-to-computer-outputs) discusses this.

## Responding To Computer Outputs
After producing the clustering data, I used it to drive audition processes in Max and REAPER. In Max, I created an interface allowing me to retrieve the samples from clusters and simultaneously navigate between the shared membership connections. The samples were played back contiguously by a sequencer so that I could quickly hear the samples in the selected group in succession. This allowed me to audition samples rapidly and to ascertain a rough relationship between the cluster members. It also allowed me to start with a broad cluster of samples and hone the selection by following the connections of shared membership. This allowed me to find very specific "pockets" of sounds within the entire corpus by whittling down the selection from broad collections of perceptually similar sounds to those that were even more similar. A screen capture demonstrating this auditioning process can be found in [VIDEO 10]({m.re}#vid10).

<VideoMedia2 
url="/re/meta-analysis.webm"
figure="VIDEO 10"
caption="Using Max and the meta-analysis data to explore the corpus in a structured manner."
id="vid10"
/>

This process was immensely useful for becoming acquainted with the samples from the corpus in a structured manner. Despite the fact that the source material was generated from disparate sources of raw data on my hard drive, connections began to form in my mind about how the material could be used in composition and what an EP of different pieces might explore as a *collection*. As I was auditioning I created a set of written annotations for each cluster and created my own set of tags to try and group the clusters broadly. Chronologically, the notes capture increasingly specific imagined applications, ranging from just noting which samples I thought were similar, to devising forms based on selecting clusters in a certain order. These notes can be found at this link: [cluster notes](https://github.com/jamesb93/reconstruction_error/blob/master/clusters.md).

Up to this point, the audition process was orchestrated entirely in Max. However, I did not want to further compose in this environment as I was begining to formulate concepts around formal structures and the way these samples would actually be constructed into a set of compositions. Working entirely in Max had already proven to be problematic in [Stitch/Strata]({m.ss}), especially in the earlier phases for that project. Instead, I wanted to be able to access samples as clusters  in a environment where I could "freeze" them, arrange them, as well as modify and alter them with audio effects. In order to achieve this workflow,  I built a series of Lua scripts that allowed me to turn a cluster from a JSON output into a track in REAPER by leveraging the ReaScript API. This can be found in [CODE 2]({m.re}#code2). 

<Code
figure='CODE 2'
caption='Lua script for importing a cluster of samples as a track of contiguous samples into REAPER'
id='code2'
>

```lua
local info = debug.getinfo(1,'S');
local script_path = info.source:match[[^@?(.*[\/])[^\/]-$]]
package.path = package.path .. ';' .. script_path .. '?.lua'
package.path = package.path .. ';' .. script_path .. 'lunajson/src/?.lua'
loadfile(script_path .. "../ReaCoMa/lib/reacoma.lua")()
local lunajson = require 'lunajson'

local confirm, user_input = reaper.GetUserInputs(
    'Provide JSON info', 
    2, 
    'JSON Path:, Cluster Number:, extrawidth=100', ''
)

if confirm then
    reaper.Undo_BeginBlock()

    -------- start processing --------
    reaper.SetEditCurPos(0.0, false, false)
    reaper.InsertTrackAtIndex(0, true)

    -- Parse user input --
    local fields = reacoma.utils.commasplit(user_input)
    local json_path = fields[1]
    local cluster_num = fields[2]

    -- Do json parsing --
    local file_in = assert(io.open(json_path, "r"))
    local content = file_in:read("*all")
    local cluster_data = lunajson.decode(content)
    for k, p in ipairs(cluster_data[cluster_num]) do
        reaper.InsertMedia(
            '/Users/james/Cloud/Projects/DataBending/DataAudioUnique/'..p, 
            0
        )
    end
    reaper.SetEditCurPos(0.0, false, false)
    reaper.Undo_EndBlock("Insert Media Cluster", 0)
end
```

</Code>

Running this script would present me with the option to provide a JSON file and a cluster number. This would generate a new track containing the samples belonging to that cluster. This allowed me to rapidly audition samples in Max, then draw them into a REAPER session without friction.[VIDEO 11]({m.re}#vid11) demonstrates the process of importing a cluster into REAPER. This was the basis on which I composed the next few pieces. I would oscillate between listening in Max, importing sounds into REAPER as clusters and then using intuitive compositional processes to organise that material. When I hit a creative block in the DAW I would return to listening and try to find something new in the material, guided by the computer.

<VideoMedia2 
url="/re/scripting.webm"
figure="VIDEO 11"
caption="Using ReaScript API to create tracks of samples from cluster data."
id="vid11"
/>

The next sections, [_.dotmaxwave]({m.re}#_dotmaxwave), [X86Desc.a]({m.re}#x86desca), [segmnoittet]({m.re}#segmnoittet), [--isn]({m.re}#isn) and [sys.ji_]({m.re}#sysji_) describe the specific compositional processes for each piece in Reconstruction Error and how the computer informed my compositional choices and led the computer-aided compositional process.

### _.dotmaxwave

<Waveform 
file='/pieces/re/dotmaxwave.mp3'
peaks='/pieces/re/dotmaxwave.dat'
segments={maxwaveSegs}
title='_.dotmaxwave'
/>

*_.dotmaxwave* was the first piece I composed. The samples that I used in this piece were all derived from cluster 131 which I discovered by auditioning cluster samples in Max. While listening to this cluster I realised that the source material was all derived from a particular raw data type with the .maxwave extension and had similar morphological properties. All of the samples begin in a similar fashion, with four repeated iterated pitched sounds leading into a static texture (see [AUDIO 6]({m.re}#aud6)). This static texture contains a predominant pitch component with some quiet narrow-band noise. I wanted to capitalise on what I had found, and create a track that focused on the relationship between pitch and noise as well as the shared morphology between each sample.

I started by importing cluster 131 into REAPER and listening to the samples. I navigated through the cluster relationships and identified samples which I found to have the most novel differences in the opening sequence. As such I was able to create a pallette of small iterations on the same idea. These are presented in [AUDIO 6]({m.re}#aud6).

<Album 
title='Cluster 131 .maxwave sounds'
tracks={maxwave}
id='aud6'
caption='AUDIO 6'
/>

In each of these samples there is a noise component that is relatively quiet compared to the pich component. Using the FluCoMa harmonic percussive source separator, `fluid.bufhpss~`on the command line I processed each of the sounds. I achieved a very clear separation of these two elements using this technique and found that the percussive components had their own complex morpholgy and character it was just low in volume and masked by the more dominant pitch sound. I wanted to emphasise the noise, so I summed the two components back together after increasing the volume of the percussive component.

The rest of the piece grew from this concept. The first section from 0:00 to 1:13 highlights the morphological characters of each variant, while the remainder of the pieces draws on the decomposition process to structure different presentations of percussive material buried within these samples. The final crescendo from 2:25 to 3:37 exploits what I found to the most intricate and non-static percussive material which prior to the decomposition procedure was relatively hidden and buried.

While composing this first piece I constantly was stepping in and out of REAPER to perform decomposition processes. It was tedious to have to move *out* of the DAW to perform audio processing as well as constantly having to move between the command line, the file system and the DAW timeline in order to setup the necessary media items for audition and composition. This friction led me to create [ReaComa]({m.reacoma}), a set of ReaScripts that bridge the FluCoMa command-line tools with REAPER. These scripts allow one to process media items directly in REAPER, and to configure the parameters of various decomposition algorithms such as harmonic percussive source separation, non-negative matrix factorisation, transient extraction, sinunsoidal isolation. In addition to this, the segmentation algorithms from FluCoMa's first toolbox are implemented so that media items can be split in time using several different approaches to dividing sounds in time. In the remaining pieces of Reconstruction Error [ReaCoMa]({m.reacoma}) played a vital role in unpicking the internal content of sounds, once I had discovered and selected them through analysis. It also led me to performing additional layers of segmentation in order to restructure sounds in time.

### --isn
<Waveform 
file='/pieces/re/isn.mp3'
peaks='/pieces/re/isn.dat'
segments={isnSegments}
points={isnPoints}
/>

[--isn]({m.re}#isn) is composed from clusters of samples with short impulse-based material. While composing I was often using the annotations and notes I made for each cluster as a way of informing me of what samples to experiment with next. I noticed that I classified a large number of clusters as short, fast and delicate. This specific portion can be read [here](https://github.com/jamesb93/reconstruction_error/blob/master/clusters.md#short-fast--delicate).

<!-- WIDGET: maxwave start, able to select different streams of impulses overlapping each other 
-->

I selected these specific clusters and auditioned them by playing samples back-to-back in a sequencer-like fashion. The repetition of impulses formed rhythmic looping patterns where the length of the samples dictated the internal structure. The slight differences in the shape of the impulses gave these patterns a musical quality. From this method of concatenating and arranging samples, the internal dynamics of each cluster were brought to the foreground and I wanted to capitalise on the tension this created between the tight perceptual grouping of each cluster, the slight differences between the samples within these and the overall perceptual separability between clusters. 

The piece is structured around the superimposition of several of these loops, which are individually derived by concatenating clusters together. This approach was in part influenced by Robert Wannamakers piece, *[I want to tell you something...](http://www.robertwannamaker.com/MusicFrameset.htm)* which uses the summation of several sinunsoidal components to eventually culminate in a noise texture. Through this process varying textures and morphologies emerge from the interaction of highly constrained atomic units of sound. This approach resonated with my relationship to the impulse-based material.

From 0:00 to 2:34, I created a passage of gradually increasing intensity and textural complexity by taking several of the clusters and applying linear fades. Some clusters are more similar than others and layering them in this way creates subtle phasing and otoacoustic effects, reminscient of those one might find in Thomas Ankerschmidt's compositions or [Olivier Pasquet's work with impulse-based material](https://www.opasquet.fr/herbig-haro/). As more layers are introduced and the differences between them increases the interactions between different layers become harder to unpick from one another and the texture as a whole becomes denser and more complex. The relatively abrupt break at 2:34 reduces down to just two of these layers significantly decluttering the texture and constructing a much sparser musical behaviour. The remainder of this piece explores this stripped back and reduced sound world only ever superimposing two or three clusters at any one time.

I imagined that I might proliferate the musical ideas I had built up to this point. However, I could not derive any more novel superimpositions of clusters that I found satisfactory and envisaged that to continue the piece I woudl need to introduce more material or to start moving in another musical direction. I decided to keep the material constrained to just the clusters I initially grouped in my annotations and to keep the piece very focused.

### X86Desc.a
<Waveform 
file='/pieces/re/X86desc.a.mp3'
peaks='/pieces/re/X86desc.a.dat'
caption='X86Desc.a'
points={descPoints}
/>

This piece developed the concepts which underpinned [--isn]({m.re}#isn) and perhaps was a response to restricting myself to a narrow selection of impulse-based material. [X86Desc.a]({m.re}#x86desca) focuses on the incorporation and treatment of similar sound types but expands the palette of transient-based material through processing complex sounds from other clusters. 

For example, the transient material at 0:45 is created using a transient extraction algorithm from [ReaCoMa]({m.reacoma}). The original source material, *libLLVM-6.0.dylib_144.wav*, is predominantly pitched though and the end result is perhaps not recognisable as the source material due to the removal of the non-transient component of that sound. This process is used throughout the piece to generate different materials. Depending on the morphological properties of the input sound, a number of different quasi-rhythmic patterns and impulse passages are carved away from the spectrally rich sources. For example,  at 2:07, one of the key sonic elements in that texture was derived by taking a cluster of samples and processing them through the transient extraction algorithm. I did this with three different settings, to create variations on that material which I then used to formulate a morphing passage of sound. This is isolated in [AUDIO 7]({m.re}#aud7).

<Waveform 
title='Material from cluster 165 processed with the transient extraction algorithm in three different ways. 2:07 in X86Desc.a.'
caption='AUDIO 7'
file='/re/cluster165-preparation.mp3'
peaks='/re/cluster165-preparation.dat'
id='aud7'
/>

In isolation the results of these types decomposition processes were uninspiring to me and the musical effect was underwhelming. However, I found that by layering multiple instances I could create complex interactions between different *streams* of transient-based material. The rest of the piece was intuitively composed around my experimentation with this layering. Overall, this piece contained all of the trailing ideas that I did not want to pollute [--isn]({m.re}#isn) with. For me the results were distinct enough that it warranted making a new piece in the album, although the mechanics of composing the piece were quite similar.

### segmnoittet

<Waveform 
file='/pieces/re/segmnoittet.mp3'
peaks='/pieces/re/segmnoittet.dat'
points={segmPoints}
title='segmnoittet'
/>
For this piece, I wanted to create something that was less rhythmically homogenous than the previous tracks and to create something more mechanical and sparse. The first idea I had involved taking clusters that when concatenated would create such aspects. This would result if for example the samples within a cluster had significantly differing lengths. I was able to control this aspect by selecting samples from different hierarchical levels of the meta analysis. The deeper I went into the hierarchy, the more homogenous the clusters would become when concatenated. A short composed section of music emerged from this and can be heard in [AUDIO 8]({m.re}#aud8}).  

<Waveform 
file='/re/initial-segm.mp3'
peaks='/re/initial-segm.dat'
id='aud8'
title='A short section of composed music using various clusters of mechanical material.'
caption='AUDIO 8'
/>

This short section did not develop and I had exhausted a lot of the material which I had collected up to this point. Most, if not all this material was derived from cluster 250, so I started following the connections that this cluster had to the other outputs via the meta-analysis. This did not introduce many additional samples, though it did present the possibility to separate the same or similar material into more specific clusters. In [IMAGE 5]({m.re}#img5), the REAPER session of this piece is shown, and displays numerous tracks that were used in order to structure each sub cluster alongside its parent cluster into the session. These are colour coded to represent the shared membership and annotated such that I could discern which output they originated from.

<ImageMedia2 
url='/re/segmnoitte-session.jpg'
figure='IMAGE 5'
caption='REAPER session for segmnoittet. Tracks displayed on the left side of the screen contain various tracks of clusters and their respective "sub-clusters".'
id='img5'
/>

After importing these clusters I was able to keep composing, and eventually the compositional process took a new direction. The overall aesthetic became much more relaxed and less rhythmically assertive and block-like. The presentation of short gestures became much more reserved as many of them were superimposed or more sparingly used to ornament longer textural sounds. I was able to create various effects by carefully selecting perceptually homogenous groups of materials and treating them as individual compositional units. For example, at 1:10 a small cluster processed is processed with a moving filter. The original impulses in this small section of source material are very bright and contain lots of high frequency content, so the result is punchy and almost overdriven. A similar process is applied to ther clusters at 0:25, 0:35 and 0:40 where concatenations function as interjections to the base texture that is introduced at the start.

The slow unfolding ending is created by taking another subcluster of samples and concatenating them with different spacings. This occurs both vertically, in that multiple spacings are layered simultaneously, and over time all of the spacings become much longer. This creates the effect of pitch disspearing and only the impulse remaining. To orchestrate this, I created a Lua script which could interactively space out items in the timeline by setting an amount, as well as a noise factor which could skew values along the fixed spacing. This is demonstrated in [VIDEO 12]({m.re}#vid12), in which I take several impulse samples and modify the spacing between them.

<VideoMedia2 
url='/re/quantise-gui-demo.webm'
figure='VIDEO 12'
id='vid12'
caption='Interactive item spacing with Lua accessing the ReaScript API.'
/>

### sys.ji_

<Waveform 
file='/pieces/re/sys.ji_.mp3'
peaks='/pieces/re/sys.ji_.dat'
id=''
title='sys.ji_'
points={sysPoints}
/>

This was the last piece that was composed for the album. It is the most different in compositional approach and the material that was used. For me, it signified a developmental milestone in the compositiona process, where I was comfortable enough with the corpus that I could navigate almost intuitively. Instead of entirely relying on the computer to seek out sounds, or to be offered potential groupings, I relied on knowledge I had built up throughout composing the other four pieces. In three of the previous pieces I relied on using short impulse-based material to compose. The way that I arranged and incorporated these sounds were delicate, and focused on intricate and fragile qualities with quasi-interlocking rhythms. I wanted to create something antaganostic to these pieces, and include the more bombastic and spectrally rich sounds that I knew existed elsewhere in the corpus.

When I was experimenting with different clustering algorithms, I tried another algorithm, HDBSCAN, which is discussed in some detail in ["Clustering"]({m.re}#clustering). One cannot specify HDBSCAN to produce a set number of clusters, rather it determines this for itself in response to a number of constraints in the algorithm. I found this clustering technique much worse in the results, and the clusters often contained only two or three samples across a large parameter range. Most interestingly though, this algorithm creates a cluster that contains everything it can not confidently assign membership to. I auditioned this cluster in my data and found exactly the type of sounds I was looking for - noise-filled, chaotic and spectrally rich. I noticed that many samples in this cluster had originated as segments from the file *sys.ji_.wav*. I isolated several of them that I instinctively liked the sound of and worked backwards by looking at the agglomerative clustering data to see where those files had been grouped by that algorithm. This led me to a number of other noise-based samples which were not present in the HDBSCAN data. In effect, I used the two different clustering algorithms as a mechanism for discovery through *cross referencing*. 

This approach revelead to me how the curation of algorithms and technical implementation can be instrumental as an expressive tool. Each of the clustering algorithms produced different representations of the corpus, and through their differences I was afforded a novel method of corpus exploration leading to the incorporation of new samples. The first half uses segments from *sys.ji_.wav*, and as material develops around 1:18, samples derived through cross referencing are gradually included. These samples are more striated and iterative, and start to display shades of almost filtered noise as well as some pitched components. This is solidified at 2:34  in which a totally new category of noise samples are presented and are derived from source files with the *.reapeaks* extension. 
 
## Reflection

Do you need to mention sound searching, or something about searching *with* the computer as a mutual process?

Reconstruction Error gave rise to a major point of development in this PhD research in terms of compositional workflow and the relationship that I built with the computer for composition. By engaging in a dialogue with the computer , I was *led* through the creative process and the computer amplified and enhanced my textural and timbral focused listening as a primary driver of compositional action. The technical development described in ["Initial Technical Development"]({m.re}#initial-technical-development) were essential in building this dialogue and I do not consider it as a "preamble" to the composition process but rather a highly integrated part. This fusion of technology and musical thinking was one of the most successful aspects in this project. The computer's function of grouping material into clusters was commensurate with my own interpretation of the sounds while still offering something unique and producing results that were *beyond* what I could cognise. It functioned as both a way of gathering material into compositional units that were compatible with my preference for perceptual homogeneity as well as offering unexpected and divergent results. This application of content-awareness allowed me to hone my initially blunt interests and to transform my surface-level appreciation for the [databending practice]({m.re}#databending), into a project where I incorporated those ideas in an artistically authentic and personalised manner.

<!-- REVIEW: as a result fix -->
Musically, being able to flexibly put aside or work entirely with the computer allowed me to compose more radical and daring presentations of material. In previous projects I had been thinking more formalistically and trying to make the computer structure works with constraints or algorithms. This was entirely not the case for Reconstruction Error and formal decision making was up to me for the most part. Although the computer had a significant effect in suggesting strategies for organising samples across local scales of time (this is prevalent in [--isn]({m.re}#isn) for example), a majority of the high-level formal thinking and approach to creating overarching structures was my own doing. This was mediated by the process of the computer structuring my listening and engagement to materials and the conceptual underpinnings of each work were formulated by what  I discovered using machine listening and learning. As a result, each piece to me feels more focused in terms of compositional material and perhaps has a higher level of conceptual purity.

Another key feature of this work is the technological contribution of Python combined with the DAW. The same combination was used in [Stitch/Strata]({m.ss}) and I reflect on the success of the back and forth workflow of generation, computational curation and my responding to the output of that. For me, Reconstruction Error was proof that this workflow is how I compose most effectively and incorporate the computer in a meaningful way that integrates with my musical thinking and motivations. It engendered the right level of interaction, where I could step between the mindset of listening and action without those two things necessarily being intertwined temporally. As an analogy to flesh out this idea, my workflow could be contrasted to that of the live coder - who inputs their commands to the computer in real-time and listens to the response in order to guide their future actions. Their reactions and interactions are entrenched in what will happen next musically. My script-based workflow separates the immediate musical concerns from the compositional and conceptual thinking that I want to explore in my creative process. This allows me to stretch, compress, shift, cut and reorganise the temporal aspects of my work separately from the material that will fill the musical structures I create. The building blocks of that process are selected and navigated, *out of time* through structured listening by the computer. I realised on reflection in this project that this is an essential aspect to how I want to interact with the computer to make music. 

### Finding Things In Stuff
The success I had with scripting in combination with REAPER motivated me to create a software framework for facilitating similar types of processes in the future. Much of the code I had written for this project would have been difficult to use for future projects. Elements such as hard-coded paths, an unstandardised file strucutre for keeping corpus materials and a general inflexibility in the code to modify it meant that it would just be easier to start again. Similary, a majority of the iterative back and forth is not necessarily well captured in the artefacts remaining at the end of the project. Towards the end of [Reconstruction Error]({m.re}), scripts were adjusted with configuration files and these act as a form of documentation for what I did in the creative process, but on the whole it is difficult to trace the compositional process back to the technology. This is largely a consequence of not knowing in the moment if something I am doing is going to be significant later. Ideally, these aspects are abstracted away and performed automatically by the computer.

In response to this I developed [FTIS (Finding Things In Stuff)]({m.ftis}), an extensible Python-based framework for performing audio analysis and machine learning that enables the outputs of these processes to be drawn into REAPER and Max rapidly. Instead of relying on "single-use" scripts, FTIS was designed to facilitate the construction of computer-aided processes as extensible and flexible pipelines of connected *components* that could be modified in response to creative exploration. I discuss and outline the capabiltiies and possibilities in FTIS' section of the ["Technical Implementation"]({m.ti}) as well as outline the underlying programming aspects that service my compositional thinking and workflow.

The next project, [{m.emname}]({m.em} ) is the final project in this PhD portfolio. Technologically it incorporates FTIS heavily and informed the development of various alpha versions of that software. To me, it is the the next step in development of processes that I discovered and used in Reconstruction Error. Musically, it draws on a similar digital and synthetic sound world. That said, the treatment of material is different and focuses on long form structure and opening up deeper modes of listening. Developing FTIS reduced the amount of time I needed to spend on building the tools necessary for my computer-aided pracitce allowing me more time to be daring an explorative with my own compositional expression.

## Other Related Links and Media

I gave two presentations that discussed this project at different points in its development. The first presentation I gave was at the November 2019 FluCoMa plenary, where the pieces were partially composed and I focused on demonstrating my application of the FluCoMa tools. This can be seen in [VIDEO 13]({m.re}#vid13). Much longer after the pieces had finished and I moved on to composing [{m.emname}]({m.em}), I reflected on the entire development process and its relationship to the compositions in a [short article](https://zenodo.org/record/4285398) for the 2020 AI Music Creativity Conference. This involved creating a short video demo too which can be watched in [VIDEO 14]({m.re}#vid14).

<YouTube 
url="https://www.youtube.com/embed/IpD_XzW1Az4" 
figure='VIDEO 13'
caption='"Finding Things In Stuff" presentation at the November 2019 FluCoMa Plenary'
id='vid13'
/>

<YouTube 
url="https://www.youtube.com/embed/-FNO0QovfsI"
caption='"Corpus exploration with UMAP and agglomerative clustering" presentation at the AI Music Creativity Conference 2020.'
figure='VIDEO 14'
id='vid14'
/>

<NextSection 
next="Interferences"
link={m.em}
/>
<script>
    import {metadata as m} from '../directory.svx'
    import NextSection from '$lib/components/NextSection.svelte';
    import VideoMedia from '$lib/components/VideoMedia.svelte';
    import VideoMedia2 from '$lib/components/VideoMedia2.svelte';
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
    import YouTube from '$lib/components/YouTube.svelte';
    import Album from '$lib/components/Album.svelte';
    import Code from '$lib/components/Code.svelte';
    import Waveform from '$lib/components/Waveform.svelte';

    // Album Tracks

    import { reAlbum, ibuffer, generation, libllvmSlices, noveltySlices } from '$lib/data/reconstruction-error.js'
</script>

# Reconstruction Error

<Album 
tracks={reAlbum}
title="Reconstruction Error"
id="audio1"
figure="AUDIO 1"
/>

Reconstruction Error is the fourth project in this portfolio. It is a set of five pieces, presented in an EP format.

There is also a git repository which contains all of the Python code and Max patches which were used in the. This can be found at: https://github.com/jamesb93/data_bending.

## Motivations and Influences
In the previous three projects, I explored a number of techniques, approaches and workflows for computer-aided composition. These approaches took several forms and were motivated by different factors such as opportunities to compose for visiting artists or by responding to immediate influences in my creative coding practice and musical listening. Throughout each project in the process of testing new technologies and strategies for working with the computer, I found that the most successful workflow was developed in [Stitch/Strata]({m.ss}). Despite being a difficult process and inducing many failed attempts to reach the finished state of the piece, using the computer to sieve through a corpus of sounds in order to help guide me towards organisations of material felt like a fluid and meaningful workflow that I could develop in the future. Given these conditions produced by reflection and consideration of the work done up to this point, I decided to move forward with this in mind. I wanted to create a piece by using the computer in a similar fashion.

### Databending
Amongst this reflection on my own practice up to this point, I had been listening to artists such as Alva Noto, Autechre and Ryoji Ikeda. Their work draws on hyper-digital, surgical and sometimes ultra-minimalist sonic materials associated with post-digital aesthetics. The raw digital sounds found in such artists work have always been appealing to me, and to some extent a part of my compositional vocabulary and aesthetic. [Stitch/Strata]({m.ss}), while focusing on the sound of experimental vocal techniques at times arranged sounds into glitch-like, artificial and quasi-rhythmic structures.

I was also becoming interested in databending, a process in which raw data is used for the purposes of constructing visual and sound materials in art. This can be achieved a number of ways, and the process one chooses to realise this is as instrumental in the practice as performing the act itself. A number of tools exist that facilitate databending either intentionally or through divergent use. Sch programs include [SoundHack](https://www.soundhack.com/) which allows you to convert any file on a computer into a valid WAVE audio file, [Audacity](https://www.audacityteam.org/) which supports the reading of raw bytes as audio data, or [PhotoShop](https://digital-photography-school.com/make-abstract-glitch-art-photographs/) by opening non-image files as if they were images. 

A number of artists have experimented with databending in their own work and this guided my own research of this digital practice. [Nick Briz](http://nickbriz.com/), an "new-media artist"  explores various "attacks" on data to degrade and produce distortions of video data in *Binary Self-Portraits* (see [VIDEO 1]({m.re}#vid1)). Another video-art piece, *From The Group Up In Order, Embrace* (see [VIDEO 2]({m.re}#vid2)), explores the creation of visual material through the interface of the binary code that represents itself. Briz states: 

<div class="bigquote">

*Anything that is digital contains beneath it layers of code and at the base is an immense series of ones and zeroes known as binary code. Like the atom in the natural world, the ones and zeroes of binary code are the smallest component of the digital medium. In my efforts to strengthen my relationship with the digital medium I set out to create a piece where I dealt directly with the binary code of a video file. The result was this video as well as an obsession with the digital medium, and a first in a series of binary video pieces.* (Briz, n.d )

</div>

<VideoMedia id='vid1'>
<iframe slot='media' src="https://player.vimeo.com/video/4510943" width="640" height="524" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot="caption">VIDEO 1: Binary Self-Portraits by Nick Briz</p>
</VideoMedia>

<VideoMedia id='vid2'>
<iframe slot='media' src="https://player.vimeo.com/video/4506517" width="640" height="524" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot="caption">VIDEO 2: From The Ground Up In Order, Embrace by Nick Briz</p>
</VideoMedia>

*Unitxt Code (Data to AIFF)* (see [VIDEO 3]({m.re}#vid3)) by [Alva Noto](http://www.alvanoto.com/) although not explicitly stated in liner notes or interviews, likely uses data converted to audio (as suggested by the name) for the core compositional material of that work. The harsh and degraded noise sounds are commonly produced when converting text files into audio. [*Datamatics*](https://www.ryojiikeda.com/project/datamatics/) (see [VIDEO 4]({m.re}#vid4)), by[Ryoji Ikeda's](https://www.ryojiikeda.com)  incorporates raw data as source material which drives both the visual and audio of this installation work. [Pimmon's] album *Electronic Tax Return*  (see [VIDEO 5]({m.re}#vid5))has several works based on synthesising sound by converting dynamic link libraries (.dll files) into files for playback and manipulation.

<YouTube 
url="https://www.youtube.com/embed/vN9FrqrPptg"
figure="VIDEO 3"
caption="Unitxt Code (Data to AIFF) by Alva Noto"
id='vid3'
/>

<YouTube 
url="https://www.youtube.com/embed/eaIrSZIyxxk" 
figure="VIDEO 4"
caption="Datamatics (Limited Section) by Ryoji Ikeda"
id='vid4'
/>

<YouTube 
url="https://www.youtube.com/embed/AbF8NFanUdQ"
figure="VIDEO 5"
caption="Electronic Tax Return by Pimmon"
id='vid5'
/>

These works of sound art and composition were influential in my exposure and research into databending. 

In particlar though, [Michael Chinen's](https://michaelchinen.com/) practical experiments demonstrated a tactile, hands on approach to databending in which he developed various fledged programs. 

These programs, explore sonification of raw bytes held in memory for different applications across a variety of different use cases. [FuckingAudacity](https://michaelchinen.com/music/fuckingaudacity/) is a fork of the popular audio application Audacity. Mischievously, instead of actually being able to play any audio files, the sound output from this program is a snapshot of the memory state at regular intervals. Interactions with the user interface and user-driven processes such as importing files or enabling effects themselves become ways of generating sound. [VIDEO 6]({m.re}#vid6) is a demonstration of Chinen playing with the program.

<VideoMedia id='vid6'>
<iframe slot='media' src="https://player.vimeo.com/video/8144372?byline=0&portrait=0" width="640" height="480" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot='caption'>VIDEO 6: FuckingAudacity demonstration by Michael Chinen.</p>
</VideoMedia>

[FuckingWebBrowser](https://michaelchinen.com/music/fuckingwebbrowser/) (see [VIDEO 7]({m.re}#vid7)) follows in suit with its Audacity counterpart. The application presents as a standard web browser. However, the memory state of the programÂ is used to generate audio data which is then played back as the user interacts with it. Navigating to or from a website or interacting with a page generates sounds in unpredictable ways. Even doing nothing at times reveals the hidden internal workings as sounds. Making http requests, polling for data or caching files in the background result in sounds that are detached from the user's interaction with the application.

<VideoMedia id='vid7'>
<iframe slot='media' src="https://player.vimeo.com/video/9635993?byline=0&portrait=0" width="640" height="480" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot='caption'>VIDEO 7: FuckingWebBrowser demonstration by Michael Chinen.</p>
</VideoMedia>

The logical conclusion for this pattern of application development is one that can be attached to anything running on a computer and be able to sonify the memory state as it runs. [lstn](https://michaelchinen.com/2010/12/09/listen-to-your-computer-lstn/) performs this function, and can observe and sonify the opcodes, callstack and active memory of any program on the computer. [VIDEO 8]({m.re}#vid8) is a demonstration of this in practice.

<VideoMedia id='vid8'>
<iframe slot='media' src="https://player.vimeo.com/video/17617200?byline=0&portrait=0" width="640" height="480" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot='caption'>VIDEO 8: lstn demonstration by Michael Chinen.</p>
</VideoMedia>

### Employing The Computer To Structure A Corpus
Coming into contact with Chinen's work was influential because there is a mixture of predictable and unpredictable behaviours exhibited by the computer through his various databending programs. I was able to  sense the overarching character of the sounds would be (digital, harsh and noise-based), but occasionally the results were surprising. Silence, fragility and delicacy emerged at times and this notion that there were "needles in the haystack" was a tension that I wanted to explore myself. Part of my motivation for this project was to create a scenario where I could be surprised by the process of extracting sounds and wading through them with the aid of the computer.

As part of my preliminary research toward these motivations, I came into contact with the  outputs of the Fluid Corpus Manipulation project. I had up to this point been loosely involved with some of their research activities and privy to internal workshops with their commissioned composers as a helper. As such, I was following what they were doing, and they released a video demonstration for some software called *[FluidCorpusMap](https://github.com/flucoma/FluidCorpusMap)* (see [VIDEO 9]({m.re}#vid9)).  Neither the paper (see Roma et. al. (2019)) or source code for this program were released at the time. However, I was able to discern from the information in the video that the program was using a processing pipeline of producing mel-cepstrum frequency coefficients (MFCCs) analysis, calculating statistics on this data and then using dimensionality reduction algorithm to create a two dimensional latent space for exploration and navigation. I found two aspects of this process fascinating. Firstly,  the *global* shapes produced from the dimensionality reduction stage possessed novel and identifiable characteristics depending on the algorithm. The various layouts to me, were evocative, in that the computer seemed to be creating its own understanding of the corpus which could structure the engagement with those sounds by exploring the space. Secondly, the *local* distribution of samples was perceptually smooth and coherent. Movements between adjacent points in the demonstration returned sounds which were texturally similar. Even aspects such as morphology seemed to be captured (this can be seen around 1:19 where sounds with upwards glissandi are grouped together) giving the sense that the computer could capture both texture *and* morphology. This to me, was far more sophisticated than my previous experiments using instantaneous statistical summaries.

<VideoMedia id='vid9'>
    <iframe slot='media' src="https://player.vimeo.com/video/314301724?byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
    <p slot='caption'>VIDEO 9: FluidCorpusMap demonstration.</p>
</VideoMedia>

In previous projects, the notion of computational sound searching had been already been explored, albeit in different ways to what was found in FluCoMa's research. The workflow of [Stitch/Strata]({m.ss}) was predicated on forming groups of material from a corpus of voice samples by audio descriptor query. Similarly, [Annealing Strategies]({m.as}) was motivated by the desire to have the computer find parameter inputs that would produce particular perceptual quality from the Fourses synthesiser. To me, this approach of mapping samples seemed like it could be useful for enabling the computer to suggestive andÂ for it to be instrumental in structuring my engagement with sound materials. With that in mind, I moved forward into the practical part of this project. This is discussed in ["Initial Technological Experimentation"]({m.re}#initial-technological-experimentation).

## Initial Technological Experimentation
This section describes the use of technology and software at the start of the composition process. It outlines five key stages of a data processing timeline that begins with a corpus of samples generated with databending techniques. Following this, various analytical routines are described alongside the rationale for their design and implementation. 

### Generation
Reconstruction Error began with me generating a corpus of materials using databending techniques. Ultimately, I wanted to create a large sound collection that I would initially have little knowledge and understanding of. From that point, I could use the computer to structure my listening and engagement with that corpus, ideally influencing the piece as a hybrid of my own and the computer's agency.

I began databending by modifying the code of Alex Harker's [`ibuffer~`] so that it could load *any* file, not just those intended to store audio data for playback. Reading audio file formats such as WAVE and AIF typically involves parsing a section of bytes at the start of the file, the "header", which tells the computer how to interpret the structure of another chunk of data which is the audio itself. My forked version of `ibuffer~` operated by adding a new method to the object, `readraw` which could read a file without looking for the header, and instead assumed all the data of that file was 8-bit encoded 44.1kHz sample-rate mono audio. Some of the conversions from this process can be found in [AUDIO 2]({m.re}#aud2). Note that these samples have been reduced in volume by 90% as the originals were excessively loud.

<Album 
tracks={ibuffer}
title="ibuffer~ databending"
id="aud2"
figure="AUDIO 2"
/>

Using the fork of`ibuffer~` rendered results that gave me confidence in the databending approach. While many of the outputs were usually shapeless noise (*AEC.pdf*, *carl.png*) some examples presented novel morphological and textural properties. For example, *clickers.rpp* and *crash.rtf*,  although possessing a background layer of noise were ornamented with brutally artificial and almost gestural synthesis sounds. However, there were two issues with my approach at this point. Firstly, my fork of `ibuffer~` would occasionally crash causing results stored in memory to be lost. In addition to this the conversion process was not deterministic and would be slightly different between consecutive executions even with the same input file. No doubt, this was due to my lack of familiarity in C programming. Secondly, the feedback between converting a file and listening to it was slow because I had to manually manage the task of loading the file and playing it back. While this second issue may seem trivial, the repetitiveness of the task  was time consuming. These two problems together created a lot of friction when trying to explore the results of databending. In response to this, I replicated the functionality of `ibuffer~` with a combination of Python scripting and the [SoX (Sound eXchange )](http://sox.sourceforge.net) command-line tool. 

This combination of Python and Sox formed a powerful command line scripting tool for automatically converting batches of raw files from my personal computer into audio files. Instead of having to manually select files, load them into Max, audition the output and then save the result to disk if I liked it, I could point this script to a starting directory from which it would "walk", converting any file it encountered into audio. I made it so that I could specify a maximum size limit of files to convert in this process so that it would not fill up the hard drive infinitely. The script for this can be found in the git repository for this piece: [scrape_files.py](https://github.com/jamesb93/data_bending/blob/master/python_scripts/scraping/scrape_files.py). 

While the `scrape_files.py` scripting workflow was rapid and could produce numerous files almost instantaneously, I wanted to understand how the conversion process worked at a lower-level and expose various parameters that could effect the output, such as bit-depth, sampling rate and number of channels. SoX allows the user to supply parameters such as this. However, I wanted to take ownership of the technology behind this and program it myself. As such, I built a command line tool, *[mosh]({m.mosh})* with help from [Francesco Cameli](https://github.com/vitreo12) for converting any file into an audio file. More specific information can be found in the ["Technical Implementation"]({m.ti}) section. The development of [mosh]({m.mosh}) provided an opportunity to learn more about the data structure of audio files and from that level of engagement I began to understand the causal effects that the parameters had on the perception of the sound output. Another benefit of devoting time to replicating the functionality of `scrape_files.py` myself was that I can now integrate it into future projects and cultivate it as a fixture in my practice. Furthermore, SoX is not specifically designed for databending, and there is no guarantee that its ability to convert any file into audio was an intended feature or will work the same going forward.

Once I had finished [mosh]({m.mosh}), I put it into practice by databending 5 gigabytes of raw files by walking from the root of my laptop's file system. This produced 8517 audio files which I ambitiously began auditioning manually. Given the sheer quantity, I only managed to audition around 100 audio files before I realised that I was not able to remember what the first one sounded like, or why I may have liked it in the first place. I did find though that amongst the typically noisy sounds there were those with diverse morphologies and textures - precisely the type that I wanted to discover more of in the first place. A small selection of manually auditioned samples were kept aside and be found below in [AUDIO 3]({m.re}#aud3). Clearly, there was going to be a problem navigating through this collection though and at this stage I had produced my "haystack" and began to think about ways in which I could find the "needles" within it.

<Album 
tracks={generation}
figure="AUDIO 3"
caption=" hmm "
id="aud3"
title='Samples generated using the mosh command line tool'
/>

### Segmentation
Samples from the databending process ranged in length from 1 second to 53 minutes. For longer samples especially, they often were comprised discrete musical ideas, gestures or even what may have been considered sections.  An example that exemplifies this finding is  *libLLVMAMDGPUDecs.a.wav* (see [AUDIO 4]({m.re}#aud4)) which has been manually segmented to demonstrate where I perceived such divisions of the databending material.

<Waveform 
file='/re/generation/libLLVMAMDGPUDesc.a.mp3'
peaks = '/re/generation/libLLVMAMDGPUDesc.a.dat'
id='aud4'
title='AUDIO 4: libLLVMAMDGPUDesc.a'
segments={libllvmSlices}
/>

These longer sounds, comprising of many unique internal sounds were not entirely useful to me compositionally. They were too long to compose with directly and I wanted to these sections within each sample so that I could operate on small homogenous samples. As such, I devised a strategy to *automatically* segment the entire corpus. Designing an algorithm for this was based on replicating how I had segmented *libLLVMAMDGPUDesc.a* manually. After experimenting with several algorithms from Librosa (McFee et al. 2015), such as [Laplacian Segmentation](https://librosa.org/doc/latest/auto_examples/plot_segmentation.html) I came to the conclusion that the demarcations I had made were based on differences in the spectrum. There are algorithms for performing automatic segmentation based on this principle, such as measuring the *spectral flux* between windows of contiguous spectral frames and I experimented with this using the FluCoMa `fluid.onsetslice~` object. However, I found that while the algorithm was intuitive to understand it was very sensitive to parameter changes and often difficult to encourage toward an intended result. Similarly, the Laplacian Segmentation approach was beyond my understand to use effectively. While these attempts seemed ineffective at the time the documentation of the Laplacian Segmentation algorithm would often refer to self-similarity or recurrence matrices. I used part of the code from this documentation to create one for *libLLVMAMDGPUDesc.a*, and interpreted the structure of this matrix visually to be reflective of the demarcations that I had made manually before. The recurrence matrix plot be seen in [IMAGE 1]({m.re}#img1). 

<ImageMedia2 
url='/re/recurrence.png'
figure="IMAGE 1"
caption='Recurrence matrix visualisation for libLLVMAMDGPUDesc.a.wav.'
id='img1'
/>

I followed this line of inquiry and made a thread on the [FluCoMa discourse](https://discourse.flucoma.org/) asking how I might approach segmentation based on these types of representations or if there were any existing tools which already did something similar to this. The thread can be found titled ["Difficult segmentation"](https://discourse.flucoma.org/t/difficult-segmentation/222). Owen Green responded (see [IMAGE 2]({m.re}#img2)), and made it clear to me that the`fluid.noveltyslice~` object (part of the FluCoMa first toolbox) was based on Andrew Foote's novelty slicing algorithm (Foote, 2000). This algorithm is predicated on shifting a sliding window of a specified size (the kernel) across a recurrence matrix created from spectral data. This produces a one dimensional time series, a novelty *curve* that in theory captures the *novelty* of the audio. A peak detection strategy can then be applied to this time series in order to derive time points of significance within the sound, based on the data from which the original recurrence matrix was produced. An example of a *novelty curve* can be found in [IMAGE 3]({m.re}#im3), extracted from Foote (2000, p. 454). 

<ImageMedia2 
url='/re/owen.jpeg'
figure='IMAGE 2'
caption='Owen Green answering my question in the FluCoMa discourse'
id='img2'
/>

<ImageMedia2
url='/re/noveltycurve.jpg'
figure='IMAGE 3'
caption='Example of a novelty curve, taken from Foote (2000, p. 454)'
/>

Using a *threshold* of 0.61, *kernelsize* of 3, *filtersize* of 1, *fftsettings* of 1024 512 1024 and a *minslicelength* of 2, I produced the segmentation which can be seen in [AUDIO 5]({m.re}#aud5). To me, this segmentation result was commensurate with my initial intuitive approach seen in [AUDIO 4]({m.re}#aud4). While at some points the results of `fluid.noveltyslice~` were over-segmented (slices one to five) it seemed like it could discern the changes I perceived as salient, and was able to extract the longer musical phrases such as slice seven and twelve without having to tune the parameters intensely.

<Waveform 
file='/re/generation/libLLVMAMDGPUDesc.a.mp3'
peaks = '/re/generation/libLLVMAMDGPUDesc.a.dat'
id='aud5'
title='AUDIO 5: libLLVMAMDGPUDesc.a segmented with the fluid.noveltyslice~ algorithm'
segments={noveltySlices}
/>

I then segmented every corpus item with these settings accepting that some files might be segmented poorly or not close to how I would do it manually, as I did not want to spend an indefinite amount of time working on just the segmentation aspect of this workflow. At this point I had accrued 21551 corpus items.

### Analysis
The next stage involved analysing the corpus of segmented samples. I planned to use audio descriptors that I was comfortable with and had used before, such as spectral centroid, loudness, pitch and spectral flatness. However, I wanted to focus on the textural qualities of the corpus items and capture the morphology through analysis. This led me to use MFCCs as they are robust against differences in loudness and are capable of differentiating the textural characteristics of sounds. For each corpus item, an MFCC analysis was performed  using `fluid.mfcc~` with an *fftsize* of 2048 1024 2048, *numbands* of 40 and *numcoeffs* 13. I then calculated seven statistics- mean, standard deviation, skewness, kurtosis, minimum, median and the maximum and up to the third derivative for each coefficients.  I anticipated that calculating the derivates of the statistics would capture the morphology of the sound by describing both the change between frames as well as the change in the change. These statistical summaries were flattened to one-dimension and each column of these vectors containing 273 values was standardised.

### Dimension Reduction
https://github.com/jamesb93/data_bending/tree/master/python_scripts/dimensionality_reduction

At this point I had a wealth of corpus items and MFCC analysis data. While MFCC values are strictly defined, they lack the approachability and potential to be understood intuitively compared to descriptors such as spectral centroid.  It is unlikely that a person could discern any perceptual value from just reading MFCC values. However, an audio descriptor such as spectral centroid is more likely to map onto our perception of a sound, perhaps describing describing the relative brightness when used comparatively between several sounds. 

In other similar corpus navigation and exploration work, dimensionality reduction techniques have been implemented in order for the computer to compress dense data such as that derived from spectral analysis and to facilitate exploring the latent structure of a corpus. Examples include the work of Stefano Fascinai (Fasciani, 2015), *Flow Synthesizer* (Esling, Masuda, Bardet, Despres, & Chemla-Romeu-Santos, 2019), *[AudioStellar](https://audiostellar.xyz/)* (Garber et. al. 2020), Thomas Grill (Grill & Flexer, 2012), LjudMap (Vegeborn, 2020) and the previously mentioned *FluidCorpusMap*. My aim for using dimensionality reduction was to start moving away from mostly analytical work, and to have the computer produce outputs that could facilitate exploring the corpus. One of my primary goals in this regard, was to find a way that the computer could draw together perceptually similar sounds and present this information to me in a simple and informative manner.

As such, I used the Uniform Manifold Approximation and Projection (UMAP) algorithm and reduced the 273 points in each vector for each sample to 2 points. This way, I could visualise the results as a two-dimensional *map* where each sample would be represented as a singular point. A strength of UMAP is its potency for capturing non-linear features in data compared to algorithms such as Principal Component Analysis (PCA). Furthermore, UMAP can be coerced to favour global or local structure by altering the *minimum distance* and *number of neighbours parameters*. A number of projections which were made with various parameters can be seen in [DEMO 1]({m.re}#demo1). This can have a significant effect on the projection, and I found that it was an expressive interface for changing how the computer represented the corpus analysis data. 

I responded to these  visual projections to intuitively interpret the results of the dimension reduction process. For me, the topology of the projections became a source of inspiration compositionally by observing the location, shape and relationships between clusters of samples. In particular, the UMAP process would often produce characteristic outputs, where clusters could be located away from the main body of the projection or the overall shape would be comprised of a consistent character or feature. These were novel to me and were investigated further through manual audition. This will be described in more detail in ["Responding To Computer Outputs"]({m.re}#responding-to-computer-outputs). Most importantly I found that the UMAP algorithm produced a layout where perceptually similar samples were located relatively close to each other in the space. Navigating around a single area of the space rendered samples that were similar to each other. Moving across the space more broadly positioned different morphological sounds away from each other as well as creating small pockets where samples were unique and novel. These successes aside, I did not have a way to really incorporate these findings into a compositional workflow where pieces were being made. It was mostly motivating and inspirational in the sense that I found the technology exciting and capable of dealing with large listening tasks competently, rather than bringing me closer to the creation of a piece. It also functioned as a way of rapidly discerning the overall nature of the corpus. Up to this point I had never really listened to most of the sounds and was operating in good faith that there would be something interesting buried. I was encouraged by this initial form of soundful exploration.

<!-- TODO explain that these parameters were used -->
7 Neighbours and 0.1 mindist were selected

### Clustering
https://github.com/jamesb93/data_bending/tree/master/python_scripts/clustering

Part of my positive response to the dimension reduction process was the robustness in how samples with similar perceptual qualities were drawn together in the two dimensional projection. I wanted to take this further, and have the computer perform clustering on this data in order to return groups of samples. I imagined this type of output would be useful as a starting point for composition.

To do this I experimented by running several clustering algorithms on UMAP reduction data,  including [k-means](https://en.wikipedia.org/wiki/K-means_clustering), [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) and [Agglomerative Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html). Each of these algorithms offers a different interface and will cluster input data based on a different set of assumptions. K-means, for example, operates by partioning data into a fixed number of partitions set by the user. Individual points are then drawn into these partitions to create clusters. This process assumes that the data is spherically distributed and that each cluster will have approximately the same number of clusters. This can be a problematic assumption to make and a number of conclusions can be drawn if these limitations are not accepted. In particular, I found [this discussion on the drawbacks of k-means](https://stats.stackexchange.com/a/133841) useful when researching different clustering methods.  

HDBSCAN, on the other hand determines an appropriate number of clusters automatically depending on a set of constraints that determine *linkage* between data points. It uses a minimum spanning tree to hierarchically divide the space into clusters and so it can often respect the presence of nested structures within a larger dataset. This means that it can respect non-spherical shaped groupings of data too.

Between such algorithms their differences in interface and underlying algorithm invite different questions to be asked and enable the computer to discern different features from data.  I found the notion of asking for a specific number of clusters from k-means valuable because it meant that you I could determine the density of each cluster roughly. If I wanted to create very specific groupings, for example, I could just ask for more clusters. However, the minimum spanning tree data structure intriguied me as a computational output that might be able to inform my inspection and understanding of the corpus. Instead of just assigning a cluster membership to each sample, the tree itself projects a web of 'leaves' throughout the data which indicate connections between different parts. An example of this can be found in [IMAGE 4]({m.re}#img4).

<ImageMedia2 
url='/re/hdbscan.png'
caption='HDBSCAN minimum spanning tree visualisation. Data points are represented by circular nodes in the space with connections of the tree indicated by lines connecting them. Taken from https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html.'
figure='IMAGE 4'
id='img4'
htmlCap={true}
/>

https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering

To get the benefits of both of these features, I settled on using another algorithm, *Agglomerative Clustering*,  which allows the user to specify a desired number of clusters while also working in the same fashion to HDBSCAN and creating a minimum spanning tree. Despite my best efforts to parse and understand the minimum spanning tree, I found it difficult to reason around programatically and to draw any value from it. Nonetheless, my exposure to the minimum spanning tree concept informed my further application of clustering. I used the agglomerative clustering algorithm to calculate various results on the reduced data, asking for four increasing numbers of clusters. This returned a collection of clustering outputs with different granularities and specifities. The first output produced 250 clusters, then 500, then 1600 and finally 3200. These were chosen based on the number's relationship to the total number of corpus items.

I then performed a meta-analysis discerning the shared membership of each sample to clusters across the various outputs. From this I could determine different analysis. Because each analysis level is producing a higher number of clusters, they always return increasingly smaller clusters. As such, a cluster containing 500 samples in the 250 clusters analysis might have the same samples spread across two different clusters. The result of this is another set of JSON files that describes clusters which share samples and how many samples they share. In [CODE 1]({m.re}#code1), a portion of the analysis describing the shared membership of the 250 and 500 analysis is shown. The information in this example expresses that cluster 36 in the 250 clusters analysis is comprised of clusters 317 *and* 327 in the 500 clusters analysis.  

<Code
figure='CODE 1'
caption='Meta-analysis data describing the shared membership of clusters. This portion describes the shared membership of clusters between the 250 and 500 clusters output.'
>

```json
{
    "36": {
        "317": 8,
        "327": 16
    },
    "89": {
        "277": 52,
        "105": 82,
        "491": 34
    },
    "70": {
        "39": 63,
        "214": 75,
        "425": 15
    },
}
```

</Code>

### Moving Forward
At this stage I moved into *soundful* play with the various computer outputs resulting from the process described above. Using the data from led the development of these pieces, and the next section [Responding To Computer Outputs]({m.re}#responding-to-computer-outputs) discusses this.

## Responding To Computer Outputs

After producing the clustering data I used it to drive lightweight processes to audition samples in Max and REAPER. In Max, I created an interface in which I could retrieve the samples from clusters and simultaneously navigate between the shared membership connections. The samples were played back contiguously by a sequencer so that I could quickly hear the samples in the selected group. This allowed me to audition samples rapidly and to ascertain the nature of each cluster quickly. In part, the decision to structure the playback in this way also serendipitously lead me to arranging some of the materil in similar ways in the compositions. It also allowed me to start with a broad cluster of samples and hone the selection by following the connections of shared membership. This allowed me to find very specific "pockets" of sounds within the entire corpus by whittling down the selection from a broad collection of perceptually similar sounds, to those that were more similar. A screen capture demonstrating this auditioning process can be found in [VIDEO 10]({m.re}#vid10).

<VideoMedia2 
url="/re/meta-analysis.mp4"
figure="VIDEO 10"
caption="Using Max and the meta-analysis data to explore the corpus in a structured manner."
id="vid10"
/>

This process was immensely useful for becoming acquainted with the samples from the corpus in a structured manner. Most importantly, this engagement was based on perception and each cluster that I selected would present a microcosm of the larger corpus with novel and coherent sounds existing in each one. Despite the fact that the source material was generated from disparate sources of raw data on my hard drive, connections began to form in my mind about how the material could be used in composition and what an EP of different pieces might explore as a collection of pieces. I created a set of written annotations for each cluster and their imagined usefulness. Although rough and not particularly clear, I think that it demonstrates the level of engagement that this process enabled. Chronologically, the notes capture increasingly specific imagined applications, ranging from just recording which samples I thought were similar, to composing pathways between different clusters. These notes can be found at this link: https://github.com/jamesb93/data_bending/blob/master/clusters.md.

I did not want to compose entirely in Max though. This had already proven to be problematic in [Stitch/Strata]({m.ss}), especially in the earlier phases of composition for that project. Instead, I wanted to be able to access these clusters and manipulate their contents in a more musically fluid process as well as modify and alter them with audio effects. In order to achieve this workflow,  I built a series of Lua scripts that allowed me to turn a cluster from a JSON file into a track in REAPER by leveraging the ReaScript API. This can be found in [CODE 2]({m.re}#code2). 

<Code
figure='CODE 2'
caption='Lua script for importing a cluster of samples as a track of contiguous samples into REAPER'
id='code2'
>

```lua
local info = debug.getinfo(1,'S');
local script_path = info.source:match[[^@?(.*[\/])[^\/]-$]]
package.path = package.path .. ';' .. script_path .. '?.lua'
package.path = package.path .. ';' .. script_path .. 'lunajson/src/?.lua'
loadfile(script_path .. "../ReaCoMa/lib/reacoma.lua")()
local lunajson = require 'lunajson'

local confirm, user_input = reaper.GetUserInputs('Provide JSON info', 2, 'JSON Path:, Cluster Number:, extrawidth=100', '')

if confirm then
    reaper.Undo_BeginBlock()

    -------- start processing --------
    reaper.SetEditCurPos(0.0, false, false)
    reaper.InsertTrackAtIndex(0, true)

    -- Parse user input --
    local fields = reacoma.utils.commasplit(user_input)
    local json_path = fields[1]
    local cluster_num = fields[2]

    -- Do json parsing --
    local file_in = assert(io.open(json_path, "r"))
    local content = file_in:read("*all")
    local cluster_data = lunajson.decode(content)
    for k, p in ipairs(cluster_data[cluster_num]) do
        reaper.InsertMedia('/Users/james/Cloud/Projects/DataBending/DataAudioUnique/'..p, 0)
    end
    reaper.SetEditCurPos(0.0, false, false)
    reaper.Undo_EndBlock("Insert Media Cluster", 0)
end
```

</Code>

Running this script would present me with the option to provide a JSON file and a cluster number. This would generate a new track containing the samples belonging to that cluster. This allowed me to rapidly audition samples in Max, then draw them into a REAPER session without having to deal with clumsy file system interfaces. Instead, I could work with the same data in a familiar fashion. [VIDEO 11]({m.re}#vid11) demonstrates the process of importing a cluster into REAPER. This was the basis on which I composed the next few pieces. I would oscillate between listening, importing sounds into REAPER as clusters and then using intuitive compositional processes to organise that material. When I hit a creative block in the DAW I would return to listening and try to find something new in the material, guided by the computer.

<VideoMedia2 
url="/re/scripting.mp4"
figure="VIDEO 11"
caption="Using ReaScript API to create tracks of samples from cluster data."
id="vid11"
/>

The next sections, [_.dotmaxwave]({m.re}#_dotmaxwave), [X86Desc.a]({m.re}#x86desca), [segmenoitte]({m.re}#segmenoitte), [--isn]({m.re}#--isn) and [sys.ji_]({m.re}#sysji_) describe the specific compositional processes for each piece of Reconstruction Error and how this combination of Max and REAPER informed my compositional choices.

### _.dotmaxwave

*_.dotmaxwave* was the first piece I composed in the EP. The samples that I used in this piece were all derived from cluster 131 which I discovered through audition in Max. While listening to this cluster I realised that the source material was all derived from a particular raw data type with the .maxwave extension. Each of the samples generated from moshing created a file with a similar morphological template with slight alterations. All of the samples begin in a similar fashion with four repeated iterated pitched sounds leading into a static texture. This texture is predominantly pitch material with some underlying noise.

I wanted to capitalise on what I had found, and create a track that focused on the relationship between pitch and noise in these sounds, and the shared morpholog between them.

I started by pulling cluster 131 into REAPER and listening to the samples. I parsed the large section and identified samples which I found to have the most novel differences in the opening sequence. As such I was able to create a pallette of small iterations on the same idea. These are presented in [AUDIO 6]({.re}#aud6).

<!-- DO AN ALBUM OF THE THINGS -->
<!-- <Album/> -->

harmonic percussive separation to listen to each component in isolation. The noise components were relatively quiet compared to the pich component. I boosted the volume of these and found that while the harmonic part is relatively stable, the noisy part has its own complex noise-based morpholgy and character. I wanted to emphasise this and recombine the sound by summing the two components back together after increasing the volume of the percussive component.

The rest of the piece followed from this type of manipulation of the two components. The first section highlights the different morphological character of each variant, the middle section focuses on pure pitch and the last section is a simple crescendo from a sample which possessed perhaps the most complex percussive component which was only discovered after performing decomposition.

This piece also gave me the idea to create [ReaComa]({m.}). I was constantly stepping in and out of the command line to process files...


### --isn
Robert Wanamaker
### X86Desc.a
Otoacoustic Thomas Ankerschmidt
### segmenoitte
### sys.ji_

Noise is good but only in the context of not-noise.
 
## Reflection

The computer structured my engagement with materials.
I was *led* to different ideas in a hybrid of my intuition and using the computer to structure the initially unstructured corpus.

Computer never imposed on my process, rather acted as a sounding board. In other projects I have had to fight the influecne of the computer, or find ways to curtail it. Although the computer has no intentionality, This is the first project where it felt like there was mutuality in the compositional approach. At times I would compose intuitively, returning to the computer intending to find a specific result - perhaps a new sound or a different way of listening to a cluster of samples. This itself would allow me to find something new in the corpus or to discover a tangent I hadn't thought of before. The pieces almost emerged from each other in a way, despite my best efforts to make each one unique and to explore different elements of the corpus.
### Intervention and Iteration

Intervention as paramount
- Compositional fluidity
- Being thrown ideas works! (remember how you explained it to Oli Bown...)
- Honing initially 'blunt' questions
- Forming clear compositional intent
- Responding to constraints _with_ the computer 

### A New Workflow
#### Issues of friction leading to FTIS

Simplify the workflow and make it a tool, rather than set of ad-hoc scripts.

Create data that could be used anywhere, REAPER, Max whatever. Almost completely agnostic to environment supporting flexibility in approach and no "lock in".

1. Discovering structure in a corpus
2. Finding groupings of perceptually similar materials
3. Discovering unknown and unique items
4. Capitalisation on known points of interest
5. Forming/finding connections between corpus items 

#### Re-embracing the DAW
- Stitch/Strata, i explain some of the benefits that re-embracing the DAW did for that project, especially for managing intermediate states of projects and "scaffolding" or "sketching"
Linking the DAW to extra information (i.e extending the role of the DAW through FTIS by letting me work with information that is not well represented in a timeline view)
navigating-sample-based-music 


## Other Related Links and Media

I gave two presentations that discussed this project at different points in its development. The first presentation I gave was at the November 2019 FluCoMa plenary, where the pieces were partially composed and I focused on demonstrating my application of the FluCoMa tools. Much longer after the pieces had finished and I moved on to composing [{m.emname}]({m.em}), I reflected on the entire development process and its relationship to the compositions in a [short article](https://zenodo.org/record/4285398) for the 2020 AI Music Creativity Conference. This involved creating a short video demo too.

<YouTube 
url="https://www.youtube.com/embed/IpD_XzW1Az4" 
figure='VIDEO'
caption='"Finding Things In Stuff" presentation at the November 2019 FluCoMa Plenary'
/>

<YouTube 
url="https://www.youtube.com/embed/-FNO0QovfsI"
caption='"Corpus exploration with UMAP and agglomerative clustering" presentation at the AI Music Creativity Conference 2020.'
figure='VIDEO'
/>

<NextSection 
next="ElectroMagnetic"
link={m.em}
/>
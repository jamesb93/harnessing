<script>
    import {metadata as m} from '../directory.svx'
    import NextSection from '$lib/components/NextSection.svelte';
    import VideoMedia from '$lib/components/VideoMedia.svelte';
    import VideoMedia2 from '$lib/components/VideoMedia2.svelte';
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
    import YouTube from '$lib/components/YouTube.svelte';
    import Album from '$lib/components/Album.svelte';
    import Code from '$lib/components/Code.svelte';
    import Waveform from '$lib/components/Waveform.svelte';

    // Album Tracks

    import { reAlbum, ibuffer, generation, libllvmSlices, noveltySlices } from '$lib/data/reconstruction-error.js'
</script>

# Reconstruction Error

<Album 
tracks={reAlbum}
title="Reconstruction Error"
id="audio1"
figure="AUDIO 1"
/>

Reconstruction Error is the fourth project in this portfolio. It is a set of five pieces, presented in an EP format.

## Motivations and Influences
In the previous three projects, I explored a number of techniques, approaches and workflows for computer-aided composition. These approaches took several forms and were motivated by different factors such as opportunities to compose for visiting artists or by responding to immediate influences in my creative coding practice and musical listening. Throughout each project in the process of testing new technologies and strategies for working with the computer, I found that the most successful workflow was developed in [Stitch/Strata]({m.ss}). Despite being a difficult process and inducing many failed attempts to reach the finished state of the piece, using the computer to sieve through a corpus of sounds in order to help guide me towards organisations of material felt like a fluid and meaningful workflow that I could develop in the future. Given these conditions produced by reflection and consideration of the work done up to this point, I decided to move forward with this in mind. I wanted to create a piece by using the computer in a similar fashion.

### Databending
Amongst this reflection on my own practice up to this point, I had been listening to artists such as Alva Noto, Autechre and Ryoji Ikeda. Their work draws on hyper-digital, surgical and sometimes ultra-minimalist sonic materials associated with post-digital aesthetics. The raw digital sounds found in such artists work have always been appealing to me, and to some extent a part of my compositional vocabulary and aesthetic. [Stitch/Strata]({m.ss}), while focusing on the sound of experimental vocal techniques at times arranged sounds into glitch-like, artificial and quasi-rhythmic structures.

I was also becoming interested in databending, a process in which raw data is used for the purposes of constructing visual and sound materials in art. This can be achieved a number of ways, and the process one chooses to realise this is as instrumental in the practice as performing the act itself. A number of tools exist that facilitate databending either intentionally or through divergent use. Sch programs include [SoundHack](https://www.soundhack.com/) which allows you to convert any file on a computer into a valid WAVE audio file, [Audacity](https://www.audacityteam.org/) which supports the reading of raw bytes as audio data, or [PhotoShop](https://digital-photography-school.com/make-abstract-glitch-art-photographs/) by opening non-image files as if they were images. 

A number of artists have experimented with databending in their own work and this guided my own research of this digital practice. [Nick Briz](http://nickbriz.com/), an "new-media artist"  explores various "attacks" on data to degrade and produce distortions of video data in *Binary Self-Portraits* (see [VIDEO 1]({m.re}#vid1)). Another video-art piece, *From The Group Up In Order, Embrace* (see [VIDEO 2]({m.re}#vid2)), explores the creation of visual material through the interface of the binary code that represents itself. Briz states: 

<div class="bigquote">

*Anything that is digital contains beneath it layers of code and at the base is an immense series of ones and zeroes known as binary code. Like the atom in the natural world, the ones and zeroes of binary code are the smallest component of the digital medium. In my efforts to strengthen my relationship with the digital medium I set out to create a piece where I dealt directly with the binary code of a video file. The result was this video as well as an obsession with the digital medium, and a first in a series of binary video pieces.* (Briz, n.d )

</div>

<VideoMedia id='vid1'>
<iframe slot='media' src="https://player.vimeo.com/video/4510943" width="640" height="524" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot="caption">VIDEO 1: Binary Self-Portraits by Nick Briz</p>
</VideoMedia>

<VideoMedia id='vid2'>
<iframe slot='media' src="https://player.vimeo.com/video/4506517" width="640" height="524" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot="caption">VIDEO 2: From The Ground Up In Order, Embrace by Nick Briz</p>
</VideoMedia>

*Unitxt Code (Data to AIFF)* (see [VIDEO 3]({m.re}#vid3)) by [Alva Noto](http://www.alvanoto.com/) although not explicitly stated in liner notes or interviews, likely uses data converted to audio (as suggested by the name) for the core compositional material of that work. The harsh and degraded noise sounds are commonly produced when converting text files into audio. [*Datamatics*](https://www.ryojiikeda.com/project/datamatics/) (see [VIDEO 4]({m.re}#vid4)), by[Ryoji Ikeda's](https://www.ryojiikeda.com)  incorporates raw data as source material which drives both the visual and audio of this installation work. [Pimmon's] album *Electronic Tax Return*  (see [VIDEO 5]({m.re}#vid5))has several works based on synthesising sound by converting dynamic link libraries (.dll files) into files for playback and manipulation.

<YouTube 
url="https://www.youtube.com/embed/vN9FrqrPptg"
figure="VIDEO 3"
caption="Unitxt Code (Data to AIFF) by Alva Noto"
id='vid3'
/>

<YouTube 
url="https://www.youtube.com/embed/eaIrSZIyxxk" 
figure="VIDEO 4"
caption="Datamatics (Limited Section) by Ryoji Ikeda"
id='vid4'
/>

<YouTube 
url="https://www.youtube.com/embed/AbF8NFanUdQ"
figure="VIDEO 5"
caption="Electronic Tax Return by Pimmon"
id='vid5'
/>

These works of sound art and composition were influential in my exposure and research into databending. 

In particlar though, [Michael Chinen's](https://michaelchinen.com/) practical experiments demonstrated a tactile, hands on approach to databending in which he developed various fledged programs. 

These programs, explore sonification of raw bytes held in memory for different applications across a variety of different use cases. [FuckingAudacity](https://michaelchinen.com/music/fuckingaudacity/) is a fork of the popular audio application Audacity. Mischievously, instead of actually being able to play any audio files, the sound output from this program is a snapshot of the memory state at regular intervals. Interactions with the user interface and user-driven processes such as importing files or enabling effects themselves become ways of generating sound. [VIDEO 6]({m.re}#vid6) is a demonstration of Chinen playing with the program.

<VideoMedia id='vid6'>
<iframe slot='media' src="https://player.vimeo.com/video/8144372?byline=0&portrait=0" width="640" height="480" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot='caption'>VIDEO 6: FuckingAudacity demonstration by Michael Chinen.</p>
</VideoMedia>

[FuckingWebBrowser](https://michaelchinen.com/music/fuckingwebbrowser/) (see [VIDEO 7]({m.re}#vid7)) follows in suit with its Audacity counterpart. The application presents as a standard web browser. However, the memory state of the programÂ is used to generate audio data which is then played back as the user interacts with it. Navigating to or from a website or interacting with a page generates sounds in unpredictable ways. Even doing nothing at times reveals the hidden internal workings as sounds. Making http requests, polling for data or caching files in the background result in sounds that are detached from the user's interaction with the application.

<VideoMedia id='vid7'>
<iframe slot='media' src="https://player.vimeo.com/video/9635993?byline=0&portrait=0" width="640" height="480" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot='caption'>VIDEO 7: FuckingWebBrowser demonstration by Michael Chinen.</p>
</VideoMedia>

The logical conclusion for this pattern of application development is one that can be attached to anything running on a computer and be able to sonify the memory state as it runs. [lstn](https://michaelchinen.com/2010/12/09/listen-to-your-computer-lstn/) performs this function, and can observe and sonify the opcodes, callstack and active memory of any program on the computer. [VIDEO 8]({m.re}#vid8) is a demonstration of this in practice.

<VideoMedia id='vid8'>
<iframe slot='media' src="https://player.vimeo.com/video/17617200?byline=0&portrait=0" width="640" height="480" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p slot='caption'>VIDEO 8: lstn demonstration by Michael Chinen.</p>
</VideoMedia>

### Employing The Computer To Structure A Corpus
Coming into contact with Chinen's work was influential because there is a mixture of predictable and unpredictable behaviours exhibited by the computer through his various databending programs. I was able to  sense the overarching character of the sounds would be (digital, harsh and noise-based), but occasionally the results were surprising. Silence, fragility and delicacy emerged at times and this notion that there were "needles in the haystack" was a tension that I wanted to explore myself. Part of my motivation for this project was to create a scenario where I could be surprised by the process of extracting sounds and wading through them with the aid of the computer.

As part of my preliminary research toward these motivations, I came into contact with the  outputs of the Fluid Corpus Manipulation project. I had up to this point been loosely involved with some of their research activities and privy to internal workshops with their commissioned composers as a helper. As such, I was following what they were doing, and they released a video demonstration for some software called *[FluidCorpusMap](https://github.com/flucoma/FluidCorpusMap)* (see [VIDEO 9]({m.re}#vid9)).  Neither the paper (see Roma et. al. (2019)) or source code for this program were released at the time. However, I was able to discern from the information in the video that the program was using a processing pipeline of producing mel-cepstrum frequency coefficients (MFCCs) analysis, calculating statistics on this data and then using dimensionality reduction algorithm to create a two dimensional latent space for exploration and navigation. I found two aspects of this process fascinating. Firstly,  the *global* shapes produced from the dimensionality reduction stage possessed novel and identifiable characteristics depending on the algorithm. The various layouts to me, were evocative, in that the computer seemed to be creating its own understanding of the corpus which could structure the engagement with those sounds by exploring the space. Secondly, the *local* distribution of samples was perceptually smooth and coherent. Movements between adjacent points in the demonstration returned sounds which were texturally similar. Even aspects such as morphology seemed to be captured (this can be seen around 1:19 where sounds with upwards glissandi are grouped together) giving the sense that the computer could capture both texture *and* morphology. This to me, was far more sophisticated than my previous experiments using instantaneous statistical summaries.

<VideoMedia id='vid9'>
    <iframe slot='media' src="https://player.vimeo.com/video/314301724?byline=0&portrait=0" width="640" height="360" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
    <p slot='caption'>VIDEO 9: FluidCorpusMap demonstration.</p>
</VideoMedia>

In previous projects, the notion of computational sound searching had been already been explored, albeit in different ways to what was found in FluCoMa's research. The workflow of [Stitch/Strata]({m.ss}) was predicated on forming groups of material from a corpus of voice samples by audio descriptor query. Similarly, [Annealing Strategies]({m.as}) was motivated by the desire to have the computer find parameter inputs that would produce particular perceptual quality from the Fourses synthesiser. To me, this approach of mapping samples seemed like it could be useful for enabling the computer to suggestive andÂ for it to be instrumental in structuring my engagement with sound materials. With that in mind, I moved forward into the practical part of this project. This is discussed in ["Composition Process"]({m.re}#composition-process).

## Composition Process
This section describes the development of software used in the piece and its connection to the musical ideas that came to fruition as a result of this intertwined process.

### Generation
Reconstruction Error began with me generating a corpus of materials using databending techniques. The motivations which led to this decision are described in [Motivations and Influences]({m.re}#motivations-and-influences). Ultimately, I wanted to create a large sound collection that I would initially have little knowledge and understanding of. From that point, I would use the computer to structuring my listening and engagement with that corpus.

I began creating my own databent sounds by modifying the code of Alex Harker's [`ibuffer~`] so that it could load *any* file, not just those intended to store audio data for playback. Reading audio file formats such as WAVE and AIF typically  involve parsing a section of bytes at the start of the file (the header) which tells the computer how to interpret the structure of another chunk of data which is the audio itself. My forked version of `ibuffer~` operated by adding a new method to the object, `readraw` which could read a file without looking for the header, and instead assumed all the data of that file was 8-bit encoded 44.1kHz sample-rate mono audio. Some of the conversions from this process can be found in [AUDIO 2]({m.re}#aud2). Note that these samples have been reduced in volume by 90% as the originals were excessively loud.

<Album 
tracks={ibuffer}
title="ibuffer~ databending"
id="aud2"
figure="AUDIO 2"
/>

Using `ibuffer~` in this way rendered results that gave me confidence in the databending approach. While many of the outputs were mostly consistent shapeless noise (*AEC.pdf*, *carl.png*) some examples presented novel morphological and textural properties. For example, *clickers.rpp* and *crash.rtf*,  while possessing a background layer of noise were ornamented with brutally artificial and almost gestural material. However, there were two issues with my approach at this point. Firstly, my fork of `ibuffer~` would occasionally crash causing results stored in memory to be lost. In addition to this the conversion process was not deterministic and sometimes the conversion process would be slightly different even with the same input file. No doubt, this was due to my lack of familiarity in C programming. Secondly, the feedback between converting a file and listening to it was slow because I had to manually manage the task of loading the file and playing it back. While this second issue may seem trivial, the repetitiveness of the task can quickly make it consuming. These two problems together created a lot of friction when trying to explore the results of databending. In response to this, I replicated the functionality of `ibuffer~` with a combination of Python scripting and the [SoX (Sound eXchange )](http://sox.sourceforge.net) command-line tool. 

The combination of Python and Sox formed a powerful command line scripting tool for automatically converting batches of raw files from my personal computer into audio files. Instead of having to manually select files, load them into Max and audition the output I could point this script to a starting directory from which it would "walk", converting any file it encountered into audio. I made it so that I could specify a maximum size limit of files to convert in this process so that it would not fill up the hard drive infinitely. The script for this can be found in the git repository for this piece: [scrape_files.py](https://github.com/jamesb93/data_bending/blob/master/python_scripts/scraping/scrape_files.py). 

While the `scrape_files.py` scripting workflow was rapid and could produce numerous files almost instantaneously, I wanted to understand how the conversion process worked at a lower-level and expose various parameters that could effect the output, such as bit-depth, sampling rate and number of channels. SoX allows the user to supply parameters such as this. However, I wanted to take ownership of the technology behind this and program it myself, which for me makes it feel more integrated into overall creative process. As such, I built a command line tool, *[mosh]({m.mosh})* with help from [Francesco Cameli](https://github.com/vitreo12) for converting any file into an audio file. More specific information can be found in the [Technical Implementation]({m.ti}). The development of [mosh]({m.mosh}) provided an opportunity to learn more about the data structure of audio files and from that level of engagement with the technology I began to understand the causal effects that the parameters had on the perception of the sound output. Another benefit of creating [mosh]({m.mosh}) is that I can now integrate it into future projects and cultivate it as a fixture in my practice. Furthermore, SoX is not specifically designed for databending, and there is no guarantee that its ability to convert any file into audio was an intended feature or will work the same going forward.

Once I had finished [mosh]({m.mosh}), I put it into practice by databending 5 gigabytes of raw files by walking from the root of my laptop's file system. This produced 8517 audio files which I ambitiously began auditioning manually. Given the sheer quantity though I only managed to audition around 100 audio files before I realised that I was not able to remember what the first one sounded like, or why I may have liked it in the first place. However, I was fortunate to find that amongst the typically noisy sounds there were those with diverse morphologies and textures - precisely the type that I wanted to discover more of. A small selection of manually auditioned samples were kept aside and be found below in [AUDIO 3]({m.re}#aud3). Clearly, there was going to be a problem navigating through this collection though and at this stage I had produced my "haystack" and began to think about ways in which I could find the "needles".

<Album 
tracks={generation}
figure="AUDIO 3"
caption=" hmm "
id="aud3"
title='Samples generated using the mosh command line tool'
/>

### Segmentation
Samples from the databending process ranged in length from 1 second to 53 minutes. For longer samples especially, they often were comprised of what might be considered discrete musical ideas, gestures or sections depending on the length.  An example that exemplifies this finding is  *libLLVMAMDGPUDecs.a.wav* (see [AUDIO 4]({m.re}#aud4)) which I have manually segmented to show where I would intuitively segment and separate out the various ideas within this single sample. This functioned as a type of "ground truth" for trying to develop an automatic segmentation process later.

<Waveform 
file='/re/generation/libLLVMAMDGPUDesc.a.mp3'
peaks = '/re/generation/libLLVMAMDGPUDesc.a.dat'
id='aud4'
title='AUDIO 4: libLLVMAMDGPUDesc.a'
segments={libllvmSlices}
/>

These sounds in their original form were not entirely useful to me compositionally. They were too long to compose with directly and I wanted to separate discrete sections within each sample so that I could operate on small homogenous bits of sound. As such, I devised a strategy to *automatically* segment the entire corpus. Devising an appropriate algorithm for this was based on replicating how I had segmented *libLLVMAMDGPUDesc.a*. After experimenting with several algorithms from Librosa (McFee et al. 2015), such as [Laplacian Segmentation](https://librosa.org/doc/latest/auto_examples/plot_segmentation.html) I came to the conclusion that the demarcations I had made were based on differences in the spectrum. There are algorithms for performing automatic segmentation based on this principle, such as measuring the *spectral flux* between windows of contiguous spectral frames and I experimented with this using the FluCoMa `fluid.onsetslice~` object. However, I found that while the algorithm was intuitive to understand it was very sensitive to parameter changes. Similarly, the Laplacian Segmentation approach was too far beyond my grasp to understand and use effectively. from this research into segmentation techniques, was the recurrence matrix visualisations that describe the self-similarity of data in time which are used . I created one of these for *libLLVMAMDGPUDesc.a*, and interpreted the structure of this matrix visually to be reflective of the demarcations that I had made manually before. The recurrence matrix plot be seen in [IMAGE 1]({m.re}#img1). 

<ImageMedia2 
url='/re/recurrence.png'
figure="IMAGE 1"
caption='Recurrence matrix visualisation for libLLVMAMDGPUDesc.a.wav.'
id='img1'
/>

I made a thread on the [FluCoMa discourse](https://discourse.flucoma.org/) asking how I might approach segmentation based on these types of representations or if there were any existing tools which already did something similar to this. The thread can be found as ["Difficult segmentation"](https://discourse.flucoma.org/t/difficult-segmentation/222). Owen Green responded (see [IMAGE 2]({m.re}#img2)), and made it clear to me that the`fluid.noveltyslice~` object (part of the FluCoMa first toolbox) was based on Andrew Foote's novelty slicing algorithm (Foote, 2000). This algorithm is predicated on shifting a sliding window of a specified size (the kernel) across a recurrence matrix created from spectral data. This produces a one dimensional time series, a novelty *curve* that in theory captures the *novelty* of the audio. A peak detection strategy can then be applied to this time series in order to derive time points of significance within the sound, based on the data from which the original recurrence matrix was produced. An example of a *novelty curve* can be found in [IMAGE 3]({m.re}#im3), extracted from Foote (2000, p. 454). 

<ImageMedia2 
url='/re/owen.jpeg'
figure='IMAGE 2'
caption='Owen Green answering my question in the FluCoMa discourse'
id='img2'
/>

<ImageMedia2
url='/re/noveltycurve.jpg'
figure='IMAGE 3'
caption='Example of a novelty curve, taken from Foote (2000, p. 454)'
/>

Using a *threshold* of 0.61, *kernelsize* of 3, *filtersize* of 1, *fftsettings* of 1024 512 1024 and a *minslicelength* of 2, I produced the segmentation which can be seen in [AUDIO 5]({m.re}#aud5). To me, this segmentation produced a segmentation that was similar to my intuitive one(seen in [AUDIO 4]({m.re}#aud4)). While at some points the results of `fluid.noveltyslice~` were over-segmented (slices one to five) it seemed sensitive to the changes I heard and was able to extract the longer musical phrases such as slice seven and twelve without having to tune the parameters intensively.

<Waveform 
file='/re/generation/libLLVMAMDGPUDesc.a.mp3'
peaks = '/re/generation/libLLVMAMDGPUDesc.a.dat'
id='aud5'
title='AUDIO 5: libLLVMAMDGPUDesc.a segmented with the fluid.noveltyslice~ algorithm'
segments={noveltySlices}
/>

I then segmented every corpus item with these settings accepting that some files might be segmented strangely or not close to how I would do it manually, as I did not want to spend an indefinite amount of time working on segmentation. At this point I had accrued 21551 corpus items.

### Analysis

https://github.com/jamesb93/data_bending/blob/master/python_scripts/descriptor_analysis/6_mfccs.py

The next stage of the compositional process was to analyse the corpus of segmented samples. I planned to use audio descriptors that I was comfortable understanding and had used before such as spectral centroid, loudness, pitch and spectral flatness. However, I wanted to focus on the spectrum of the sounds, and attempt to analyse the changes in the perception of texture. This led me to use MFCCs for analysis as they are robust against differences in loudness and are capable of differentiating the textural characteristics of sounds in a more detailed fashion compared to three previously mentioned descriptors. For each corpus item, an MFCC analysis was conducted using `fluid.mfcc~` with an *fftsize* of 2048 1024 2048, 40 *melbands* and 13 *coefficients*. I then calculated seven statistics- mean, standard deviation, skewness, kurtosis, minimum, median and the maximum and up to the third derivative for each coefficient . 

This stage was imperative in determining what features of the sounds the computer would be listening to as well as forÂ guiding "downstream" processes such as [Dimension Reduction]({m.re}#dimension-reduction) and [Clustering]({m.re}#clustering). I anticipated that calculating two derivates of the statistics would capture the morphology of the sound by describing both the change of these statistics between frames as well as the change in the change. These statistical summaries were flattened to one-dimension and each column of these vectors is standardised in order to scale the data for each corpus item within a commensurate range.

### Dimension Reduction

https://github.com/jamesb93/data_bending/tree/master/python_scripts/dimensionality_reduction

At this point I had a wealth of corpus items and MFCC analysis data. While MFCC values are strictly defined, they lack the approachability of other descriptors such as spectral centroid.  Reading MFCC values is far less intuitive in being able to discern any human perceptual value for example than interpreting how a spectral centroid calculation might map onto our perception of the brightness of a sound. In other similar corpus navigation and exploration work, dimensionality reduction techniques have been implemented in order for the computer to compress dense data such as this and to facilitate exploring the latent structure of a corpus. Examples include the work of Stefano Fascinai (Fasciani, 2015), *Flow Synthesizer* (Esling, Masuda, Bardet, Despres, & Chemla-Romeu-Santos, 2019), [AudioStellar](https://audiostellar.xyz/) (Garber et. al. 2020), Thomas Grill (Grill & Flexer, 2012), LjudMap (Vegeborn, 2020) and the previously mentioned *FluidCorpusMap*. My aim for using dimensionality reduction at this point was to start moving away from mostly analytical work, and to begin enabling the computer so that it could facilitate exploring the corpus. One of my primary goals in this regard, was to find a way so that the computer could draw together perceptually similar sounds.

To do this, I used the Uniform Manifold Approximation and Projection (UMAP) algorithm and reduced the 273 points in each vector for each sample to 2 points. This way, I could visualise the results as a two-dimensional *map* where each sample would be represented as a singular point. A strength of UMAP is its potency for capturing non-linear features in data compared to algorithms such as Principal Component Analysis (PCA). Furthermore, UMAP can be coerced to favour global or local structure by altering the *minimum distance* and *number of neighbours parameters*. A number of projections which were made with various parameters can be seen in [DEMO 1]({m.re}#demo1).

I used such visual projections to intuitively interpret the results of the dimension reduction process.  For me, the topology of the projection became a source of inspiration compositionally by observing the location, shape and relationships between clusters of samples. In particular, the UMAP process would often produce characteristic outputs, where clusters could be located away from the main body of the projection or the overall shape would be comprised of a repeating shape or feature. These were novel and interesting to me and were investigated further through manual audition. Most importantly I found that the UMAP algorithm produced a layout of samples in space which samples were perceptually meaningful relative to each other in the space. Navigating around a single area of the space rendered samples that were similar to each other. Moving across the space more broadly positioned different morphological sounds away from each other as well as creating small pockets where samples were unique and novel. These successes aside, I did not have a way to really incorporate these findings into a compositional workflow where pieces were being made. It was mostly motivating and inspirational in the sense that I found the technology exciting and capable of dealing with large listening tasks competently, rather than bringing me closer to the creation of a piece.

### Clustering

Part of my positive response to the dimension reduction process was the robustness in which samples with similar perceptual qualities were drawn together in the two dimensional projection. I wanted to take this further, and have the computer perform clustering on this data  in order to return groups of samples which I imagined would be useful as a starting point for composition.

https://github.com/jamesb93/data_bending/tree/master/python_scripts/clustering

To do this I experimented with several algorithms including [k-means](https://en.wikipedia.org/wiki/K-means_clustering), [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) and [Agglomerative Clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html). Each of these algorithms offers a different interface and will cluster input data based on a different set of assumptions. K-means for example operates by partioning a data space into a fixed number of partitions set by the user, and then drawing individual points into these based on their barycentres. This application assumes that the data is spherically arranged in clusters which can be a problematic assumption depending on the data. HDBSCAN, on the other hand 

<!-- WIDGET: clustering comparison -->

Agglomerative clustering






## Responding To The Output Of The Computer
Exploring them with a max patch + Lua to pull them into REAPER
### dotmaxwave
- similar morphologies brought together from the .maxwave input data
### X86Desc.a
### segmenoitte
### 340685107feisraebbaatsaed--isn.sqlite
### sys.ji 

Noise is good but only in the context of not-noise.
 
## Reflection
### Intervention and Iteration

Intervention as paramount
- Compositional fluidity
- Being thrown ideas works! (remember how you explained it to Oli Bown...)
- Honing initially 'blunt' questions
- Forming clear compositional intent
- Responding to constraints _with_ the computer 

### A New Workflow
#### Issues of friction leading to FTIS

1. Discovering structure in a corpus
2. Finding groupings of perceptually similar materials
3. Discovering unknown and unique items
4. Capitalisation on known points of interest
5. Forming/finding connections between corpus items 

#### Re-embracing the DAW
- Stitch/Strata, i explain some of the benefits that re-embracing the DAW did for that project, especially for managing intermediate states of projects and "scaffolding" or "sketching"
Linking the DAW to extra information (i.e extending the role of the DAW through FTIS by letting me work with information that is not well represented in a timeline view)
navigating-sample-based-music 


## Other Related Links and Media

I gave two presentations that discussed this project at different points in its development. The first presentation I gave was at the November 2019 FluCoMa plenary, where the pieces were partially composed and I focused on demonstrating my application of the FluCoMa tools. Much longer after the pieces had finished and I moved on to composing [{m.emname}]({m.em}), I reflected on the entire development process and its relationship to the compositions in a [short article](https://zenodo.org/record/4285398) for the 2020 AI Music Creativity Conference. This involved creating a short video demo too.

<YouTube 
url="https://www.youtube.com/embed/IpD_XzW1Az4" 
figure='VIDEO'
caption='"Finding Things In Stuff" presentation at the November 2019 FluCoMa Plenary'
/>

<YouTube 
url="https://www.youtube.com/embed/-FNO0QovfsI"
caption='"Corpus exploration with UMAP and agglomerative clustering" presentation at the AI Music Creativity Conference 2020.'
figure='VIDEO'
/>

<NextSection 
next="ElectroMagnetic"
link={m.em}
/>
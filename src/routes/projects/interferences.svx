<script>
	import NextSection from "$lib/components/NextSection.svelte"
    import {metadata as m} from "../directory.svx"
    import VideoMedia from '$lib/components/VideoMedia.svelte';
    import VideoMedia2 from '$lib/components/VideoMedia2.svelte';
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
    import YouTube from '$lib/components/YouTube.svelte';
    import Album from '$lib/components/Album.svelte';
    import Code2 from '$lib/components/Code2.svelte';
    import Waveform from '$lib/components/Waveform.svelte'; 
    import KDTree from '$lib/demos/kdtree/KDTree.svelte';

    import { 
        interAlbum, foundations, rhythmicConstructs,
        eeeSegments, 
        anchors, anchorsSegments,
        kindleModes,xiaomiModes,xboxModes,
        classifier,loopExperiments,baseSketchSegs
    } from '$lib/data/interferences.js';
</script>

# Interferences

<Album
tracks={interAlbum}
title='Interferences'
id='audio1'
figure='AUDIO 1'
/>

Interferences is the fifth and final project in this portfolio. It is pair of works presented as an EP. There is a git repository which contains all of the Python code and Max patches which were used to create the work. This can be found at: https://github.com/jamesb93/interferences

## Motivations and Influences
A primary motivation for this project was to develop the workflow established in [Reconstruction Error]({m.re}), and to build on my creative-coding practice with Python, Max, REAPER and [FTIS]({m.ftis}). There were two aspects to this motivation. Firstly, I wanted to recreate the experience of navigating through a corpus of unknown materials led by the computer through *structured listening*. Although I only utilised a small portion of the corpus in [Reconstruction Error]({m.re}), I wanted to generate a new corpus with its own origins, whilst still situating my work within a digital and synthetic aesthetic. At the time, I was interested in [circuit bending](https://en.wikipedia.org/wiki/Circuit_bending) and the vast number of sounds that breaking apart and reconstructing old electronics could generate. The aesthetic of these sounds suits my sensibilities well, and I imagined that collecting a number of them could be a path forward for creating the corpus that I needed. Tangential to this research, I came across a video of Nicolas Collins using an induction microphone to amplify the sounds of electromagnetic interferences generated by everyday appliances and objects. This video can be seen in [VIDEO 1]({m.em}#vid1). It struck me that instead of circuit bending, this might be a suitable technique for creating a large corpus with the properties I desired. Following this hunch, I obtained an [Elektrosluch Mini induction microphone](https://store.lom.audio/products/elektrosluch-mini-city-diy-kit-1?variant=4542176460832) capable of recording electromagnetic interference. I describe how this device was instrumental in the compositional process further in [[4.5.2.1 Recording Objects With Induction]]({m.em}#recording-objects-with-induction).

<YouTube 
title='Nicolas Collins demonstrating induction to record electromagnetic fields generated by everyday electrical appliances.'
url="https://www.youtube.com/embed/4T7qkYY7LZM"
figure='VIDEO 1'
id='vid1'
/>

Secondly, I wanted to iterate on the pipeline of segmentation, analysis, dimension-reduction and clustering and to apply it more fluidly to new compositional materials. The technology implemented in [Reconstruction Error]({m.re}) was successful for enabling me to be creative, but was largely inflexible and so to perform the same tasks on new material would require rewriting the scripts. Thus, [FTIS]({m.ftis}) supported this goal, and made it possible to create scripts that supported experimentation and data manipulation. This aspect is discussed in more detail in [[4.5.2.3 Pathways Through The Corpus]]({m.em}#pathways-through-the-corpus).

## Composition Process
The composition process for [Interferences]({m.em}) can roughly be divided into two phases. The first phase involved experimentation with [FTIS]({m.ftis}) and devising various ways to navigate through the corpus with the aid of the computer. The second phase involved a more direct level of composition in which I used the computer to search and hone in on specific materials in the corpus for this project. This contrasts with the broader exploration undertaken in the first phase.

### Recording Objects With Induction
As outlined in [[4.5.1 Motivations And Influences]]({m.em}#motivations-and-influences), I procured an electromagnetic induction microphone and wanted to use it to generate a corpus of digital and synthetic-type sounds. To do this, I took electronic objects from around the home and recorded them, at times operating those objects simultaneously. I was surprised by the diversity of sounds this produced. Despite devices in my environment seeming dormant, the induction microphone could detect and uncover their invisible electronic signatures and operation. I recorded a number of devices, including my computer keyboard, mouse, a mobile phone, a sound card, a laptop and an e-reader. Each device produced its own characteristic outputs and some of these objects offered me the ability to trigger certain sonic behaviours. For example, [AUDIO 2]({m.em}#aud2) captures the sound of an e-reader left relatively untouched. The change at 0:02 is triggered by switching the "aeroplane mode" on and off. This user interaction causes the components in the e-reader to operate differently, which is captured by the induction microphone. Eventually the initial static state is returned to. This can be heard at 1:59, lasting until the end of the clip.

<Waveform 
file='/inter/kindle-gesture.mp3'
peaks='/inter/kindle-gesture.dat'
id='aud2'
title='Induction recording of e-reader while switching the aeroplane mode on and off'
caption='AUDIO 2'
segments={kindleModes}
/>

Other objects produced their own characteristic sonic materials although I found that across all of the objects I listened to my interpretation would always coalesce around an antiphonal distinction between *active* and *static* musical states. For example, [AUDIO 3]({m.em}#aud3) is extracted from the electromagnetic interferences of a mobile phone. In my perception, there are two sonic classes at work here, where 0:00 to 0:40 is *static*, and where the remainder of the recording is *active*.

<Waveform
file='/inter/xiaomi-gesture.mp3'
peaks='/inter/xiaomi-gesture.dat'
id='aud3'
title='Induction recording of mobile phone without user interaction'
caption='AUDIO 3'
segments={xiaomiModes}
/>

As I recorded and browsed through these sounds, I was reminded of the work of Australian field-recording artist Jay-Dea Lopez and the way he taps into similar sources using contact microphones attached to monolithic infrastructural objects in the outback. In particular, three blog posts of his, ["Low Frequencies"](https://soundslikenoise.org/2019/09/07/low-frequencies/), ["Coil pickups and microsounds"](https://soundslikenoise.org/2020/08/16/coil-pickups-and-microsounds) and ["Raising the Inaudible to the Surface"](https://soundslikenoise.org/2014/08/31/listening-to-the-inaudible-field-recording-and-the-pursuit-of-the-microsound/), contain these types of material. Similarly, I felt a certain connection between my experiments and the morphologies and textures present in Bernhard Gunter's *Untitled I/92* (1993) ([VIDEO 2]({m.em}#vid2)), and Alvin Lucier's *Sferics* (1981) ([VIDEO 3]({m.em}#vid3)). To me, both of these pieces shared some of the characteristics of what I was discovering with the electromagnetic microphone – delicate and fragile sound worlds full of intimate clicks, pops and spectrally constrained sonic fragments. Furthermore, these external influences started to funnel into my conceptualisation of the initial material as that which can occupy two distinct states – busy and rapidly changing or unchanging and glacial in nature. This became an essential starting point in the technological process of using machine listening to navigate through this corpus of sounds framed by the opposition of *static* and *active* states.

<YouTube 
title="Untitled I/92 (1993) from Un Peu De Neige Salie – Bernhard Gunter"
url="https://www.youtube.com/embed/QZfMp6tredw"
figure='VIDEO 2'
id='vid2'
/>

<YouTube 
title='Sferics (1981) – Alvin Lucier'
url="https://www.youtube.com/embed/rxUvMl_IxoQ"
figure="VIDEO 3"
id="vid3"
/>

### Static And Active Material
I began by trying to dissect the corpus into two groups based on classifying whether they were static or active. To do this, I first had to segment the corpus, and then find a way of crudely classifying those segments into either of these two categories.

#### Segmentation
The end goal for segmentation in this project was to divide each sound into static and active components automatically. I began by splitting each corpus sound into rough homogenous regions of spectral activity. I planned to apply a classification or clustering analysis to the segments made by this process so that the computer could discern which group each segment might belong to. This was complicated because the time span of static and active states was highly variable, to my perception, depending on the file. [AUDIO 2]({m.em}#aud2) shows only a portion of what was created from recording an e-reader, and the static and active states last for several minutes between alternations. [AUDIO 4]({m.em}#aud4) shows another recording taken from a wireless game controller, which demonstrates similar oscillations between static and active states, although these changes occur over seconds rather than minutes. Given these disparities in time scale, a frame-by-frame analysis was likely to be unsuitable and hard to tune appropriately to capture the sparse and specific moments in time in longer samples, while also being sensitive to the shorter-term changes.

<Waveform 
file='/inter/xbox-gesture.mp3'
peaks='/inter/xbox-gesture.dat'
caption='AUDIO 4'
title='Shorter induction recording taken from a game controller.'
id='aud4'
segments={xboxModes}
/>

To tackle this, I created a bespoke algorithm influenced by Lefèvre & Vincent (2011), which performs segmentation by sampling short sequences within a sound and clustering those sequences together. My algorithm, *Clustered Segmentation*, is structured as a two-phase process. The first phase encompasses a "preprecessing" segmentation pass, in which a sample is divided into numerous small divisions. These divisions are not necessarily perceptually meaningful; rather, the process aims to divide the signal finely into minute segments that can be recombined later in the second phase. Any segmentation algorithm can be inserted into the algorithm at this point in order to perform this preprocessing. The second phase applies audio-descriptor analysis to each segment and then groups each one into a user-specified number of clusters. This *number of clusters* parameter determines how discerning the algorithm is in distinguishing the segments from each other. After clustering, the algorithm recursively takes a window of clustered segments and merges contiguous segments that have been assigned the same cluster label. This process starts at the first segment and shifts forward in time, merging clusters recursively as it moves along. Thus, it gradually creates ever larger combined segments until each consecutive segment belongs to a different cluster. The size of the window of clusters (*window size*) in this process is also configurable. [IMAGE 1]({m.em}#img1) is a visual representation of this algorithm. 

<ImageMedia2 
url='/inter/clusterseg-explanation.svg'
caption='Visual depiction of "cluster segmentation" algorithm'
figure='IMAGE 1'
id='img1'
/>

My intention here was to create a generalisable algorithm that would be relatively agnostic toward the nature of the material it was segmenting. I also wanted to avoid some of the issues whereby frame-by-frame approaches often only target short windows of time and do not respect the longer-term implications of changes in the signal. Using a shifting window that merges while it slides across the segments, it gives the algorithm a "lookahead" and "look behind" capacity that constantly updates as it progresses.

I evaluated the effectiveness of *Clustered Segmentation* by modifying each parameter of its two-phase segmentation process and observing the results as a REAPER session. Overall, my observation was that the choice of specific segmentation algorithm for the preprocessing stage had little effect on the final result. I experimented with several flavours of segmentation from the Fluid Decomposition Toolbox (which were at this point embedded directly in Python), including `fluid.onsetslice` (a spectral segmentation algorithm), `fluid.noveltyslice` (another spectral segmentation algorithm) and `fluid.ampgate` (an amplitude-based segmentation algorithm). I discovered that for any of these algorithms, the most desirable outcome was one in which the source sample was segmented very finely. If the segments were too long, *Clustered Segmentation* would often deem contiguous segments to be similar because the data in each segment regressed to the mean. Thus, it was important that the preprocessing stage produced segments which are very short. Furthermore, the *window size* and *number of clusters* parameters had a significant effect on the results. In the development and testing phase of *Clustered Segmentation*, I found that different combinations of these two parameters were difficult to predict in terms of how they affected the result. [IMAGE 2]({m.em}#img1) shows the REAPER session I used to inspect the results, where each track is the result of a different parameter combination. The session itself can also be found in the repository here: [/Projects/Interferences/Code/segmentation_scripts/2020-06-18 04-09-54.718681/session.rpp](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation_scripts/2020-06-18%2004-09-54.718681/session.rpp) and the code that performed the segmentation can be found here: [/Projects/Interferences/Code/segmentation_scripts/clustered_segmentation.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation_scripts/clustered_segmentation.py).

<ImageMedia2 
url='/inter/clusterd-segmentation.jpg'.
figure='IMAGE 2'
caption='Clustered segmentation results rendered as a REAPER session.'
id='img2'
/>

I was hoping that the algorithm would be able to automatically detect the salient textural changes in the source materials between active and static states. Testing different parameter combinations on sounds from the corpus produced a variety of results. Some of the results were clearly unsatisfactory and there was little amalgamation of segments from the preprocessing stage. Other results showed more promising results, and in many cases were harder to evaluate between different parameter settings. Segmentation points that I would inuitively changes in texture and morphology had been detected. However, rarely did one configuration agree on the same set of results. Given this disparity it was difficult to determine a "best" configuration. Moving forward, I turned this algorithm into a [FTIS]({m.ftis}) analyser, as previously I had been testing and developing it as a "one off" script. I then ran the algorithm on all of the corpus items using this script: [/Projects/Interferences/Code/segmentation.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation.py). The results from this script were satisfactory but not perfect. In some cases, areas of homogeneity in the signal had been divided. However, distinct areas showing static and active traits had been detected and separated.

#### Classification Through Clustering
I then classified each of these newly generated segments into two distinct groups by calculating the statistics of spectral flux across windowed frames in each corpus sound, and then clustering that data with the *Agglomerative Clustering* algorithm. The code for this can be found here: [/Projects/Interferences/Code/split_by_activity.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/split_by_activity.py). The results were acceptable for dividing sounds into static and active groups. A selection of different results from this process are presented in [AUDIO 5]({m.em}#aud5).

<Album 
tracks={classifier}
id='aud5'
title='Several classification results.'
figure='AUDIO 5' 
/>

### Pathways Through The Corpus
This section describes different *pathways* through the corpus of segmented and classified sounds I created, and experiments which aided in formulating concepts and ideas for the two pieces through audition and structured listening. I wanted to begin by deconstructing further those sounds classified into the active category, into individual impulses, utterances and segments. For example, I imagined that [AUDIO 6]({m.em}#aud6), a segment from *Kindle_04_08.wav*, might be restructured to be less chaotic and more ordered; perhaps a rhythmic structure could be imposed upon individual segments extracted from this sound.

<Waveform 
file='/inter/kindle-04-8-seg.mp3'
peaks='/inter/kindle-04-8-seg.dat'
id='aud6'
caption='AUDIO 6'
title='Kindle_04_08.wav segment'
/>

Following this intuitive desire, I used [FTIS]({m.ftis}) to segment the active sounds and then clustered the resulting segments based on their perceptual similarity. This was orchestrated with two separate [FTIS]({m.ftis}) scripts. Firstly, a segmentation script, [/Projects/Interferences/Code/micro_segmentation.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/micro_segmentation.py), that uses the *Clustered Segmentation* algorithm to segment the active sounds. Secondly, this is followed by a clustering script, [/Projects/Interferences/Code/micro_clustering.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/micro_clustering.py), which calculates MFCCs for each segment and agglomerates them using the HDBSCAN algorithm. The clustering results can be found here: [/Projects/Interferences/Code/outputs/micro_clustering/5_HDBSCluster.json](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/outputs/micro_clustering/5_HDBSCluster.json) and I will refer to them as "micro-clustering" results henceforth. I explored the results of this process in Max aurally and found that this process rendered perceptually homogenous clusters in terms of texture and morphology. [VIDEO 4]({m.em}#vid4) captures this audition process and demonstrates the clusters' perceptual homogeneity. This patch can be found here: [/Projects/Interferences/Code/classification_explorer](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/classification_explorer.maxpat).

<VideoMedia2 
url='/inter/exploring-clusters'
caption='Exploring segmented and clustered active material'
figure='VIDEO 4'
id='vid4'
/>

This Max patch looped a selected sample from a cluster in the micro-clustering results. As I navigated between clusters and explored samples within them, the intricacy and fragility of the sounds within specific clusters was foregrounded. Curiously, some material that I would have perceived as static had been included in what was the active classification performed earlier. The nature of the looping playback, combined with the already mechanised and artificial quality of these sounds was evocative. I was compelled to the qualities of different groups, imagining composing with the segments to form my own artificial machine-like sound composites. Clusters 1, 2, and 37, for example, produced structured rhythmic sounds articulated by sparse and subtle transients. While I was changing the sample, the rhythmic patterns of these articulations could be altered. I imagined that combining several of them could be used to build layers of intricate quasi-rhythmic patterns. Clusters 0, 11 and 18 contained a large amount of drone-like static material with a prominent pitch content. To an extent, I was thankful this material had been poorly grouped into the active classification causing it to be serendipitously discovered by me in this process. Looping short segments of this material highlighted how it could be transformed from its originally glacial and static form into a more dynamic and active behaviour.

For me this was a revelatory listening experience in the compositional process. I imagined those sounds with more rhythmic and iterative qualities could form a piece in this EP, while those with drone-like qualities might form another. This elicited my further utilisation of [FTIS]({m.ftis}) to hone my exploration and deconstruction in terms of these two compositional aims. This next phase of exploration and computer-aided composition did not, however, directly lead to the creation of the two works in the EP; rather, I would characterise it as a series of "ad-hoc experiments" that functioned as stepping stones in formulating my compositional intent.

#### Ad-Hoc Experiments

One such experiment was to run similar clustering processes on a subset of the corpus, derived by filtering each item according to its average loudness. The aim of this was to hone in on quiet, drone-like sounds and to nudge the corpus towards only those sounds and away from the active, rhythmic sounds. This was facilitated by extending [FTIS]({m.ftis}) and the implementation of a *Corpus* object, a programming construct which contains both the data (a collection of samples and audio-descriptor analysis), and a series of methods to operate on that data. Up to this point, *Corpus* objects only held the names of files in their data structure as a way of establishing where a corpus resided on a hard disk. I then added a number of filtering methods which allowed me to sieve the list of files contained in the corpus. These methods were composable, and could be combined in order to carve away samples from the overall corpus in order to return a new corpus. Thus the notion of a corpus became more malleable, and something that itself contained severl sub-corpora. [CODE 1]({m.em}#code1), for example, demonstrates filtering a corpus by the loudness of each item so that only the quietest 10% of samples would remain. Any number of those methods can be chained to filter progressively what is returned by each filter method.

<Code2
caption='Corpus filtering by loudness example using the loudness() method of a Corpus() object.'
figure='CODE 1'
id='code1'
>

```py
from ftis.corpus import Corpus
corpus = Corpus('path/to/corpus')
corpus.loudness(max_loudness=10)
```

</Code2>

The extension to [FTIS]({m.ftis}) via the *Corpus* object was effective for filtering toward the quietest and most delicate sounds as a preprocessing stage to clustering. Although this process did not specifically isolate those that were more drone-like, as I had intended, I embraced this, and combined the materials into a short sketch which can be heard in [AUDIO 7]({m.em}#aud7). Reflecting on this sketch, there are noticeable sonic connections to one of the finalised pieces in this project, [{m.anchors}]({m.em}#manchors), in terms of the type of material and the focus on static sounds.

<Waveform 
file='/inter/short-quiet-idea.mp3'
peaks='/inter/short-quiet-idea.dat'
title='Short draft of a piece composed with sounds derived by corpus filtering'
caption='AUDIO 7'
id='aud7'
/>

Another ad-hoc experiment was based on exploring the micro-clustering results by exporting them to REAPER as materials to be composed with intuitively. Using samples from clusters such as 1, 2, and 37, I constructed a series of different loop-based palettes manually in the DAW timeline. I aimed to create several iterated structures that were cyclical in nature, but that could also have been perceived as relatively static at the same time. 

Several of these experiments can be heard in [AUDIO 8]({m.em}#aud8). They were less evocative for me, and I felt as if the underlying concept of what I was trying to do was not being explored in as much detail as it demanded. That said, it did inform some of the ways that material was constructed in [{m.eee}]({m.em}#{m.eee}), in terms of how sounds were deconstructed and restructured through intuitive compositional decision-making.

<Album 
tracks={loopExperiments}
id='aud8'
title='Various loop experiments using clusters such as 1, 2, and 37'
figure='AUDIO 8'
/>

#### Mixing Sounds From Reconstruction Error
These experiments were useful in acquainting me with the broader nature of the corpus and were meaningful as a way of stepping back into a compositional mindset after much preliminary technical work had taken place. While I was performing these experiments, I had a constant impression that the amount of variation in the corpus was not sufficient enough for me to carry forward initial compositional sketches into something more fully-fledged as a piece. Specifically, the more I worked with short segments, the more they seemed to lose their significance when pulled apart from the original temporal structures. This was unlike the situation in [Reconstruction Error]({m.re}), in which I found a lot of value in dissecting sounds into atomic units and composing with them in that deconstructed state. 

In response to this, I reasoned that *combining* the [Reconstruction Error]({m.re}) databending corpus with this project's corpus might expand the possibilities for working with small segments of sounds while maintaining a sense of coherence in the sonic identity of the project. This was not straightforward to realise technically, because [FTIS]({m.ftis}) was built in such a way that a corpus represented files contained inside a single directory. I did not want to "pollute" each corpus by moving them around the file system or creating copies which would be operated on differently to their master versions. My ideal interface would instead point [FTIS]({m.ftis}) to the location of several different corpora and be able to combine them programmatically at the script level.

This catalysed a development to the [FTIS]({m.ftis}) *Corpus* object to serve these emerging compositional aims. Until this point, a *Corpus* represented a single directory of samples or a single sample on the computer. Using [operator overloading](https://docs.python.org/3/reference/datamodel.html#special-method-names), I made it possible to combine corpora programmatically. This functionality made it possible for me to combine multiple corpora and treat them as a single corpus, or as I termed it internally, to create a *multi-corpus*. [CODE 2]({m.em}#code2) shows an example of how this is orchestrated as [FTIS]({m.ftis}) code.

<Code2
id='code2'
caption='Adding two FTIS Corpus objects together using operator overloading. the variable multi_corpus is a new corpus as a result of adding corpus_one to corpus_two.'
figure='CODE 2'
>

```py
from ftis.corpus import Corpus
corpus_one = Corpus('path/to/corpus')
corpus_two = Corpus('path/to/corpus')
multi_corpus = corpus_one + corpus_two
```

</Code2>

While the notion of combining multiple sources of sonic materials is not novel to composition with digital samples, for me this change allowed me to use [FTIS]({m.ftis}) more flexibly than before, in order to overcome the mental pattern of one project pertaining to a single corpus. Instead of having to merge files from a number of different directories on disk to form a corpus, I could instead point a script to a number of directory locations and deal with them in a fashion where the organisational house-keeping is separated from the creative workflow. This provoked another phase of exploration with FTIS, one that ultimately generated the two works presented as [Interferences]({m.em}). The next two sections, [4.5.2.4 {m.anchors}]({m.em}#manchors) and [4.5.2.5 G090G10564620B7Q]({m.em}#{m.eee}) discuss how these works emerged from this point onward.

### anchors
<Waveform 
file='/pieces/Interferences/anchors.mp3'
peaks='/pieces/Interferences/anchors.dat'
id='aud9'
title={m.anchors}
caption='AUDIO 9'
/>

Developing the *multi-corpus* capabilities of [FTIS]({m.ftis}) prompted me to think how I might re-approach composing with the drone-like, static materials I encountered in the earliest cluster-exploration process described in [[4.5.2.3 Pathways Through The Corpus]]({m.em}#pathways-through-the-corpus).

The first step I took in addressing this was to investigate in more detail what level of variation existed *between* these static sounds. To facilitate this, I created a [FTIS]({m.ftis}) script to take all of the samples initially classed as static (see [[4.5.2.2 Static And Active Material]]({m.em}#static-and-active-material)) and to create three clusters from this material. The number of clusters was an intuitive choice, and I imagined that by dividing static material into three groups I might be able to discern if there were any significant differences within this sub-section of the corpus. From this, I generated a REAPER session and auditioned the outputs. This script can be found here: [/Projects/Interferences/Code/MultiCorpus/scripts/base_materials.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/base_materials.py).

Combing through this REAPER session confirmed my intuition that there was not much variation in the static material. While three clusters were produced by the clustering process, the sounds grouped into clusters 1 and 2 possessed almost identical morphologies and textural qualities. The differences between clusters 1 and 2, on the one hand, and cluster 0, on the other, were more perceptually obvious. Highlighting these differences, I created a sketch based on hard panning clusters 1 and 2 to left and right channels, respectively, and situating cluster 0 centrally. I arranged the material in order to explore different juxtapositions of these two sonic identities. This sketch can be heard in [AUDIO 10]({m.em}#aud10) alongside some annotations related to my perception of different combinations between of the clusters.

<Waveform 
id='aud9'
title='A sketch created from the output of base_materials.py'
caption='AUDIO 9'
file='/inter/base-materials-sketch.mp3'
peaks='/inter/base-materials-sketch.dat'
segments={baseSketchSegs}
/>

Following the composition of this sketch, I aimed to discover other sounds which could be used for creating detailed superimpositions with those sounds found in [AUDIO 10]({m.em}#aud10), as well as to expand the palette of possible combinations between sounds. By utilising the *multi-corpus* techniques in [FTIS]({m.ftis}) further, I produced a two-dimensional map containing all the sounds from the [Reconstruction Error]({m.re}) corpus *and* the static materials I was already working with. The map was created in a similar fashion to how I represented the [Reconstruction Error]({m.rt}) databending corpus, described in [[4.4.2.4 Dimension Reduction]]({m.re}#dimension-reduction), in which audio descriptor analysis is filtered and transformed, such that a set of fewer numbers than the original analysis portrays the perceptual differences between samples. However, instead of using MFCCs, I analysed each sound using a Constant-Q Transform (CQT) descriptor. I selected this particular audio descriptor based on the assumption that its logarithmic representation of the spectrum would be suitable for capturing differences and similarities in pitch as well as discerning fine-grained spectral complexity in the high frequencies. From this spatial representation I computed a [k-d tree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) which enabled me to query for a number of neighbours to a specific point in that space. Using this data structure, I selected a sample ([AUDIO 11]({m.em}#aud11)) from the sketch heard in [AUDIO 10]({m.em}#aud10) with a prominent pitch component and then queried for the 200 samples which were closest to that sample in the spatial representation. [DEMO 1]({m.em}#demo1) is an abstracted example showing how this process works, albeit based on a smaller set of synthetic data. Furthermore, the code which orchestrated this dimension-reduction and k-dtree process can be found here: [/Projects/Interferences/Code/MultiCorpus/scripts/find_tuned.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/find_tuned.py).

<KDTree />

<Waveform 
id='aud11'
title='Central query sample for find_tuned.py'
caption='AUDIO 11'
file='/inter/find-tuned-centre.mp3'
peaks='/inter/find-tuned-centre.dat'
/>

I auditioned the 200 sounds returned by the [find_tuned.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/find_tuned.py) script, and found that there were many cohesive compositional materials. Many of these samples had a strong pitched component, much like the central query samples, while others were texturally similar. A short video capturing this listening process can be seen in [VIDEO 5]({m.em}#vid5). I imported all of these files into REAPER and created a longer-form sketch with them, which can be heard in [AUDIO 12]({m.em}#aud12). 

<VideoMedia2 
url='/inter/exploring-tuned-anchors'
caption='Auditioning the output of find_tuned.py'
figure='VIDEO 5'
id='vid5'
/>

<Waveform 
file='/inter/28-10-20.mp3'
peaks='/inter/28-10-20.dat'
title='Sketch using a combination of sounds from initial sketch (AUDIO 9) and from find_tuned.py output.'
caption='AUDIO 12'
id='aud12'
/>

From composing these two sketches, the beginning of a clearer idea for the final work emerged. Throughout the computer-aided searching and the broader compositional process three particular samples persistently came to my attention due to their characteristic textural qualities. I termed these three sounds "anchors", and they can be heard in [AUDIO 13]({m.em}#aud12).

<Album 
tracks={anchors}
title='Three "anchor" sounds'
caption='AUDIO 13'
id='aud13'
/>

Following this, I created a replicated the previous process developed for the [find_tuned.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/find_tuned.py) script. I generated a new two-dimensional map of a *multi-corpus* containing [Reconstruction Error]({m.re}) sounds and the static sounds with which I had been sketching. This script can be found here: [/Projects/Interferences/MultiCorpus/scripts/three_anchors.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/three_anchors.py). I then constructed a k-d tree from this representation, and *branched* out from each of the three anchors and stored the 25 sounds which were closest to them within the two-dimensional map. This organised the *multi-corpus* of static sounds and samples sounds from [Reconstruction Error]({m.re}) within the frame of those three anchors. Using this data, I created a REAPER session with three tracks. Each track contained a single anchor, followed by the 25 samples which were 

My compositional decision making was largely a response to the division of material into these distinct groups. I focused on blending the groups in different ways to create composite textures with distinct musical behaviours. Such combinations were based on forming chordal sounds from the summation of pitched material or layering dynamically striated and texturally "bumpy" sounds with those that were more static and inactive. This concept informed the high-level structure of the work, which is underpinned by a drone created from material belonging to a single one of the three groups. Longer form developments are orchestrated as interactions with this layer either emphasising and adding to it or assuming the role of a sonic antagonist. 

There is only one intentionally constructed dramatic moment that occurs at 6:20, while the rest of this piece is purposefully crafted to be less directed and non-narrative in nature. This emerged from a contrasting mindset to that which I embodied in [Reconstruction Error]({m.re}). Comparatively, I had much less material to work with in [Reconstruction Error]({m.re}) and so often I had to extract as much musical value from the samples I had found within perceptually homogenous clusters. This pushed me toward recycling the material as much as possible and composing several meso-scale structures which I then blended and arranged intuitively. By the time I had utilised those meso-structures as much as I felt I could, the form of a piece had crystallised and was less flexible or adaptable.

By having access to much more material in [{m.anchors}][{m.em}#manchors], and several different computational structures representing those materials, the conceptual goals and frame of the work was fostered by many more flexible forms of interaction with the computer. In reality, the compositional process following the composition of the two sketches was characterised by a feeling that I was "firming up" the conceptual ideas of the piece and developing them iteratively from those drafts. It also engendered tight integration between my conceptual goal and realising that with the aid of the computer. I felt as if I did not have to compromise on the conceptual aims of the piece; rather this was cultivated collaboratively and was mediated by myself and the computer.

### G090G10564620B7Q
<Waveform 
file='/pieces/Interferences/G090G10564620B7Q.mp3'
peaks='/pieces/Interferences/G090G10564620B7Q.dat'
id='aud14'
title={m.eee}
caption='AUDIO 14'
segments={eeeSegments}
/>

This second piece of [Interferences]({m.em}) grew in response to the development of the [FTIS]({m.ftis}) *Corpus* object while creating {m.anchors}, and my desire to revisit some of the raw materials recorded with the Elektrosluch mini. While I was gathering these initial recordings, I had a strong aesthetic response to the recording of the e-reader. I wanted to return to this sound, and deconstruct it with the tools which at the start of this project were not yet developed or as sophisticated as they had become. The most striking aspect of this e-reader recording, in my perception, was the clarity and definition between active and static states as well as how the entire sample was structured around these two distinctive musical behaviours. The static component possessed a strong pitch element and was unwavering in dynamic, while the active was more gestural and exhibited a varied morphology. A small segment of the longer recording demonstrates the antiphonal character of these two musical behaviours in [AUDIO 15]({m.em}#aud15). 

<Waveform 
title='E-reader sample section displaying active and static states (06-Kindle Off-200513_1547.wav)'
file='/inter/eee-seed.mp3'
peaks='/inter/eee-seed.dat'
id='aud15'
caption='AUDIO 15'
points={ [ { time: 18.3, editable: false, labelText: 'Interjection'} ] }
segments= {[
    { startTime: 0.0, endTime: 18.3, labelText: 'Static' },
    { startTime: 18.3, endTime: 136.0, labelText: 'Active' },
]}
/>

I wanted to separate these two behaviours and deconstruct their relationship — incorporating it as a formal aspect of this piece as well as allowing it to inform my treatment and arrangement of compositional materials. [FTIS]({m.ftis}) was instrumental in evolving this concept and facilitated the necessary segmentation and deconstruction processes.

#### Deconstruction Of Active And Static States

Broadly speaking, the active and static states were treated differently and with a unique approach in mind. The static state is treated as a "base-layer" which is ornamented with other samples, while the active state is deconstructed in order to recycle that material and proliferate it in time. 

The first three and a half minutes of the piece are based around extending the antiphonal nature of the source material. To do this, I began by manually segmenting the unprocessed e-reader sample into regions of active and static behaviour. This way I had access to those two different behaviours in isolation. I created a number of configurations in which the static texture is interjected by the initiating gesture found in the active state. Using [ReaCoMa]({m.reacoma}) to segment this active gesture allowed me to form variations of it, by reordering short segments. The static material is mostly left untouched. Composing this short section came first in the compositional process and functioned as an exploratory phase — seeing what organisations and juxtapositions of the static and active would work compositionally. The next six minutes of the piece were composed after this exploratory phase.

At 3:41, I refrained from introducing any new interjections and started developing the static musical behaviour. I used the k-d approach (the same employed in *{m.anchors}* and depicted in [DEMO 1]({m.em}#demo1)) to have the computer return perceptually similar sounds from the combined corpus containing [Reconstruction Error]({m.re}) and [Interferences]({m.em}) samples. The code that performed this can be found here: [isolate_sample.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/isolate_sample.py). Several complementary sounds were derived from this which I then incrementally superimposed onto the static texture. Some of the sounds returned were very short so I juxtaposed them contiguously in order to create textural material. These blended subtly, and merged with the static material. These were progressively layered until 6:48 which begins a new section.

At 6:48 the texturally dense musical behaviour is interrupted by the return of a gesture extracted from the active material. This demarcates the beginning of a new section in which I deconstructed and reconstructed this gesture in a number of ways to iterate on, and temporally extend it. In order to achieve this, I used [ReaCoMa]({m.reacoma}) to first create short granular segments which I then rearranged and duplicated into prolonged concatenated sequences. To avoid a sense of strong repetition I then used a Lua script to shuffle the order of those contiguous items randomly. [VIDEO 6]({m.em}#vid6) demonstrates how this was performed.

<VideoMedia2 
url='/inter/reorganising-material'
caption='Using ReaCoMa to segment and extend the active gesture '
figure='VIDEO 6'
id='vid6'
/>

I also processed the same granular segments using one of the [ReaCoMa]({m.reacoma}) "sorting" scripts, which takes a group of selected media items and arranges them in the timeline view according to audio descriptor values. I used this to sort the segments according to perceptual loudness. The outcome of this tends toward grouping segments with similar dynamic profiles, resulting in a glitch-like and artificial sonic result. This can be heard in [AUDIO 16]({m.em}#aud16). This particular type of gesture can be heard at 7:22, 7:36, 8:03 and 10:46.

<Waveform 
id='aud16'
title='ReaCoMa sorting used on small segments of active gesture.'
caption='AUDIO 16'
peaks='/inter/sorter-reacoma.dat'
file='/inter/sorter-reacoma.mp3'
/>

In the final section beginning at 8:32, the separation of static and active states becomes less discrete and the musical behaviours are merged and blended together. Rhythmicity is central to this section, as well as the sense that the structure of this is loose and off the grid. As such, this section is structured around my placement of whole samples that are inherently rhythmic or iterated without processing, as well as my synthesis of such musical behaviours through segmentation and arrangement. For example, I returned to some of the original data produced by the process described earlier [[4.5.2.3.1 Ad-Hoc Experiments]]({m.em}#ad-hoc-experiments). Several of the groups within the clustering output that I produced at this phase of the composition process contained samples that were borderline between active and static states. They were relatively stationary in nature, but when observed over shorter time scales presented localised repeating and looping changes in texture and dynamics. These samples were drawn into this section as foundational layers upon which rhythmic structures were synthesised by myself. These foundational layers can be heard in isolation in [AUDIO 17]({m.em}#aud17).

<Album 
title='Foundational layers'
figure='AUDIO 17'
tracks={foundations}
id='aud16'
/>

These foundational layers were left mostly untreated, while I performed surgical segmentation and rearrangement of short granular materials extracted from the active state in order to create my own synthetic rhythmic passages. The segmentation was again executed with [ReaCoMa]({m.reacoma}), and the organisation of the results were structured using the same interactive item spacing script described in [[4.4.3.4 segmnoittet]]({m.re}#vid12). Two examples of this can be heard in [AUDIO 18]({m.em}#aud17).

<Album 
title='Intuitively composed rhythmic constructs.'
figure='AUDIO 18'
tracks={rhythmicConstructs}
id='aud17'
/>

## Reflection
Interferences was a highly successful project in terms of consolidating my computer-aided workflow towards a stable set of tools and creative coding interfaces. It was the first project where I was able to use [FTIS]({m.ftis}) and although I anticipated additional friction from having to support a complex piece of bespoke software, this created a fruitful dialogue between me and the computer. Compositional blockages and frustrations drove technological development, and those technological developments catalysed further compositional innovation and breakthroughs. {m.anchors}, for example, was significantly influenced by improvements to the *Corpus* object, allowing me to fluidly draw together multiple corpora together to be treated as a single object. This development itself was prompted by my feeling that there was not enough variety in the induction recordings and so I needed to create mechanisms for combining other sources in my computer-aided sample searching. Furthermore, I was engaged in this project more often in *soundful* forms of exploration and experimentation with the computer. This can be attributed to the fact that [FTIS]({m.fits}) enabled me to fluidly move between REAPER, Max and scripting different machine learning and content-aware programs.

At the time, I was unable to foresee the ramifications that this would have musically on the EP as a whole. Enabling me to incorporate separate corpora into my creative coding, increased the variety of sounds in those processes while not disrupting my interaction and flow with [FTIS]({m.ftis}) through scripting. This was important to me in not feeling like I was diverging in the compositional process, but rather just *exploring more deeply* in an authentic way, and without having to manage technical concerns while working with sound. Combining different corpora sources at times improved the results of processes in which I aimed to find specific morphological and textural material through matching. This was pivotal in *{m.eee}* for example, as the limited subset of materials contained in the active and static states were only proliferated by my finding appropriately similar and connected samples to compose with. Without this access to similarity, there would not have been enough novel material to compose the longer form of this work.

In addition to this, drawing together several sound sources into one computational object created aesthetic bridges between the unique qualities, behaviours and characteristics of samples from each source. For example, at the beginning of composing {m.anchors} , I aimed to use a constrained set of morphologically stationary sounds only from the induction recordings. Using the k-d tree to *branch* out through a spatial representation of samples within both corpora based on the CQT analysis, was like observing the connections between one corpus and another and seeing their shared characteristics. Instead of conceiving of the two corpora as separate entities with their own creative history and baggage, they became unified as a collection and this challenged the material boundaries of the project. The piece became much more texturally variegated from this, and the tension between invariant static sounds, and lively dynamic sounds became a central compositional feature. As a result, the computer shaped the piece at its conceptual and aesthetic core.

### Formal Repercussions
In [[2.2.3 Time]]({m.preoc}#time), I describe my compositional approach and thinking toward form as the hierarchical superimposition of increasingly granular blocks of material. Treating form in this way is central to my engagement with the compositional process and the way that I build pieces from atomic building blocks such as audio segments and samples. [FTIS]({m.ftis}) and [ReaCoMa]({m.reacoma}), allowed me to fully explore this through several methodologies of destructuring corpora and corpora items into perceptually relevant taxonomies and groupings. This fostered a workflow where the compositional process was led by engaging with gradually increasing detailed levels of structure in the source sounds. As a result, my treatment of sounds was reflected in the level which I was currently engaging with at the time. For example, both pieces began with high-level interests in the structure of particular sounds. In {m.anchors}, this was situated in the conception of three material groups, derived from conceiving as the whole corpus being neatly divided into static and active archetypes. By deconstructing those groups into three key clusters and discovering connections between them and other sounds within the corpus, the identity of the piece emerged. *{m.eee}* also developed as a direct result of deconstructive processes — the entire piece is predicated on the pulling apart of a single sample. This followed a similar pattern to {m.anchors}, beginning with my dividing of the materials into static and active behaviours and then proliferating material from that point using the computer to aid intuitive compositional decision making.

Overall, deconstructing sounds and restructuring their constituent parts in response to the perceptually guided outputs of [FTIS]({m.ftis}) and [ReaCoMa]({m.reacoma}) led both my low and high-level compositional decision making. The computer became an extension of my hierarchical thinking and was coupled to my compositional action in an inseparable manner. For me this was an important reflection that I made after composing this piece — that I had developed a combination of technologies which closely aligned with fundamental and core models which I conceive of sound. At the start of the other projects in this thesis, I have often shifted to a new approach and endeavoured to find a workflow that would leave the problems of the previous project behind. I did not have this experience from this project, and now only want to extend and improve these existing tools to continue composing in a similar way. 

The next section outlines the technical implementation for the software outputs included in this PhD thesis.

<NextSection 
next="Technical Implementation and Software"
link={m.ti}
/>

<style>
	h1 {counter-reset: h2}
    h2 {counter-reset: h3}
    h3 {counter-reset: h4}
    h4 {counter-reset: h5}

    h1:before {content: "4.5" " "}

    h2:before {
        content: "4.5." counter(h2, decimal) " ";
        counter-increment: h2;
    }
    h3:before {
        content: "4.5." counter(h2, decimal) "." counter(h3, decimal) " ";
        counter-increment: h3;
    }
    h4:before {
        content: "4.5." counter(h2, decimal) "." counter(h3, decimal) "." counter(h4, decimal) " ";
        counter-increment: h4;
    }

    h2.nocount:before, h3.nocount:before, h4.nocount:before {
        content : "";
        counter-increment: none;
    }
</style>
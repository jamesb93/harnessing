<script>
    import Waveform from '$lib/components/Waveform.svelte';
    import VideoMedia from "$lib/components/VideoMedia.svelte";
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
	import NextSection from "$lib/components/NextSection.svelte";
    import {metadata as m} from "../directory.svx";
    import Sage from "$lib/demos/Sage.svelte";
    import { sageUnshuffled, sageShuffled } from "$lib/components/data/ss.js";

</script>

# Stitch/Strata

Spiel about piece

The first performance of this piece was at the [2019 Electric Spring Festival](http://electricspring.co.uk/electric-spring-2019/) using the [Huddersfield Immersive Sounds System (HISS)](http://electricspring.co.uk/electric-spring-2019/). The recording of the piece that is submitted in the portfolio exists in two versions. A binaural version that attempts to capture the spatialisation from the first performance and a normal stereo version.

## Motivations and Influences

At the beginning of this PhD, I focused on the question of "how can the computer be creative in composition?". This question was a driving factor in what I wanted to explore and was a reflection of the curiosities spawned from a previous project, [Biomimicry]({m.biom}) in which the computer was seemingly creative and was able to construct sophisticated formal events in real-time.  I attribute many successful aspects of Biomimicry  in this regard, to the contributions from the improvisers. Primarily, the improviser drove formal events and the computer was only able to respond to the changes and structuring of time imposed by its human counterpart. Reflecting on this, my supervisor Dr. Alex Harker encouraged me to write a piece entirely "in the computer" and without any improvisers or instrumental component. The aim of this challenge was to remove the influence that a human could have on the computer in the performance of a piece and to encourage me to find computational strategies for creating sophisticated formal events.

As such, a major motivation for this project was to investigate how the computer could structure all formal levels of a work, from the selection of individual samples, their arrangement into gestures, their arrangement into sequences of gestures and so on and so forth until the work is additively built from a set of atomic units (samples) and a governing algorithm. Ideally, this would take my influence away from arranging materials at different levels of detail myself, to inventing procedures that the computer would then use to realise a work automatically. In this regard, a number of other practitioners were influencing me and driving this type of thinking. These are described in the next section, ["Other Practices"]({m.ss}#other-practices).

<!-- A motivation for me making the computer generate material is so that I do not have to conceive of all the material myself, and instead can create a strategy in which the computer can encompass variations and different forms of an idea. Within this problem though I want the computer to be able to be seeded with definitive qualities, such as a certain types of sound materials or constraints on how it uses these. Finding a strategy to represent both known and unknown qualities in the system design  is something that I wanted to achieve in this project, ideally so that I can create a form of interplay between aspects which are fixed and those which are not. -->

### Other Practices
Michael Young's "Neural Network Music" (NN Music) was particularly prominent in my set of influences. This is a real-time improvisational piece for an unspecified instrumental improviser and the computer. The piece is based around the computer as an improvisational partner that moves between a number of states. It decides which state to be in based on classifying the sound behaviours of the improvisational partner and the nature of their playing. The caveat of this is that the computer begins with a limited classification model that it then builds up over time in the course of the piece. The neural network that underpins how the computer responds to the performer is constantly retraining on the fly and developing a mapping between a set of actions it has stored in memory and the performance in the moment.

I found the technological approach of this work inspiring for two reasons. Firstly, much of the "composing" is done without necessarily making decisions about specific and detailed musical structures. Instead, a more general model for how sound should be made in real-time is devised and the results of this are somewhat open-ended. Secondly, the computer *forms its own connections* and uses those to make decisions about the structure of its responses to the performer. Those connections are not explicitly pre-composed or made ahead of time and instead are formed in the moment dynamically. 

At the time of composing [Stitch/Strata] I was also reading articles by [Adam Linson](http://percent-s.com/) and interested in his improvisational and creative coding practice. In particular, I found the way that he incorporates the computer into practice as an autonomous creative agent something that I could learn from and adapt to my own needs. Linson et. al (2015), for example, describes the design of "Odessa", a system that uses a subsumption architecture in order to build up an improvisation behaviour. The architecture is described:

<div class="bigquote">

*In Subsumption terminology, the “competing behaviors” of a system are organized into a network of interactive “layers” (Brooks 1999). Following this idea, and further informed by psychological research, Odessa’s behavioral layers consist of its basic agency to spontaneously produce output ("Play"), its ability to respond to musical input ("Adapt"), and its ability to resist by disregarding musical input, introducing silence, and initiating endings ("Diverge").* (Linson et. al., 2015, p. 4)

</div>

IMAGE 1 depicts how at a programmatic level these layers contain individual sound producing and interactive modules which are sensitive to the input of a live performer. By connecting these modules to each other across subsumption architecture, Linson et. al (2015, p. 13). hypothesise that this "would serve to produce cues that suggest intentionality". 

<ImageMedia2 
url="/static/ss/odessa.jpg"
caption='Odessa subsumption architecture taken from Linson et. al (2015). The boxes represent modules belonging to one of the three behavioural layers, "Play", "Diverge", "Adapt".'
figure="IMAGE 1"
/>

The subsumption architecture by Linson et. al. appealed to me in simplifying the problem of having the computer deal with time and form on multiple levels. I envisaged that I could experiment with something similar and construct layers as computational behaviours and then devise various mechanisms for how those layers would interact with each other. In a similar fashion to the resonance I found in Young's work, the computer would not have to be specifically programmed to deal with the minutia of musical decision making, rather, I could build a system that embodied a number of strategies for dealing with decision making, and the details would resolve themselves. 

These kinds of generalisations led to many failed attempts and created such a frustrating compositional process that I ended up rejecting the initial constraints of working "in the computer" and returned to me involving intuitive compositional processes enacted by me manually to finish the piece. This process is described next.

## Compositional Process
In this section I will describe the journey of the compositional process from the earliest experiments composing *with* the computer to the final result. Along the way I will detail some additional musical influences and reflect on the trajectory of this project.

### The Influence of Experimental Vocal Improvisation
Alongside the compositional influences described in [Motivations and Influences]({m.ss}#motivations-and-influences), listening had an impact on the sound materials that I gathered for composition. In this regard,  [Sage Pbbbt](https://sagepbbbt.com), whose enormous catalogue of vocal improvisation "daily sketches" became a trove of material that I would later draw on. These improvisations demonstrate the richness of extended vocal techniques and the diversity of sounds that can be produced. In connection with Sage's practice of extended vocal technique improvisation, I came into contact with the discography of [Phil Minton](https://www.philminton.co.uk) as well as his collaborative efforts with [Audrey Chen](http://www.audreychen.com) in *By The Stream*. Lastly, [Trevor Wishart's](http://www.trevorwishart.co.uk) piece [*Tongues of Fire*](https://www.youtube.com/watch?v=Ude4717dlsQ).

Extended vocal sounds had been a material that I wanted to work with in composition more deeply, so coming in contact with this rich world of experimental vocal improvisation created a confluence of sonic and technological interests. I myself am not a vocal improviser and needed some source materials to compose with and decided to use the entirety of the daily sketches by Sage Pbbbt due to the liberal [Creative Commons](https://creativecommons.org/licenses/by-nc/3.0/) license. My plans were to deconstruct this large body of work and use the computer aid me in selecting and arranging it.

### Building A Corpus Of Materials
I began by segmenting all of the recordings using REAPER's dynamic split tool and configuring it to segment based on the detection of transients. This choice was made because I wanted to extract individual utterances and phonetic-like sounds from composite gesture to be rearranged and re-concatenated. AUDIO 1 shows an example of this , by taking a portion of ["One Breath Poem Failure"](https://sagemusick.bandcamp.com/track/2016-02-16-one-breath-poem-failure), segmenting it and then recombining the segments randomly. The colours of each segment track the displacement of each utterance.

<Sage
    file1="/static/ss/gesture.mp3"
    peaks1="/static/ss/gesture.dat"
    file2="/static/ss/shuffled_gesture.mp3"
    peaks2="/static/ss/shuffled_gesture.dat"
    segs1={sageUnshuffled}
    segs2={sageShuffled}
/>

### Selecting Segments
Experimenting in this rapid way produced results that immediately sparked an interest in me for how I could recombine the individual segments of these gesture and to synthesise new materials by recycling items from this corpus of vocal utterances. The next challenge to face was building a system that would be responsible for organising phrases from these materials. I did not necessarily have a strong idea about what would constitute a phrase in terms of formal scale, but I knew that I wanted to explore recombining segments such that result would be perceptually "smooth", and not discontinuous and digital sounding. Given these loose constraints, I set out experimenting with a number of approaches in which individual segments would be concatenated into continuous *streams*.  

A strategy I explored early on was to employ audio descriptors in order to analyse the RMS, spectral centroid and spectral flatness measure of each sample. I anticipated that these three bits of data would allow me to differentiate each sound from each other by capturing their "brightness" (spectral centroid), "noisiness"  (spectral flatness) and intensity (RMS). I could then use the computer to find samples that were relatively homogenous in the percept. I could then combine samples that were similar enough according to a metric of distance. Using a Max patch, I was able to test this idea by querying a database of this descriptor information to return samples that fit a set of criteria. In [VIDEO 1]({m.ss}#vid1), I demonstrate this in practice, where I query for up to 50 samples from the corpus of segments based on three different constraints. Each slider corresponds to a different descriptor, so by changing the value in this interface I can modify the query.

1. Samples that have a spectral centroid greater than the given threshold
2. Samples that have a spectral flatness measure than a threshold
3. Samples that have an amplitude within 6dB of a given value

<VideoMedia id="vid1">
    <video slot="media" controls>
        <source src="/static/ss/descriptorspace.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/static/ss/descriptorspace.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 1: Querying the corpus of voice segments using audio descriptors.</p>
</VideoMedia>

Using the computer to query the corpus and find groups of samples matching a particular descriptor specification, I was presented with a number of possibilities that appealed to me aesthetically. In particular, results which produced texturally focused groupings were appealing to me, because by simply concatenating those sounds together it was possible to hear how they could form a detailed texture. Another aspect I did not intend to find, but found appealing, was the way that a morphology can be imposed on the concatenative process by changing the thresholds of the query dynamically. This can be seen specifically at 0:20 in [VIDEO 1]({m.ss}#vid1).

This descriptor processor underpinned much of the compositional process to follow. From this point I attempted to devise ways in which the computer could take these notions of forming material by querying by descriptor matching and use it in combination with another systematic layer that would construct form. This is discussed in ["Constructing Form Computationally"]({m.ss}#constructing-form-computationally)

 ### Constructing Form Computationally
In this phase of the composition process I faced a number of challenges when trying to devise systems for controlling form based on the audio descriptor information. A number of strategies were explored in which I attempted to have the computer impose a level of control over what queries were made in order to dynamically create material groups on-the-fly.

One approach was to create predefined query "shapes" that structured a dynamic query over a given period of time. Using this, I could define the overall quality and percept of the gesture, without specifying any of the details and decisions such as which samples would form that gesture would be taken up by the matching algorithm. This type of approach is discussed in Harker (2012), in which he outlines how bespoke Max externals have been used toward similar ends in his compositional practice. VIDEO 2 demonstrates one version of this, where queries are changed according to a gesture that is defined by a number of points to be reached at specific times. This creates a parabolic trajectory that increases and decreases the amplitude and spectral centroid query values over a set amount of time. This produces a strong perceptual link between the query and the sound which as a compositional tool is a useful way of having the computer render complex perceptual arrangements based on a simple way of communicating it.

<VideoMedia id="vid2">
    <video slot="media" controls>
        <source src="/static/ss/dynamic-gesture.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/static/ss/dynamic-gesture.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 2: Dynamic gestures created by dynamically querying a corpus of vocal segments based on audio descriptors.</p>
</VideoMedia>

VIDEO 3 shows another idea in which a static state of very low amplitude is intermittently "escaped" from by updating a query for new samples in real-time. This creates a series of jumps between static states.

<VideoMedia id="vid3">
    <video slot="media" controls>
        <source src="/static/ss/static-jumps.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/static/ss/static-jumps.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 3: A self-contained static musical behaviour is formed by creating an "anchor" in the descriptor space and momentarily departing and then returning to it.</p>
</VideoMedia>

These are just two of the strategies employed. Many are codified into the Max patches, or were left in half finished states and at various stages of development and left try try other things or were unsatisfactory. Ultimately with this approach, I embraced what Harker (2012, p. 7) terms a "perceptual equivalence". Individual samples are interchangeable in regards to the query and variability is constrained to the dynamic shape rather than specific choices. As such, this strategy for having the computer select samples and arranging them revolves around communicating a generalised fixed structure which is detailed and resolved computationally.

Another approach I tried involved creating a behaviour around the notion of "adversarial searching".  For this, I take two starting points within the descriptor space and attach those starting points to two different searches, one that attempts to minimise the volume of the output by changing the spectral centroid and one that tries to find the maximum centroid by minimising the volume. This adversarial process has no resolution but produces longer term morphologies and structures without having to specifically design them at each moment. This kind of thinking carried over and was a precursor in many ways to my engagement with Simulated Annealing in [Annealing Strategies]({m.as}), which exploits a similar searching behaviour. to structure music automatically. This technique produced the some interesting meso-scale results, but produced a lot of directionless forms at the highest formal level. AUDIO 1 is one of the outputs of this process.

<Waveform 
id="wave1"
file="/static/ss/5712433.mp3"
peaks="/static/ss/5712433.dat"
title="AUDIO 1: Adversarial searching"
/>
<!-- - Using descriptor values and accumulators to generate passages with finite amounts of 'energy'
    - Lots of tuning
- sample selection
 -->

Another approach was to create perceptually defined groups of samples through queries that I had found throughout experimentation as compositional units. Using these small collections, I then applied a selection algorithm to select samples based on Puckette (2015). This algorithm creates different patterns by specifying ratios between different outcomes.

<!-- WIDGET: z12 widget -->

In my mind, imposing this amount of rigour would result in a form that was perceivably ordered and might possess sophisticated qualities at the highest formal levels. Most of all though, the algorithm's behaviour was too homogenous in the sense that it *always* changed which given enough listening stops becoming change and instead becomes relatively static.

I have described a number of preliminary approaches to imposing order on descriptor-based queries. An issue that ran throughout all of these approaches was that they introduced more decision making and relinquished control from me that was not necessarily offering any benefits in return. 

Most of the approaches were experimented with in order to create high-level formal structures by selecting samples, but the results lacked direction as well as any sense of *intentionality*  in what the computer produced. To get from these sub-optimal results toward something with a more sophisticated level of organisation I would have to go and edit them manually, which to me seemed nullify the point of working so much with algorithms and complex technical procedures in the first place. Furthermore, this way of approaching the automatic generation of formal structures was framed around recreating formal structures that I would find sophisticated, rather than trying to have something *emerge* from that process. In hindsight, I do not think  that the algorithms and models I was using were complex enough, nor did I have a good enough idea about what aspects of my formal thinking could or should be modelled at the meso- and macro- levels of form.

Ultimately, the frustrations led to a radical change in approach to how I finally composed Stitch/Strata. This change is discussed in the next section ["From Modelling to Assistance"]({m.ss}).

### From Modelling To Assistance
<!-- !!! -->
One aspect that I wanted to keep were descriptor-based selections of material. To me this seemed like a good way of creating collections of material from the wider corpus of phonetic sounds.

- I needed to make a drastic change to my workflow and "break" some boundaries I had put up.
    - Desired balance between human and computer agency (was one of these)

- How did I do this?
    - "Cold turkey" leave Max behind. Project structure was incredibly messy, bloated and I couldn't trace my compositional development. It was a mess of small experiments without being able to track how things had progressed. 
    - With Max there is no 'record'. Files on disk aren't made and the notion of a memory has to be built into a patch. This adds another technical concern.
    - The way I wanted to work required creating a lot of abstraction around very trivial ideas, like looping over a container of audio files representing a gesture.
    - Pick up Python as a new tool
        - break the "rut"
        - Python is a different form of expression
        - itertools to create patterned structures.
    - A different form of expression.
    - Building smaller structures and controlling their order in time myself, or manually blending them was a more fruitful approach than finding a system to govern that type of high-level thinking.
    - Generating numerous sound files with variation very rapidly.
    - Working back in a DAW where things could be fixed to some degree and don't have to be 'programmed' in the same way.
        - Use the inherent "memory" of the DAW to create temporary stores of decision making. This acted like scaffolding to realise the piece

- Whispery textural materials created with high spectral flatness, high centroid and low amplitude searches. Randomly concatenated together to form textures.
    - 3:03

- Give concrete examples in the piece where samples were used
    - much of the original form is lost behind manual edits and cuts that arent traceable.
    - start are descriptor gestural shapes
    - /samples/
        - 3:15 > 3:50
            - a + b interspersed with each other 
        -  4:13 onwards

        - e 15
            - Interspersed among longer gestures (4:47, )
        - g
            - glitch-like
            - 17 pitchy stuff possible 4:38 
        - h
            - 4:53
        - /phrases/
            - output 4
            - output 11 is cut up in various short ways too and interspersed
        - The kind of complexity and recapitulation of motives on many levels was something I tried to have emerge in the z12 approach

Having the generated outputs and being able to listen to them inspired how they could be used which then led immediately to compositional action and results. This was a more fruitful workflow than going back into the technical side of things.

## Reflecting On The Balance Of Human And Computer Agency
- The transition from attempting to model specific musical structures, toward building smaller ones and working with them intuitively shifted the balance of contribution made by the computer to the compositional process. Instead of focusing on having the computer formalise most if not all aspects of a work using detailed systems, I moved toward a strategy in which the computer presented options to me, which I could then modify, process, change in place all while leveraging the interface of the DAW. 

- The DAW reorients practice in many ways through its affordances as an environment for composition. 
    - memory bank for ideas which can be held in half-finished states.
    - Ordering of ideas can be changed 
    - Processes can be tested on specific musical ideas/objects/sequences
    - Creates opportunity to intervene in many ways and to step between generation and manipulation. 
 
### Moving Away From Modelling
- I dont know my own rules, patterns or restrictions and so modelling becomes too complex of a process in which I am trying to both codify a musical language
- I dont want to do automatic composition because I want to work non linearly at times, moving between different types of composing (sound-design, low-level manipulation of utterances or big picture). Only interacting with the musical result through an algorithm feels less conducive to this and often focuses me toward having to build something that can't be in a half-finished state.
- A better balance between "soundful practise" and technological practise.

- I want to feel like the time structure is uncovered/discovered rather than designed as part of the material.
    - Allude to annealing strategies
        - on the threshold here and less about specific codified structures and more about a shape or simple language to compose with

how do i take reponsiblity for what comes out - how does the computer side interact with my practice.

<NextSection 
next="Annealing Strategies"
link={m.as}
/>

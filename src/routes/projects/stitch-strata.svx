<script>
	import NextSection from "$lib/components/NextSection.svelte";
    import {metadata as m} from "../directory.svx";
    import Sage from "$lib/demos/Sage.svelte";
    import { sageUnshuffled, sageShuffled } from "$lib/components/data/ss.js";

</script>

# Stitch/Strata

Spiel about piece

The first performance of this piece was at the [2019 Electric Spring Festival](http://electricspring.co.uk/electric-spring-2019/) using the [Huddersfield Immersive Sounds System (HISS)](http://electricspring.co.uk/electric-spring-2019/). The recording of the piece that is submitted in the portfolio exists in two versions. A binaural version that attempts to capture the spatialisation from the first performance and a normal stereo version.

## Motivations and Influences

At the beginning of this PhD, I focused on the question of *"how can the computer be creative in composition?"*. This question is unconstrained and general but nonetheless was a driving factor in what I wanted to explore and was a reflection of the curiosities spawned from a previous project, [Biomimicry]({m.biom}) in which the computer was seemingly creative and influential in the production of that work. To open up the initial question, my supervisor Dr. Alex Harker encouraged me to write a piece entirely "in the computer" and without any improvisers involved. The aim of this challenge was to remove the influence that a human agent could have on the process of dynamically generating music with the computer. In Biomimicry, many successful aspects of that work could have been attributed to the sonic contributions from the improviser. The computer only ever responded to the material that they fed it, so the quality of their sound making and improvisation largely affected the success of the work.

Without an improviser the compositional decision making performed by the computer becomes more exposed and the programming performed by the composer becomes vital in the success of the music generated by the machine. One aspect that I knew I would struggle with in this project was having the computer devise formal structures. An improviser can help to structure a work in an often satisfying and effective way without having to necessarily tell them what to do. The computer is then implicitly attached to that process of formal shaping in a real-time performance. In the absence of this, a model of form has to be created so that the computer can make these decisions alone.

As such, a major motivation for this project was to investigate how the computer could structure all formal levels of a work, from the selection of individual samples, their arrangement into gestures, their arrangement into sequences of gestures and so on and so forth until the work is fleshed out using a set of atomic units (samples) and a governing algorithm for arranging them. Ideally, this would take my influence away from directly structuring materials, to inventing schemes and algorithms that the computer would then use to realise a work.

Given the challenge of working only with the computer, a major challenge I envisaged was 
- How to achieve a balance between unpredictability and control
    - Interplay between "knowns" and unexpected things
    - Digression and dynamic changing of direction


Michael Young's "Neural Network Music" (NN Music) was particularly prominent in my set of influences. This is a real-time improvisational piece for an unspecified instrumental improviser and the computer. The piece is based around the computer treating the improvisational "partner" as an agent that can engage in a number of states that can be recognised through classification. The computer's role is then to detect those states and to generate sympathetic musical outputs from a selection of pre-designed musical ideas. The caveat of this is that the computer begins with a limited classification model that it then builds up over time in the course of the piece. The neural network that underpins how the computer responds to the performer is constantly retraining on the fly.

Despite not wanting to write a piece for real-time electronics and an improviser, I found the technological approach of this work inspiring for two reasons. Firstly, much of the "composing" is done without necessarily making decisions about specific and detailed musical structures. Instead, a more general model for how sound should be made in real-time is devised and the results of this are somewhat open-ended. Secondly, the computer *forms its own connections* and uses those to make decisions about the structure of its responses to the performer. Those connections are not explicitly pre-composed or made ahead of time and instead are formed in the moment dynamically.

This led to many failed schemes and created such a frustrating compositional process that I ended up rejecting the initial constraints of working "in the computer" and returned to me involving intuitive compositional processes enacted by me manually to finish the piece.

Several artists were influencing my listening and compositional thought while composing Stitch/Strata. The binding factor between these influences was an interest extended vocal techniques and the vast number of sounds which could be produced by a single source. This material fascination had long been something I wanted to work with in composition so I took the opportunity for Stitch/Strata to focus my efforts on collecting and engaging with sounds from vocal improvisers. A major influence in this regard was [Sage Pbbbt](https://sagepbbbt.com), whose enormous catalogue of "daily sketches" informed much of my listening. This collection of improvisations demonstrates the richness of vocal extended techniques and the variety of sounds that can be produced from short iterated transients to long textural inhalations. In connection with this practice of extended vocal technique improvisation I came into contact with the discography of [Phil Minton](https://www.philminton.co.uk) as well as his collaborative efforts with [Audrey Chen](http://www.audreychen.com) in *By The Stream*. Lastly, [Trevor Wishart's](http://www.trevorwishart.co.uk) piece [*Tongues of Fire*](https://www.youtube.com/watch?v=Ude4717dlsQ).

## Initial

For Stitch/Strata I wanted to use the sounds of extended vocal techniques and decided early on that this would be the material that the work would be based on. I myself am not a vocal improviser and needed some source materials to compose with and decided to use the entirety of the daily sketches by Sage Pbbbt due to their liberal [Creative Commons](https://creativecommons.org/licenses/by-nc/3.0/)license. 

I began by segmenting all of the recordings using REAPER's dynamic split tool and configuring it to segment at the transients. The reason for this is that I wanted to extract individual utterances and phonetic-like sounds from composite gesture to be rearranged and re-concatenated. AUDIO 1 shows an exmaple of this , by taking a portion of ["One Breath Poem Failure"](https://sagemusick.bandcamp.com/track/2016-02-16-one-breath-poem-failure), segmenting it and then recombining it entirely randomly. The segment colours show the the segments from the source material are modified in time.

<Sage
file1="https://jbphd-pub.s3.us-west-000.backblazeb2.com/ss/gesture.mp3"
peaks1="https://jbphd-pub.s3.us-west-000.backblazeb2.com/ss/gesture.dat"
segs1={sageUnshuffled}
file2="https://jbphd-pub.s3.us-west-000.backblazeb2.com/ss/shuffled_gesture.mp3"
peaks2="https://jbphd-pub.s3.us-west-000.backblazeb2.com/ss/shuffled_gesture.dat"
segs2={sageShuffled}
/>

This was the central tenent of this piece for me, I wanted to recombine the individual segments of these gesture to create new ones and to synthesise new materials with well defined perceptual qualities.

- A variety of approaches that were unsatisfactory.
    - Most approaches involved navigating through a descriptor space with different patterns in order to generate perceptual shapes.
    - Using descriptor values and accumulators to generate passages with finite amounts of 'energy'
    - Different arbitrary assignments, thinking of the corpus as a 'language' of phonetics that can be reconstructed into words
    - Simulated Annealing
    - Issues
        - Lacking formal direction and intention
        - Way too much time spent on low-level detail that had no relevance to the higher-level form (lots of friction)
        - Thinking about what I would 'want' to do, and an appropriate model for that. Lots of layers of abstraction to fight through to get to musical result when I already had a good idea about the types of sounds and organisations I wanted. Basically questioning why I would have the computer make certain decisions automatically?

initial_attempts > recordings > 5712433.txt

- sample selection
    - z12 and shuffling samples.
        - Just another level of shuffle, not necessarily interpreted as an something with order or a level of organisation. 

## From Modelling To Assistance
- I needed to make a drastic change to my workflow and "break" some boundaries I had put up.
    - Desired balance between human and computer agency (was one of these)

- How did I do this?
    - Python is a different form of expression
    - Using Python, to iterating over possibilities and generating small structures
    - Building smaller structures and controlling their order in time myself, or manually blending them was a more fruitful approach than fiding a system to govern that type of high-level thinking.
    - Working back in a DAW where things could be fixed to some degree and don't have to be 'programmed' in the same way.

<!-- TODO REFERENCES TO SPECIFIC MOMENTS IN THE PIECE -->
- Specific Breakdown of patches > resulting music
    - gestures.maxpat to construct gestures with dynamic descriptor searches

## Reflecting On The Balance Of Human And Computer Agency
- The transition from attempting to model specific musical structures, toward building smaller ones and working with them intuitively shifted the balance of contribution made by the computer to the compositional process. Instead of focusing on having the computer formalise most if not all aspects of a work using detailed systems, I moved toward a strategy in which the computer presented options to me, which I could then modify, process, change in place all while leveraging the interface of the DAW. 

- The DAW reorients practice in many ways through its affordances as an environment for composition. 
    - memory bank for ideas which can be held in half-finished states.
    - Ordering of ideas can be changed 
    - Processes can be tested on specific musical ideas/objects/sequences
    - Creates opportunity to intervene in many ways and to step between generation and manipulation. 
 
### Moving Away From Modelling
- I dont know my own rules, patterns or restrictions and so modelling becomes too complex of a process in which I am trying to both codify a musical language
- I dont want to do automatic composition because I want to work non linearly at times, moving between different types of composing (sound-design, low-level manipulation of utterances or big picture). Only interacting with the musical result through an algorithm feels less conducive to this.
- - A better balance between "soundful practise" and technological practise.

- I want to feel like the time structure is uncovered/discovered rather than designed as part of the material.
    - Allude to annealing strategies
        - on the threshold here and less about specific codified structures and more about a shape or simple language to compose with

how do i take reponsiblity for what comes out - how does the computer side interact with my practice


<NextSection 
next="Annealing Strategies"
link={m.as}
/>

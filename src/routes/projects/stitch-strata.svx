<script>
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
	import NextSection from "$lib/components/NextSection.svelte";
    import {metadata as m} from "../directory.svx";
    import Sage from "$lib/demos/Sage.svelte";
    import { sageUnshuffled, sageShuffled } from "$lib/components/data/ss.js";

</script>

# Stitch/Strata

Spiel about piece

The first performance of this piece was at the [2019 Electric Spring Festival](http://electricspring.co.uk/electric-spring-2019/) using the [Huddersfield Immersive Sounds System (HISS)](http://electricspring.co.uk/electric-spring-2019/). The recording of the piece that is submitted in the portfolio exists in two versions. A binaural version that attempts to capture the spatialisation from the first performance and a normal stereo version.

## Motivations and Influences

At the beginning of this PhD, I focused on the question of "how can the computer be creative in composition?". This question was a driving factor in what I wanted to explore and was a reflection of the curiosities spawned from a previous project, [Biomimicry]({m.biom}) in which the computer was seemingly creative and was able to construct sophisticated formal events in real-time.  I attribute many successful aspects of Biomimicry  in this regard, to the contributions from the improvisers. Primarily, the improviser drove formal events and the computer was only able to respond to the changes and structuring of time imposed by its human counterpart. Reflecting on this, my supervisor Dr. Alex Harker encouraged me to write a piece entirely "in the computer" and without any improvisers or instrumental component. The aim of this challenge was to remove the influence that a human could have on the computer in the performance of a piece and to encourage me to find computational strategies for creating sophisticated formal events.

As such, a major motivation for this project was to investigate how the computer could structure all formal levels of a work, from the selection of individual samples, their arrangement into gestures, their arrangement into sequences of gestures and so on and so forth until the work is additively built from a set of atomic units (samples) and a governing algorithm. Ideally, this would take my influence away from arranging materials at different levels of detail myself, to inventing procedures that the computer would then use to realise a work automatically. In this regard, a number of other practitioners were influencing me and driving this type of thinking. These are described in the next section, ["Other Practice"]({m.ss}#other-practice).

<!-- A motivation for me making the computer generate material is so that I do not have to conceive of all the material myself, and instead can create a strategy in which the computer can encompass variations and different forms of an idea. Within this problem though I want the computer to be able to be seeded with definitive qualities, such as a certain types of sound materials or constraints on how it uses these. Finding a strategy to represent both known and unknown qualities in the system design  is something that I wanted to achieve in this project, ideally so that I can create a form of interplay between aspects which are fixed and those which are not. -->

### Other Practice

Michael Young's "Neural Network Music" (NN Music) was particularly prominent in my set of influences. This is a real-time improvisational piece for an unspecified instrumental improviser and the computer. The piece is based around the computer as an improvisational partner that moves between a number of states. It decides which state to be in based on classifying the sound behaviours of the improvisational partner and the nature of their playing. The caveat of this is that the computer begins with a limited classification model that it then builds up over time in the course of the piece. The neural network that underpins how the computer responds to the performer is constantly retraining on the fly and developing a mapping between a set of actions it has stored in memory and the performance in the moment.

I found the technological approach of this work inspiring for two reasons. Firstly, much of the "composing" is done without necessarily making decisions about specific and detailed musical structures. Instead, a more general model for how sound should be made in real-time is devised and the results of this are somewhat open-ended. Secondly, the computer *forms its own connections* and uses those to make decisions about the structure of its responses to the performer. Those connections are not explicitly pre-composed or made ahead of time and instead are formed in the moment dynamically. 

At the time of composing [Stitch/Strata] I was also reading articles by [Adam Linson](http://percent-s.com/) and interested in his improvisational and creative coding practice. In particular, I found the way that he incorporates the computer into practice as an autonomous creative agent something that I could learn from and adapt to my own needs. Linson et. al (2015), for example, describes the design of "Odessa", a system that uses a subsumption architecture in order to build up an improvisation behaviour. The architecture is described:

<div class="bigquote">

*In Subsumption terminology, the “competing behaviors” of a system are organized into a network of interactive “layers” (Brooks 1999). Following this idea, and further informed by psychological research, Odessa’s behavioral layers consist of its basic agency to spontaneously produce output ("Play"), its ability to respond to musical input ("Adapt"), and its ability to resist by disregarding musical input, introducing silence, and initiating endings ("Diverge").* (Linson et. al., 2015, p. 4)

</div>

IMAGE 1 depicts how at a programmatic level these layers contain individual sound producing and interactive modules which are sensitive to the input of a live performer. By connecting these modules to each other across subsumption architecture, Linson et. al (2015, p. 13). hypothesise that this "would serve to produce cues that suggest intentionality". 

<ImageMedia2 
url="/static/ss/odessa.jpg"
caption='Odessa subsumption architecture taken from Linson et. al (2015). The boxes represent modules belonging to one of the three behavioural layers, "Play", "Diverge", "Adapt".'
figure="IMAGE 1"
/>

The subsumption architecture by Linson et. al. appealed to me in simplifying the problem of having the computer deal with time and form on multiple levels. I envisaged that I could experiment with something similar and construct layers as computational behaviours and then devise various mechanisms for how those layers would interact with each other. In a similar fashion to the resonance I found in Young's work, the computer would not have to be specifically programmed to deal with the minutia of musical decision making, rather, I could build a system that embodied a number of strategies for dealing with decision making, and the details would resolve themselves. 

These kinds of generalisations led to many failed attempts and created such a frustrating compositional process that I ended up rejecting the initial constraints of working "in the computer" and returned to me involving intuitive compositional processes enacted by me manually to finish the piece. This process is described next.

## Compositional Process
In this section I will descirbe the journey of the compositional process from the earliest experiments composing *with* the computer to the final result. Along the way I will detail some additional musical influences and reflect on the trajectory of this project.

### The Influence of Experimental Vocal Improvisation
Alongside the compositional influences described in [Motivations and Influences]({m.ss}#motivations-and-influences), listening *without* analysing compositional and technological method was formative in the kinds of material that I used. Among these influences was [Sage Pbbbt](https://sagepbbbt.com), whose enormous catalogue of vocal improvisation "daily sketches" became a trove of material that I would later draw on. Sage's improvisations demonstrate the richness of extended vocal techniques and the diversity of sounds that can be produced, from short iterated transients to long textures from inhalations. In connection with their practice of extended vocal technique improvisation I came into contact with the discography of [Phil Minton](https://www.philminton.co.uk) as well as his collaborative efforts with [Audrey Chen](http://www.audreychen.com) in *By The Stream*. Lastly, [Trevor Wishart's](http://www.trevorwishart.co.uk) piece [*Tongues of Fire*](https://www.youtube.com/watch?v=Ude4717dlsQ).

Vocal sounds had been a material that I wanted to work with in composition more deeply, so coming in contact with this rich world of experimental vocal improvisation resulted in a confluence sonic and technological interests. I myself am not a vocal improviser and needed some source materials to compose with and decided to use the entirety of the daily sketches by Sage Pbbbt due to the liberal [Creative Commons](https://creativecommons.org/licenses/by-nc/3.0/) license. 

### Building A Corpus Of Materials
I began by segmenting all of the recordings using REAPER's dynamic split tool and configuring it to segment at the transients. The reason for this is that I wanted to extract individual utterances and phonetic-like sounds from composite gesture to be rearranged and re-concatenated. AUDIO 1 shows an example of this , by taking a portion of ["One Breath Poem Failure"](https://sagemusick.bandcamp.com/track/2016-02-16-one-breath-poem-failure), segmenting it and then recombining the segments randomly. The colours of each segment track the displacement of each utterance.

<Sage
    file1="/static/ss/gesture.mp3"
    peaks1="/static/ss/gesture.dat"
    file2="/static/ss/shuffled_gesture.mp3"
    peaks2="/static/ss/shuffled_gesture.dat"
    segs1={sageUnshuffled}
    segs2={sageShuffled}
/>

Experimenting in this rapid, cheap way produced results that immediately sparked an interest in how I might recombine the individual segments of these gesture to create new ones and to synthesise new materials through recycling.

- A variety of approaches that were unsatisfactory.
    - Most approaches involved navigating through a descriptor space with different patterns in order to generate perceptual shapes.
    - Using descriptor values and accumulators to generate passages with finite amounts of 'energy'
    - Different arbitrary assignments, thinking of the corpus as a 'language' of phonetics that can be reconstructed into words
    - Simulated Annealing
    - Issues
        - Lacking formal direction and intention
        - Way too much time spent on low-level detail that had no relevance to the higher-level form (lots of friction)
        - Thinking about what I would 'want' to do, and an appropriate model for that. Lots of layers of abstraction to fight through to get to musical result when I already had a good idea about the types of sounds and organisations I wanted. Basically questioning why I would have the computer make certain decisions automatically?

initial_attempts > recordings > 5712433.txt

- sample selection
    - z12 and shuffling samples.
        - Just another level of shuffle, not necessarily interpreted as an something with order or a level of organisation. 

## From Modelling To Assistance
- I needed to make a drastic change to my workflow and "break" some boundaries I had put up.
    - Desired balance between human and computer agency (was one of these)

- How did I do this?
    - Python is a different form of expression
    - Using Python, to iterating over possibilities and generating small structures
    - Building smaller structures and controlling their order in time myself, or manually blending them was a more fruitful approach than fiding a system to govern that type of high-level thinking.
    - Working back in a DAW where things could be fixed to some degree and don't have to be 'programmed' in the same way.

<!-- TODO REFERENCES TO SPECIFIC MOMENTS IN THE PIECE -->
- Specific Breakdown of patches > resulting music
    - gestures.maxpat to construct gestures with dynamic descriptor searches

## Reflecting On The Balance Of Human And Computer Agency
- The transition from attempting to model specific musical structures, toward building smaller ones and working with them intuitively shifted the balance of contribution made by the computer to the compositional process. Instead of focusing on having the computer formalise most if not all aspects of a work using detailed systems, I moved toward a strategy in which the computer presented options to me, which I could then modify, process, change in place all while leveraging the interface of the DAW. 

- The DAW reorients practice in many ways through its affordances as an environment for composition. 
    - memory bank for ideas which can be held in half-finished states.
    - Ordering of ideas can be changed 
    - Processes can be tested on specific musical ideas/objects/sequences
    - Creates opportunity to intervene in many ways and to step between generation and manipulation. 
 
### Moving Away From Modelling
- I dont know my own rules, patterns or restrictions and so modelling becomes too complex of a process in which I am trying to both codify a musical language
- I dont want to do automatic composition because I want to work non linearly at times, moving between different types of composing (sound-design, low-level manipulation of utterances or big picture). Only interacting with the musical result through an algorithm feels less conducive to this.
- - A better balance between "soundful practise" and technological practise.

- I want to feel like the time structure is uncovered/discovered rather than designed as part of the material.
    - Allude to annealing strategies
        - on the threshold here and less about specific codified structures and more about a shape or simple language to compose with

how do i take reponsiblity for what comes out - how does the computer side interact with my practice


<NextSection 
next="Annealing Strategies"
link={m.as}
/>

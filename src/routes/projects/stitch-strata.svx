<script>
    import Waveform from '$lib/components/Waveform.svelte';
    import VideoMedia from "$lib/components/VideoMedia.svelte";
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
	import NextSection from "$lib/components/NextSection.svelte";
    import {metadata as m} from "../directory.svx";
    import Sage from "$lib/demos/Sage.svelte";
    import { sageUnshuffled, sageShuffled } from "$lib/components/data/sage.js";

</script>

# Stitch/Strata

<Waveform 
title="Stitch Strata (Stereo Mixdown)"
file="/pieces/ss.mp3"
peaks="/pieces/ss.dat"
id="audio1"
/>

The first performance of this piece was at the [2019 Electric Spring Festival](http://electricspring.co.uk/electric-spring-2019/) using the [Huddersfield Immersive Sounds System (HISS)](http://electricspring.co.uk/electric-spring-2019/). The recording of the piece that is submitted in the portfolio exists in two versions. A binaural version that attempts to capture the spatialisation from the first performance and a normal stereo version.

## Motivations and Influences

At the beginning of this PhD, I focused on the question of *"how can the computer be creative in composition?"*. This question was indicative of the curiosities spawned from a previous project, [Biomimicry]({m.biom}), which involved the computer responding to an instrumental performer and was seemingly creative and able to construct sophisticated formal events in real-time.  I attribute many successful aspects of Biomimicry  in this regard, to the contributions from the improvisers. Primarily, the improviser drove formal events and the computer was only able to respond to the changes and structuring of time imposed by the human counterpart. Reflecting on this, my supervisor Dr. Alex Harker encouraged me to write a piece entirely "in the computer" and without any improvisers or instrumental component. The aim of this challenge was to remove the in-the-moment influence of an intelligent and musically capable human and to encourage me to find computational strategies for creating sophisticated formal structures.

As such, a major motivation for this project was to investigate how the computer could structure all formal levels of a work, from the selection of individual samples, their arrangement into gestures, their arrangement into sequences of gestures and so on and so forth until the work is additively built from a set of atomic units (samples) with a governing algorithm. Ideally, this would take my influence away from arranging materials at different levels of detail myself, to inventing procedures that the computer would then use to realise a work automatically. In this regard, a number of other practitioners were influencing me and driving this type of thinking. These are described in the next section, ["Other Practices"]({m.ss}#other-practices).

<!-- A motivation for me making the computer generate material is so that I do not have to conceive of all the material myself, and instead can create a strategy in which the computer can encompass variations and different forms of an idea. Within this problem though I want the computer to be able to be seeded with definitive qualities, such as a certain types of sound materials or constraints on how it uses these. Finding a strategy to represent both known and unknown qualities in the system design  is something that I wanted to achieve in this project, ideally so that I can create a form of interplay between aspects which are fixed and those which are not. -->

### Other Practices
Michael Young's "Neural Network Music" (NN Music) was particularly prominent in my set of influences. This is a real-time improvisational piece for an unspecified instrumental improviser and the computer. The piece is based around the computer as an improvisational partner that moves between a number of states. It decides which state to be in based on classifying the sound behaviours of the improvisational partner and the nature of their playing. The caveat of this is that the computer begins with a limited classification model that it then builds up over time in the course of the piece. The neural network that underpins how the computer responds to the performer is constantly retraining on the fly and developing a mapping between a set of actions it has stored in memory and the performance in the moment.

While Young's piece relies heavily on the collaboration and musical sensitivity of the improvisor, I found the technological approach of this work inspiring for two reasons. Firstly, much of the "composing" is done without necessarily making decisions about specific and detailed musical structures. Instead, a more general model for how sound should be made in real-time is devised and the results of this are somewhat open-ended. Secondly, the computer *forms its own connections* and uses those to make decisions about the structure of its responses to the performer. Those connections are not explicitly pre-composed or made ahead of time and instead are formed in the moment dynamically.

At the time of composing [Stitch/Strata] I was also reading articles by [Adam Linson](http://percent-s.com/) and interested in his improvisational and creative coding practice. In particular, I found the way that he incorporates the computer into practice as an autonomous creative agent something that I could learn from and adapt to my own needs. Linson et. al (2015), for example, describes the design of "Odessa", a system that uses a subsumption architecture in order to build its improvisation behaviour. The architecture is described:

<div class="bigquote">

*In Subsumption terminology, the “competing behaviors” of a system are organized into a network of interactive “layers” (Brooks 1999). Following this idea, and further informed by psychological research, Odessa’s behavioral layers consist of its basic agency to spontaneously produce output ("Play"), its ability to respond to musical input ("Adapt"), and its ability to resist by disregarding musical input, introducing silence, and initiating endings ("Diverge").* (Linson et. al., 2015, p. 4)

</div>

IMAGE 1 depicts how at a programmatic level these layers contain individual sound producing and interactive modules which are sensitive to the input of a live performer. By connecting these modules to each other across subsumption architecture, Linson et. al (2015, p. 13). hypothesise that this "would serve to produce cues that suggest intentionality". 

<ImageMedia2 
url="/ss/odessa.jpg"
caption='Odessa subsumption architecture taken from Linson et. al (2015). The boxes represent modules belonging to one of the three behavioural layers, "Play", "Diverge", "Adapt".'
figure="IMAGE 1"
/>

The subsumption architecture by Linson et. al. appealed to me in simplifying the problem of having the computer deal with time and form on multiple levels. I envisaged that with a subsumption architecture, I could construct layers as computational behaviours and then devise various mechanisms for how those layers would interact with each other. In a similar fashion to the resonance I found in Young's work, the computer would not have to be specifically programmed to deal with the minutia of musical decision making, rather, I could build a system that embodied a number of strategies for dealing with decision making, and the musical details would resolve themselves. 

These kinds of generalisations led to many failed attempts and created such a frustrating compositional process that I ended up rejecting the initial constraints of working "in the computer" and prompted me to rely much more heavily on intuitive compositional processes enacted by me manually to finish the piece than I anticipated. This process is described next in ["Compositional Process"]({m.ss}#compositional-process).

## Compositional Process
In this section I will describe the journey of the compositional process from the earliest experiments to the final result. Along the way I will detail some additional musical influences and reflect on the trajectory of this project.

### The Influence of Experimental Vocal Improvisation
Alongside the compositional influences described in [Motivations and Influences]({m.ss}#motivations-and-influences), listening had an impact on the sound materials that I gathered for composition. In this regard,  [Sage Pbbbt](https://sagepbbbt.com), whose enormous catalogue of vocal improvisation "daily sketches" became a trove of material that I would later draw on. These improvisations demonstrate the richness of extended vocal techniques and the diversity of sounds that can be produced. In connection with Sage's practice of extended vocal technique improvisation, I came into contact with the discography of [Phil Minton](https://www.philminton.co.uk) as well as his collaborative efforts with [Audrey Chen](http://www.audreychen.com) in *By The Stream*. Lastly, [Trevor Wishart's](http://www.trevorwishart.co.uk) piece [*Tongues of Fire*](https://www.youtube.com/watch?v=Ude4717dlsQ).

Extended vocal sounds had been a material that I wanted to work with in composition more deeply, so coming in contact with this rich world of experimental vocal improvisation. I myself am not a vocal improviser and needed some source materials to compose with and decided to use the entirety of the daily sketches by Sage Pbbbt due to the liberal [Creative Commons](https://creativecommons.org/licenses/by-nc/3.0/) license. My plans were to deconstruct this large body of work and use the computer aid me in selecting and arranging it. This created a confluence of sonic and technological interests. 

### Building A Corpus Of Materials
I began by segmenting all of the recordings using REAPER's dynamic split tool and configuring it to segment based on the detection of transients. This choice was made because I wanted to extract individual utterances and phonetic-like sounds from composite gesture to be rearranged and re-concatenated. DEMO 1 shows an example of this , by taking a portion of ["One Breath Poem Failure"](https://sagemusick.bandcamp.com/track/2016-02-16-one-breath-poem-failure), segmenting it and then recombining the segments randomly. The colours of each segment track the displacement of each utterance.

<Sage
    file1="/ss/gesture.mp3"
    peaks1="/ss/gesture.dat"
    file2="/ss/shuffled_gesture.mp3"
    peaks2="/ss/shuffled_gesture.dat"
    segs1={sageUnshuffled}
    segs2={sageShuffled}
/>

### Selecting Segments
Experimenting with this random recombinative process produced results that sparked an interest in how I could recombine individual segments to synthesise new materials by recycling items from this corpus of vocal utterances. Thus, the consequential challenge of this was to build a system that would be responsible for organising samples in this way into *phrases* on a meso-scale. I did not necessarily have a strong idea about what would specifically constitute a phrase, but I knew that I wanted to explore recombining segments such that result would be less random than the initial experiments, and that phrases could be given strong perceptual qualities that would differentiate them from each other. With this idea in mind, I set out experimenting with a number of approaches in which individual segments would be concatenated into phrases.

A strategy I explored early on was to employ audio descriptors in order to analyse the amplitude, spectral centroid and spectral flatness measure of each sample. I anticipated that these three bits of data would allow me to differentiate each sound from each other by capturing their "brightness" (spectral centroid), "noisiness"  (spectral flatness) and intensity (amplitude). I could then use the computer to find samples that were relatively homogenous to each other and to draw them together into the construction of phrases. Using a Max patch, I was able to test this idea by querying a database of this descriptor information to return samples that fit a set of criteria and then concatenate them randomly within that group of samples. In [VIDEO 1]({m.ss}#vid1) I demonstrate this by querying for up to 50 samples from the corpus of segments based on three different constraints. Each slider corresponds to a different descriptor, so by changing the value in this interface I can modify the query dynamically. The only fixed part of this query is the comparators which are used to perform the filtering of the data. They are as follows:

1. Samples that have a spectral centroid **greater than** the threshold
2. Samples that have a spectral flatness measure **greater than** the threshold
3. Samples that have an amplitude **within 6dB** of the threshold

<VideoMedia id="vid1">
    <video slot="media" controls>
        <source src="/ss/descriptorspace.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/ss/descriptorspace.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 1: Querying the corpus of voice segments using audio descriptors.</p>
</VideoMedia>

Using the computer to query the corpus and find groups of samples matching a particular descriptor specification produced results that appealed to me aesthetically. In particular, queries that produced texturally focused groupings were appealing, because by concatenating those sounds together without much attention to that specific process it was possible to construct homogenous textures. Another aspect I did not intend to discover, but found appealing, was the way that a morphology can be imposed on the concatenative process by changing the thresholds of the query dynamically. This can be seen specifically at 0:20 in [VIDEO 1]({m.ss}#vid1).

This descriptor query process underpinned much of the compositional process to follow. From this point, I attempted to devise ways in which the computer could arrange material at a higher formal level by impose ordering of samples on homogenous groups of samples and by modifying descriptor queries dynamically. This is discussed in ["Constructing Form Computationally"]({m.ss}#constructing-form-computationally)

 ### Constructing Form Computationally
In this phase of the composition process I faced a number of challenges when trying to devise systems for controlling form based on audio descriptor querying and matching A number of strategies were explored in which I attempted to have the computer impose a level of control over what queries were made in order to dynamically create material groups on-the-fly.

One approach was to create predefined query "shapes" that structured a dynamic query over a given period of time. Using this, I could define the overall quality and percept of the gesture, without specifying any of the details and decisions such as which samples would form that gesture would be taken up by the matching algorithm. This type of approach is discussed in Harker (2012), in which he outlines how bespoke Max externals have been used toward similar ends in his compositional practice. VIDEO 2 demonstrates one version of this, where queries are changed according to a gesture that is defined by a number of points to be reached at specific times. This creates a parabolic trajectory that increases and decreases the amplitude and spectral centroid query values over a set amount of time. This produces a strong perceptual link between the query and the sound which as a compositional tool is a useful way of having the computer render complex perceptual arrangements based on a simpler language for communicating it.

<VideoMedia id="vid2">
    <video slot="media" controls>
        <source src="/ss/dynamic-gesture.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/ss/dynamic-gesture.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 2: Dynamic gestures created by dynamically querying a corpus of vocal segments based on audio descriptors.</p>
</VideoMedia>

VIDEO 3 shows another idea in which a static state of very low amplitude is intermittently "escaped" from, by updating the query for new samples in real-time and almost immediately returning to the low amplitude state. This creates a series of jumps between static states.

<VideoMedia id="vid3">
    <video slot="media" controls>
        <source src="/ss/static-jumps.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/ss/static-jumps.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 3: A self-contained static musical behaviour is formed by creating an "anchor" in the descriptor space and momentarily departing and then returning to it.</p>
</VideoMedia>

These are just two strategies I explored. Many are codified into the Max patches, or were left in half finished states and at various stages of development and left try try other things or were unsatisfactory. Ultimately with this approach, I embraced what Harker (2012, p. 7) terms a "perceptual equivalence". Individual samples are interchangeable in regards to the query and variability is constrained to the qualities of the shape rather than to specific choices about material. As such, this strategy for having the computer select samples and arranging them revolves around communicating a generalised fixed structure which is detailed and resolved computationally.

Another approach I tried involved creating a behaviour around the notion of "adversarial searching".  For this, I take two starting points within the descriptor space and attach those starting points to two different searches, one that attempts to minimise the volume of the output by changing the spectral centroid and one that tries to find the maximum centroid by minimising the volume. This adversarial process has no resolution but produces longer term morphologies and structures without having to specifically design from moment-to-moment. This kind of thinking carried over and was a precursor in many ways to my engagement with Simulated Annealing in [Annealing Strategies]({m.as}), which exploits a similar searching behaviour. to structure music automatically. The adversarial searching technique produced the some interesting meso-scale results, but produced a lot of directionless forms at the highest formal level. [AUDIO 2]({m.ss}#audio2) is one of the outputs of this process.

<Waveform 
title="AUDIO 2: Adversarial searching"
id="audio2"
file="/ss/5712433.mp3"
peaks="/ss/5712433.dat"
/>

Another approach was to create perceptually defined groups of samples through queries that I had found throughout experimentation as compositional units. Using these small collections, I then applied a selection algorithm to select samples based on Puckette (2015). This algorithm creates different patterns by specifying ratios between different outcomes.

<!-- WIDGET: z12 widget -->

In my mind, imposing this amount of rigour would result in a form that was perceivably ordered and might possess sophisticated qualities at the highest formal levels. Most of all though, the algorithm's behaviour was too homogenous in the sense that it *always* changed which given enough listening stops becoming change and instead becomes relatively static.

<!-- I have described a number of preliminary approaches for creating  on descriptor-based queries. An issue that ran throughout all of these approaches was that they introduced more decision making and relinquished control from me that was not necessarily offering any benefits in return.  -->

The purpose for these approaches was to enable the computer to generate high-level formal structures from a computational approach to selecting different sample-based materials over time.  Overall though, the results of these approaches lacked in high-levels directionality as well as any sense of *intentionality*  in what the computer produced. To move from these results toward something with a more sophisticated high-level organisation, I would have to go and edit them manually, increase the complexity of the algorithms or stack multiple algorithms on top of each other. In other words, these strategies while producing some aesthetically appealing outputs, were not enough to produce an entire piece, and my work at this time was not exploring any algorithms required for a computer to not only generate form at that level, but for it to be coherent and to satisfy my creative interests.

Furthermore, this way of approaching the automatic generation of formal structures was framed around recreating formal structures that I would find sophisticated, rather than trying to have something *emerge* from creating a set of algorithmic constraints. In hindsight, I do not think that the algorithms and models I was using were complex enough, nor did I have a good enough idea about what aspects of my formal thinking could or should be modelled at the meso- and macro- levels of form. 

Fundamentally, the way I was working at this time introduced an increasing amount of compositional decisions into the creative process, without addressing my initial aims of having the computer generate a piece at all formal levels. I found more interest in being offered small sections of material and I often wanted to go away and compose with these ideas but stopped myself to go back and explore other strategies, which I thought in combination would eventually lead to a system that could produce a piece itself. Ultimately, the frustration of not progressing towards a finalised piece, prompted a radical change in approach to how I composed from that point onwards. This change is discussed in the next section ["From Modelling to Assistance"]({m.ss}).

### From Modelling To Assistance
At this point in the project I engaged in a radical shift of technology and approach. One aspect that I wanted to continue using was descriptor-based querying to create small collections of material. What I wanted to change was the role of the computer in organising the outputs of those processes and to more sparingly rely on algorithmic approaches and to explore their arrangement into high-level forms through intuitive processes. Despite initially having strong desires to compose "in the computer", I realised that was incompatible with my need to be involved in the creative process more directly.

The first change I made was to switch creative coding environments to Python. There were a number of reasons for this and the shift was a response to the frustration of the project not developing from experiments into a piece, as well as the un-manageability of the work I had already done. In the initial experiments for this project I had accrued numerous Max patches with half-finished ideas, some in a functional state and others broken. This state of disrepair in the programming aspect made it hard to iterate on existing work, or to draw together disparate strategies into new approaches. I felt like I needed a fresh start. Furthermore, as I experimented more and more I became aware of an underlying incompatibility with how I wanted to express and interact with the outputs of the computer. Much of what I was doing revolved around producing iterations or variations of processes and auditioning them. Max is not an environment that is well suited to such procedures, particularly if one is interested in off-line processing in order to produce many iterations rapidly. The language design of Max is also made with real-time processing in mind. As such, it lacks programming constructs such as the commonly found "for" loop, a programming pattern common across many text-based languages where one can iterate over a repeated procedure or function a given number of times or in response to some data. As such, I often found resistance in the expression of my ideas into a computational form. 

By using Python, the composition process accelerated, and I quickly found good result by combining descriptor query matching to select samples, and pattern-based processes to arrange those matched samples into short musical phrases. As such, this created a two stage workflow where I would first specify a query to create a small sample selection, which would then have an organisational procedure imposed on to it. These two stages were separate and independent from each other, creating a lot of room for exploration by using different descriptor queries with a limited number of organisational principles. Each time this process was executed, several iterations were, allowing me to audition and aurally explore several variations as a group. I found this to be a particularly stimulating creative act, as once I had finished Python scripting, I could fully step into the mindset required to listen to materials and imagine their compositional usefulness. 

#### Organisation Processes
The `accum_phrase` process concatenates samples until a maximum total duration has been satisfied. While this is perhaps the most basic one, it produced results that were largely promising as a first attempt. The output of the process also rendered each stage of the accumulation so the results could be navigated as a phrase of increasing complexity and duration. This can be heard in [AUDIO 3]({m.ss}#audio3).

<!-- REIVEW: This prompted me to think about how I might use the shortest samples as tiny building blocks, or the longest phrase in its original form -->

<Waveform 
title="AUDIO 3"
file="/ss/a12.mp3"
peaks="/ss/a12.dat"
id="audio3"
/>

Another process, `jank` creates phrases where samples are repeated in chunks. The result is a stutter effect that can produce robotic, artifical qualities as well as tones and pitches as a side-effect. An example of this can be heard in [AUDIO 4]({m.ss}#audio4) and [AUDIO 5]({m.ss}#audio5).

<Waveform 
title="AUDIO 4"
file="/ss/g17.mp3"
peaks="/ss/g17.dat"
id="audio3"
/>

<Waveform
title="AUDIO 5"
file="/ss/h0.mp3"
peaks="/ss/h0.dat"
id="audio4"
/>

`search_small` creates its own random descriptor query within a narrow search range for the amplitude, spectral flatness measure and spectral centroid. This process produces perceptually tight organisations of samples without me having to specify those constraints exactly. I produced many iterations with this and selected ones that I found to be the most aesthetically pleasing. Working with this process allowed me to blindly discover descriptor queries that I could then use elsewhere. An example can be heard in [AUDIO 6]({m.ss}#audio6).

<Waveform 
title="AUDIO 6"
file="/ss/e22.mp3"
peaks="/ss/e22.dat"
id="audio6"
/>

#### Traces Of Unaltered Outputs In Stitch/Strata
For me, this methodology of generating sample phrases, auditioning them and retaining the ones that I anticipated would be useful for composing with was a more fluid compositional workflow than having to step back into the mindset of an algorithm designer as I had been before. The final work for Stitch/Strata still retains traces of the raw phrases, although many were additionally processed with playback rate modifications and rearranged or re-segmented manually. This section describes how some of them were incorporated into the final composition.

<!-- LINK: link to the main piece timecode -->
`search_small` in one iteration randomly selected samples with high spectral flatness, high centroid and low amplitude. This created a whispering texture which was used amongst several superimposed textural phrases at 3:03. The phrase itself can be heard in [AUDIO 7]({m.ss}#audio7).

<Waveform 
title="AUDIO 7"
file="/ss/whispering-texture.mp3"
peaks="/ss/whispering-texture.dat"
id="audio7"
/>

`jank` created a number of phrases that were glitchy and robotic in character. I composed with these more sparingly and wove them amongst other phrases, at times allowing it to exhibit for longer periods of time. This can be heard in [AUDIO 8]({m.ss}#audio8) which relates to 4:38 in the piece.

<Waveform
title="AUDIO 8"
file="/ss/g.mp3"
peaks="/ss/g.dat"
id="audio8"
/>

In addition to this, the outputs of `jank` were used to create dynamically constrained and tense textures that were prolonged. These became formal reprieves throughout the latter half of the piece and can be heard at 4:53 for example. [AUDIO 9]({m.ss}#audio9) shows this in isolation. 

<Waveform 
title="AUDIO 9"
file="/ss/h.mp3"
peaks="/ss/h.dat"
id="audio9"
/>

`search_small` phrases were interspersed with other materials and heavily edited from their original form to be presented more intermittently in the overall texture. [AUDIO 10]({m.ss}#audio10) captures this, which can be heard at 4:47 in the piece.
<Waveform 
title="AUDIO 10"
file="/ss/e.mp3"
peaks="/ss/e.dat"
id="audio10"
/>

Outputs from `accum_phrase` were combined together to create unrelenting concatenations of vocal utterances. The repetition of short repeated phrases amongst the larger phrase itself can be heard. An example of this is in [AUDIO 11]({m.ss}#audio11). This was used as a long gesture that underpins the middle section of the piece at 3:15.
<Waveform 
title="AUDIO 11"
file="/ss/a+b.mp3"
peaks="/ss/a+b.dat"
id="audio11"
/>

Even given the radical paradigm shift towards Python, I still used the dynamic descriptor query approach, described in ["Selecting Segments"]({m.ss}#selecting-segments). This became a pivotal method for generating material that I then manually composed with and produced some contrasting gestural material compared to the more static and iterated nature of the Python scripts output. Outputs from this process can be heard in multiple places across the piece.

The first section of the piece is entirely composed from long dynamic queries spanning over up to a minute. This created slowly evolving gestures where the change in query increases the spectral bandwidth and diversity of samples selected. This is demonstrated in [AUDIO 12]({m.ss}#audio12).

<!-- WIDGET: it might be good to make these a component where one can flick between the full piece and the segments. -->

<Waveform
title='AUDIO 12'
file='/ss/ramping-1.mp3'
peaks='/ss/ramping-1.dat'
id='audio12'
/>

At 2:27, a dynamic query is used to create a rapidly changing texture encompassing highly noisy materials that reduce in amplitude over time. Refer to [AUDIO 13]({m.ss}#audio13).

<Waveform
title='AUDIO 13'
file='/ss/ramping-2.mp3'
peaks='/ss/ramping-2.dat'
id='audio13'
/>

A less linearly shaped query creates an oscillating gesture at 3:15. This is used to punctuate the beginning of a new section. Refer to [AUDIO 14]({m.ss}#audio14).

<Waveform
title='AUDIO 14'
file='/ss/ramping-3.mp3'
peaks='/ss/ramping-3.dat'
id='audio14'
/>

## Reflecting On Human And Computer Agency In Stitch/Strata

The compositional process that took Stitch/Strata to its final version was entirely different to what I imagined it would be at the start of the composition process. The challenges incited by the initial prompt, to create a piece "in the computer", and the responses and solutions to those challenges made me reflect on my initial aims and questions for this piece as well as for the PhD research more widely. I was certain from the experience of writing this piece that working entirely in the computer and relinquishing creative control to a high degree, particularly in the formal aspects of a work, is incompatible with my artistic goals and aesthetic preferences. However, this difficult process was formative and self-realisation. In many ways it outlined what aspects of composition I am willing to give away to the computer and at what stages I need to intervene with those processes to both feel like using the computer is a meaningful choice artistically, and that I am being satisfied in the process itself. I  arrived at a workflow where the balance of agency between myself and the computer was situated more evenly and elicited a creative dialogue that allowed me to infuse the computer's generated outputs with my own intentions and processes imposed on to that material. 

A main point of "dialogue" between myself and the computer that emerged was the process of auditioning outputs from the computer. This fostered an awareness of the samples within the wider corpus of previously unknown materials. While I had some idea broad idea about the nature of the sounds that I was working with, once the original vocal samples had been segmented there was no way that I could have listened to the approximately 3500 segments and operated on them meaningfully from that point without aid from the computer. The computer provided a structured exploration of those samples, grounded in the notion of filtering the larger corpus based on audio descriptors. This approach led me to thinking about groups of samples as blocks of material that could have highly defined sonic properties, or as dynamic gestures shaped by their perceptual qualities. In the final version of Stitch/Strata this computationally "discovered" way of orientating around the materials can be heard in the form that I built. 0:00 to 2:32 dynamic queries create the formal direction and structure. From 2:32 to 3:56, various perceptually tight sample groups derived through descriptor constraints are superimposed, with the superimpositions guided by intuition and experimentation with combinations. 3:56 onwards plays on the notion of intuitive manual segmentation and rearrangement of materials from the original "phrase" materials. The result could not have been made without the computer offering initial materials for me to deconstruct to varying degrees.

This hybrid of computer generation and intuitive manipulation was a more fruitful workflow than always having to step back into a technical or "system designer" role in which my agency is expressed through an additional layer of abstraction or code. Working only from a point of designing a system, does not engage me in that way and forms a relationship between myself and the computer that is totally concerned with "systems thinking", where tuning one component in a complex network of other components can contribute to the outcome. For me, conceiving of a piece as a system is difficult, as I find it hard to imagine a complex system that can encapsulate the musical ideas that I want to express. This way of working is also to an extent predicated on having a clear vision from the outset of the creative process. For me this is never the case, and I always start from a *tabula rasa* that is built on by experiencing and experimenting with materials. As such, I need to formulate what the piece is both artistically and conceptually through creative action, and intermittently relying on the computer to guide that process.

### State And The Digital Audio Workstation
The transition from attempting to model specific musical structures, toward building smaller ones and working with them intuitively shifted my workflow from Max to a combination of REAPER and Python. Instead of focusing on having the computer formalise most if not all aspects of a work in a single "execution", I moved toward a strategy in which the computer presented audio outputs as options or suggestions to me, which I could then modify, process, and change in place. 

One significant difference engendered by this shift in workflow was the way I interacted with the computer's outputs and reflected on them to develop the piece. Max does not inherently store or memorise anything to do with the state of a patch such as changes to parameters or sounds that are generated while it runs. Those capabilities have to be built and implemented on a case-by-case basis which can be technically complicated, especially as the patch itself changes in response to creative developments or as new bits of code are introduced. This lack of "state" in my Max programming was something that I found difficult or impossible to address at the same time as thinking about creative problems. The movement to Python and REAPER helped to alleviate some of these issues, and it was only in hindsight that I realised how important intermediate state is to my creative workflow.
 
In particular, the ability to store, delete, colour-code, label, annotate and position audio files on a timeline fits with my need to keep ideas in states of temporary completeness. By taking the outputs of the Python processes and situating them in a REAPER session, they become part of a "sketchbook" that can iteratively be modified, added to and grows alongside the programming aspects of my work. Furthermore, this sketchbook approach allows for flexibility in the fixity of the work as it develops. If I am not sure of the compositional usefulness of a computer generated output it can be put aside temporarily, without having to discard it entirely. Ideas which I find particularly compelling are as easily retained. It also allows me to move between different creative mindsets fluidly, whether those mindsets are focused on sound-design, low-level manipulation of samples, or zooming out to observe the formal aspects of a work. The DAW in this sense becomes scaffolding that helps me to draw the computer into my practice when it is needed, rather than relying entirely on a system that I need to design from the ground up.


## Concluding Remarks

Stitch/Strata began with lofty compositional and creative aims that were challenged and eventually dismantled through the creative process itself. At the time, these changes to my creative practice and explorations of new ideas were both exciting as much as they were difficult to embrace. Reflecting on them now though, there were many aspects of the workflow that emerged that became fundamental in my approach to composing [Reconstruction Error]({m.re}) and [{m.emname}]({m.em}) as well as the development of [FTIS]({m.ftis}) which heavily leans on REAPER as an environment to explore computer generated outputs aurally and incorporate the contributions of the computer into my computer-aided practice.

The next project to be discussed is [Annealing Strategies]({m.as}). This work was perhaps subconsciously another attempt to see how I could relinquish control to the computer, albeit with a very different approach to the generation of sound and a deeper level of integration between the compositional material and the technical approach. In a number of ways this piece was more successful, but the workflow that I built up through the creative process confirmed for me that relinquishing a lot of my control and ability to intervene with the behaviour of the computer is not compatible with how I want to compose music.

<NextSection 
next="Annealing Strategies"
link={m.as}
/>

<script>
    import Waveform from '$lib/components/Waveform.svelte';
    import Z12 from '$lib/demos/Z12.svelte';
    import YouTube from '$lib/components/YouTube.svelte';
    import VideoMedia from "$lib/components/VideoMedia.svelte";
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
	import NextSection from "$lib/components/NextSection.svelte";
    import {metadata as m} from "../directory.svx";
    import Sage from "$lib/demos/Sage.svelte";
    import { sageUnshuffled, sageShuffled } from "$lib/components/data/sage.js";

</script>

# Stitch/Strata

<Waveform 
title="Stitch Strata (Stereo Mixdown)"
file="/pieces/ss.mp3"
peaks="/pieces/ss.dat"
id="audio1"
/>

The first performance of this piece was at the [2019 Electric Spring Festival](http://electricspring.co.uk/electric-spring-2019/) using the [Huddersfield Immersive Sounds System (HISS)](http://electricspring.co.uk/electric-spring-2019/). The recording of the piece that is submitted in the portfolio exists in two versions. A binaural version that attempts to capture the spatialisation from the first performance and a stereo version.

## Motivations and Influences

At the beginning of this PhD, I focused on the question of *"how can the computer be creative in composition?"*. This question was indicative of the curiosities spawned from a previous project, [Biomimicry]({m.biom}), in which the computer responded to an instrumental performer and was seemingly creative in the way that it created sophisticated formal events in real-time.  I attribute many successful aspects of Biomimicry in this regard, to the contributions from the improvisers and the mechanisms by which they were able to influence the machine. Because the computer only responded to what the performer did, the improviser drove formal events and the computer was only able to *react* to the changes and structuring of time imposed by the human counterpart. Reflecting on this dependency on having a "human in the loop", my supervisor Dr. Alex Harker encouraged me to write a piece entirely "in the computer" and without any improvisers or instrumental component. The aim of this challenge was to remove the in-the-moment influence of an intelligent and musically capable human and to encourage me to find computational strategies for creating sophisticated formal structures through generative and computer-originated processes.

As such, a major motivation for this project was to investigate how the computer could structure all formal levels of a work, from the selection of individual compositional units (in this case audio samples), their arrangement into gestures, their arrangement into sequences of gestures and so on and so forth until the work is additively built from a set of atomic units by a governing algorithm. A goal of this compositional workflow, would be to take my influence away from arranging materials at different levels of detail myself, to inventing procedures that the computer would then use to realise a work automatically and to achieve a sense of coherency and sophistication in the output. In this regard, a number of other practitioners were influencing me and driving this type of thinking. These are described in the next section, ["Other Practices"]({m.ss}#other-practices).

### Other Practices
Michael Young's "Neural Network Music" (NN Music) was particularly prominent in my set of influences. This piece, is a real-time improvisational work for an unspecified instrumental improviser and the computer. It is based on the premise of the computer as an improvisational partner that moves between a number of behavioural states to accompany and make music *with* the performer. It decides which state to occupy by creating and performing a classification of the instrumentalist's playing and associating this detected state to an internal one. The caveat of this is that the computer begins with a limited classification model that it then builds up over time in the course of the piece. The neural network that underpins how the computer responds to the performer is constantly retraining on the fly and developing a mapping between a set of actions it has stored in memory and the performance in the moment.

While Young's piece relies heavily on the collaboration and musical sensitivity of the improviser, I found the technological approach of this work inspiring for two reasons. Firstly, much of the composing is done without necessarily making decisions about specific and detailed musical structures. Instead, Young creates a generalised model for what he deems to be important musically, and this model  open ended and flexible to an extent. Secondly, the computer seemingly *forms its own connections* and uses those to make decisions about the structure of its responses to the performer. Those connections are not explicitly pre-composed or made ahead of time and instead are formed in the moment dynamically.

At the time of composing [Stitch/Strata] I was also reading articles by [Adam Linson](http://percent-s.com/) and interested in his improvisational and creative coding practice. In particular, I found the way that he incorporates the computer into practice as an autonomous creative agent something that I could learn from and adapt to my own needs. Linson et. al (2015), for example, describes the design of "Odessa", a real-time improvisation system that uses a "subsumption architecture" to construct its behaviour. The architecture is described:

<div class="bigquote">

*In Subsumption terminology, the “competing behaviors” of a system are organized into a network of interactive “layers” (Brooks 1999). Following this idea, and further informed by psychological research, Odessa’s behavioral layers consist of its basic agency to spontaneously produce output ("Play"), its ability to respond to musical input ("Adapt"), and its ability to resist by disregarding musical input, introducing silence, and initiating endings ("Diverge").* (Linson et. al., 2015, p. 4)

</div>

IMAGE 1 depicts how at a programmatic level the architecture is designed as a set of behavioural layers that contain modueles for sound production and interaction. These are sensitive to the input of a live performer. By connecting these modules to each other across subsumption architecture, Linson et. al (2015, p. 13). hypothesised that it "would serve to produce cues that suggest intentionality". For me, the notion of intentionality was also something that I wanted to be able to build into my work. What I often interpret as "sophistication" in computational and computer generated outputs is a sense of intentionality exhibited by the computer in the decisions that it makes. The rationale for these decisions does not have to be concrete or explainable necessarily, but it has to seem as if there is one underpinning the decision making process in a similar fashion to the way I would manually compose.

<ImageMedia2 
url="/ss/odessa.jpg"
caption='Odessa subsumption architecture taken from Linson et. al (2015). The boxes represent modules belonging to one of the three behavioural layers, "Play", "Diverge", "Adapt".'
figure="IMAGE 1"
/>

The subsumption architecture by Linson et. al. was also interesting to me as a method for simplifying the problem of having the computer deal with time and form on multiple levels. I envisaged that with a subsumption architecture, I could construct layers as computational behaviours and then devise various mechanisms for how those layers would interact with each other. Similar to my appreciation for Young's work, the computer would not have to be specifically programmed to deal with the minutia of musical decision making, rather, I could build a system that embodied a number of strategies for dealing with decision making, and the musical details would resolve themselves computationally. 

While these influences and potential strategies in this early stages of research and conceptual exploration felt promising, the generalisations I made about the application of technology led to many failed attempts to compose a piece with the initial goals in mind. This created a frustrating compositional process, so much that by the end of composing Stitch/Strata I had moved far away from the initial territory I aimed to explore of working "in the computer". In doing so relied much more on manual intuitive compositional decision making by me to finish the piece in a "hybrid" computer-aided workflow situated in the Digital Audio Workstation. The evolution to this end-point and the compositional process is described next in ["Compositional Process"]({m.ss}#compositional-process).

## Compositional Process
In this section I will describe the journey of the compositional process of Stitch/Strata from the earliest experiments to the final result. Along the way I will detail some additional musical influences and reflect on the trajectory of this particular piece and project.

### The Influence of Experimental Vocal Improvisation
Alongside the compositional influences described in [Motivations and Influences]({m.ss}#motivations-and-influences), listening had a significant impact on the type of sound materials that I gathered to compose with. In this aspect of the piece, [Sage Pbbbt](https://sagepbbbt.com), whose enormous catalogue of vocal improvisation "daily sketches", became a trove of sonic materials that I would later draw on and use as audio samples. These improvisations revealed to me the richness of extended vocal techniques as well as the diversity of morphologies and textures that can be produced from just the voice. In connection with Pbbbt's practice, I came into contact with the discography of [Phil Minton](https://www.philminton.co.uk) as well as his collaborative efforts with [Audrey Chen](http://www.audreychen.com) in *By The Stream*. 

<VideoMedia>
    <iframe slot="media" style="border: 0; width: 350px; height: 470px;" src="https://bandcamp.com/EmbeddedPlayer/album=1767024803/size=large/bgcol=ffffff/linkcol=0687f5/tracklist=false/transparent=true/" seamless><a href="https://subrosalabel.bandcamp.com/album/by-the-stream">By the Stream by Phil Minton + Audrey Chen</a></iframe>
    <p slot="caption"></p>
</VideoMedia>

One particular aspect of this collaboration was the way that I interpreted the construction of long form textures as assemblages of intricate vocal utterances. To me, the arrangement of sounds in this way causes the individual sounds to transcend from their original source and allude to a more abstract sound world. This can be heard in the *as* from 0:56 to 1:35 and 2:38 to 3:10 in *all*.

In addition to this, I came across [Trevor Wishart's](http://www.trevorwishart.co.uk) piece [*Tongues of Fire*](https://www.youtube.com/watch?v=Ude4717dlsQ).This piece also uses vocal sounds as a source material and introduces signal processing in order to manipulate sounds into new forms. I took particular interest in the way that the Wishart would sit on the fence between organic and synthetic presentations of the source material, facilitated by rapid concatenation and processing. 13:06 to 15:24 in particular emphasises this in [VIDEO 1]({m.ss}#video1)

<YouTube
url="https://www.youtube.com/embed/Ude4717dlsQ?start=786"
title="Tongues of Fire by Trevor Wishart"
figure="VIDEO 1"
id="video1"
/>

Extended vocal sounds at this point in my creative practice were a material that I wanted to work with in composition more deeply. I myself am not a vocal improviser and needed some source materials to compose with and decided to use the entirety of the daily sketches by Pbbbt as a starting point due to the liberal [Creative Commons](https://creativecommons.org/licenses/by-nc/3.0/) license. Many of the individual sounds that I found appealing from other practices were made available from this, as there is such a massive scope of material, techniques and approaches by Pbbbt  in this corpus of recordings. My plans from this point were to deconstruct the corpus through segmentation, and have computer use those segments as its compositional material to operate on generatively and algorithmically. 

### Building A Corpus Of Materials
I began by segmenting all of the recordings using REAPER's dynamic split tool and configuring it to segment based on the detection of transients. This choice was made because I wanted to extract individual utterances and phonetic-like sounds from the already structured material to be rearranged and concatenated. DEMO 1 shows an example of this process, taking a portion of ["One Breath Poem Failure"](https://sagemusick.bandcamp.com/track/2016-02-16-one-breath-poem-failure), segmenting it and then recombining the segments randomly. The colours of each segment track the displacement of each utterance between the segmentation and reordering stages.

<Sage
    file1="/ss/gesture.mp3"
    peaks1="/ss/gesture.dat"
    file2="/ss/shuffled_gesture.mp3"
    peaks2="/ss/shuffled_gesture.dat"
    segs1={sageUnshuffled}
    segs2={sageShuffled}
/>

Experimenting with this random recombinative process produced results that sparked an interest in how I could recombine individual segments to synthesise new materials by recycling items from this corpus of vocal utterances. Thus, the consequential challenge of this was to build a system that would be responsible for organising samples in this way into *phrases* on a meso-scale. I did not necessarily have a strong idea about what would specifically constitute a phrase, but I knew that I wanted to explore recombining segments such that result would be less random than the initial experiments, and that phrases could be given strong perceptual qualities that would differentiate them from each other. With this idea in mind, I set out experimenting with a number of approaches in which individual segments would be concatenated into phrases.

A strategy I explored early on was to employ audio descriptors in order to analyse the amplitude, spectral centroid and spectral flatness measure of each sample. I anticipated that for a generative strategy predicated on some selection criteria, three bits of data would allow the computer to differentiate each sound from each other by capturing their "brightness" (spectral centroid), "noisiness"  (spectral flatness) and intensity (amplitude). I could then prompt the computer to find samples that were relatively homogenous to each other and to draw them together into phrases or compositional units. Using a Max patch, I was able to test this idea by querying a database of this descriptor information to return samples that fit a set of criteria and then concatenate them randomly within that group of samples. In [VIDEO 2]({m.ss}#vid2) I demonstrate this by querying for up to 50 samples from the corpus of segments based on three different constraints. Each slider corresponds to a different descriptor, so by changing the value in this interface I can modify the query. The only fixed part of this query is the comparators which are used to perform the filtering of the data. They are as follows:

1. Samples that have a spectral centroid **greater than** the threshold
2. Samples that have a spectral flatness measure **greater than** the threshold
3. Samples that have an amplitude **within 6dB** of the threshold

<VideoMedia id="vid2">
    <video slot="media" controls>
        <source src="/ss/descriptorspace.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/ss/descriptorspace.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 2: Querying the corpus of voice segments using audio descriptors.</p>
</VideoMedia>

Using the computer to query the corpus and find groups of samples matching a particular descriptor specification produced results that appealed to me aesthetically. In particular, queries that produced texturally focused groupings were appealing, especially when subjected to the process of concatenating them in rapid succession. This produced static, textural materials with interesting and detailed morphologies, without having to make specific decisions regarding the duration or timing of individual samples. Another aspect I did not intend to discover, but found appealing, was the way that a morphology could be created and imposed on to the concatenation process by changing the thresholds of the query dynamically. This can be seen specifically at 0:20 in [VIDEO 2]({m.ss}#vid2).

This descriptor query process underpinned much of the compositional process to follow. From this point, I attempted to devise ways in which the computer could arrange material at a higher formal level by imposing ordering and patterns on to groups of samples and by modifying descriptor queries dynamically. This is discussed in ["Constructing Formal Change Computationally"]({m.ss}#constructing-formal-change-computationally)

### Constructing Formal Change Computationally
In this phase of the composition process I faced a number of challenges when trying to devise systems for controlling form based on audio descriptor querying and matching A number of strategies were explored in which I attempted to have the computer impose a level of control over what queries were made in order to dynamically create material groups on-the-fly.

One approach was to create predefined query "shapes" that structured a dynamic query over a given period of time. Using this, I could define the overall quality and percept of the gesture, without specifying any of the details and decisions such as which samples would form that gesture would be taken up by the matching algorithm. A similar approach is discussed in Harker (2012), in which he outlines how bespoke Max externals have been used toward similar ends in his compositional practice. [VIDEO 3]({m.ss}#vid3) demonstrates one version of this, where queries are changed according to a gesture that is defined by a number of points to be reached at specific times. This creates a parabolic trajectory that increases and decreases the amplitude and spectral centroid query values over a set amount of time. This produces a strong perceptual link between the query and the sound which as a compositional tool is a useful way of having the computer render complex morphologies through a set of constraints.

<VideoMedia id="vid3">
    <video slot="media" controls>
        <source src="/ss/dynamic-gesture.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/ss/dynamic-gesture.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 3: Dynamic gestures created by dynamically querying a corpus of vocal segments based on audio descriptors.</p>
</VideoMedia>

VIDEO 4 shows another strategy in which a static state of very low amplitude is intermittently "escaped" from, by updating the query for new samples in real-time and almost immediately returning to the low amplitude state. This creates a series of jumps between static states.

<VideoMedia id="vid4">
    <video slot="media" controls>
        <source src="/ss/static-jumps.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/ss/static-jumps.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 4: A self-contained static musical behaviour is formed by creating an "anchor" in the descriptor space and momentarily departing and then returning to it.</p>
</VideoMedia>

<!-- [VIDEO 3]({m.ss}#vid3) and [VIDEO 4]({m.ss}#vid4)  are just two strategies I explored. Many were trialled in Max patches, or were left in half finished states and at various stages of development due to my dissatisfaction with them or were discarded in order to attempt other methods. Ultimately with this approach, I embraced what Harker (2012, p. 7) terms a "perceptual equivalence". Individual samples are interchangeable in regards to the query and variability is constrained to the qualities of the shape rather than to specific choices about material. As such, this strategy for having the computer select samples and arranging them revolves around communicating a generalised fixed structure which is detailed and resolved computationally. -->

Another approach involved designing a behaviour on the metaphor of "adversarial searching".  For this, I took two starting points within the descriptor space and attached those starting points to two simultaneous "searches", one that attempts to minimise the volume of the output by changing the spectral centroid and one that tries to find the maximum centroid by minimising the volume. This adversarial process has no end, but produces longer term morphologies and structures without having to specifically design from moment-to-moment. This kind of thinking carried over and was a precursor in many ways to my engagement with Simulated Annealing in [Annealing Strategies]({m.as}), which exploits a similar searching behaviour to structure music automatically. The adversarial searching technique had the potential to create some sophisticated meso-scale forms. However it mostly generated dull and uninspired macro-scale structures . [AUDIO 2]({m.ss}#audio2) presents an output of this process.

<Waveform 
title="AUDIO 2: Adversarial searching"
id="audio2"
file="/ss/5712433.mp3"
peaks="/ss/5712433.dat"
/>

The last strategy I tried and have record of is taking perceptually defined groups of samples from queries and trying to apply a more rigorous selection procedure. This selection procedure was an implementation of the "z12" algorithm described in Puckette (2015) which can generate different patterns by specifying probabilities between different outcomes. DEMO 2 shows an example of the patterns this process can generate.

<!-- WIDGET: z12 widget -->

<Z12 />

I imagined that imposing this type of rigourous statistical process over an already perceptually linked group of samples would result in structures that could be formally sophisticated. This was not the case though, the algorithm's behaviour was too homogenous in the sense that it *always* changed, which given enough listening stops becoming change and instead becomes relatively static.

I have just described a number of approaches that were explored initially in Stitch/Strata. The purpose for exploring these approaches was to enable the computer to generate high-level formal structures from a computational and descriptor-driven approach to selecting different sample-based materials over time.  Overall though, these strategies did not produce sophisticated high-level forms, something that I was deeply concerned with from the outset of the project. While some aspects of these strategies produced some aesthetically appealing outputs, they were not enough to produce an entire piece, and my work at this time was not exploring the right type of algorithms required for a computer to not only generate form at a high-level, but for it to be coherent and satisfying toward my aesthetic preferences.

Furthermore, this way of approaching the automatic generation of formal structures was framed around recreating formal structures that I would find sophisticated, rather than trying to have something *emerge* from creating a set of algorithmic or generative constraints. In hindsight, I do not think that the algorithms and models I was using were complex enough, nor did I have a good enough idea about what aspects of my formal thinking could or should be modelled at the meso- and macro- levels of form.

Fundamentally, the way I was working at this time introduced an increasing amount of compositional decisions into the creative process, without addressing my initial aims of having the computer generate a piece at all formal levels. I found more interest in being offered small sections of material and I often wanted to go away and compose with these ideas but stopped myself to go back and explore other strategies, which I thought in combination would eventually lead to a system that could produce a piece itself. Ultimately, the frustration of not progressing towards a finalised piece, prompted a radical change in approach to how I composed from that point onwards. This change is discussed in the next section ["From Modelling to Assistance"]({m.ss}#from-modelling-to-assistance).

### From Modelling To Assistance
At this point in the project I engaged in a radical shift of technology and approach. One aspect that I wanted to continue using was descriptor-based querying to create small collections of material. What I wanted to change was the role of the computer in organising the outputs of those processes and to more sparingly rely on algorithmic and generative approaches and to explore their arrangement into high-level forms through intuitive processes. Despite initially having strong desires to compose "in the computer", I realised that was incompatible with my need to be involved in the creative process more directly.

A significant change that also emerged from the frustrating circumstances at this point was to the creative coding environments I was using. I began exploring by recreating ideas previously made in Max in Python, and testing how I could implement and express procedures such as audio descriptor matching and short concatenations of samples. There were a number of reasons for this and the shift was a response to the frustration of the project not developing from experiments into a piece, as well as the un-manageability of the work I had already done. In the initial experiments for this project I had accrued numerous Max patches, some in a functional state and others broken. This state of disrepair in the programming aspect made it hard to iterate on existing work, or to synthesise different approaches into hybridised new ones. I felt like I needed a fresh start. Furthermore, as I experimented more and more I became aware of an underlying incompatibility in how I wanted to express and interact with the outputs of the computer. Much of what I was doing revolved around producing iterations or variations of processes and auditioning them. Max is not an environment that is well suited to such procedures, particularly if one is interested in off-line processing in order to produce many iterations rapidly. The language design of Max is also made with real-time processing in mind. As such, it lacks programming constructs such as the commonly found "for" loop, a programming pattern common across many text-based languages where one can iterate over a repeated procedure or function a given number of times or in response to some data. As such, I often found resistance in the expression of my ideas into a computational form. 

By using Python, the composition process accelerated, and I quickly found good results by combining descriptor query matching to select samples, and pattern-based processes to arrange those matched samples into short musical phrases. This workflow was based around interacting with the command line, a text-based interface where one enters input data that the program consumes and acts on. As such, I would first specify a descriptor query such as `amp <-> 3 -16 centroid > 3000 sfm < 0.1` (amplitude within 3db of -16 and centroid greater than 3000Hz and spectral flatness measure less than 0.1) followed by a selection of a type of organisational procedure (these are explained shortly in [Organisational Procedures]({m.ss}#organisation-processes)). These two stages were separate and independent from each other, creating room for experimentation by combining different descriptor queries with a small number of organisational principles. Each time this process was executed, several iterations were made, allowing me to audition and aurally explore several variations as a collection. I found this to be a particularly stimulating creative act, as once I had finished Python scripting, I could fully step into the mindset required to listen to materials and imagine their compositional usefulness. 

#### Organisation Processes
Using Python made it possible and easier for me to express a number of organisational procedures that would operate on collections of descriptor-based selections from the wider corpus. These procedures are described next.

The `accum_phrase` process concatenates samples until a maximum total duration has been satisfied. While this is perhaps the most basic one, it produced results that were largely promising as a first attempt. The output of the process also rendered each stage of the accumulation so the results could be navigated as a phrase of increasing complexity and duration. This can be heard in [AUDIO 3]({m.ss}#audio3).

<Waveform 
title="AUDIO 3"
file="/ss/a12.mp3"
peaks="/ss/a12.dat"
id="audio3"
/>

`jank` creates phrases where samples are repeated in chunks of a given length and variation to that length. The result is a stutter effect that can produce robotic, artificial qualities as well as tones and pitches as a side-effect. An example of this can be heard in [AUDIO 4]({m.ss}#audio4) and [AUDIO 5]({m.ss}#audio5).

<Waveform 
title="AUDIO 4"
file="/ss/g17.mp3"
peaks="/ss/g17.dat"
id="audio3"
/>

<Waveform
title="AUDIO 5"
file="/ss/h0.mp3"
peaks="/ss/h0.dat"
id="audio4"
/>

`search_small` creates its own random descriptor query within a narrow search range for the amplitude, spectral flatness measure and spectral centroid. This process produces perceptually tight organisations of samples without me having to specify those constraints exactly. I produced many iterations with this and selected ones that I found to be the most aesthetically pleasing. Working with this process allowed me to blindly discover descriptor queries that I could then use elsewhere. An example can be heard in [AUDIO 6]({m.ss}#audio6).

<Waveform 
title="AUDIO 6"
file="/ss/e22.mp3"
peaks="/ss/e22.dat"
id="audio6"
/>

#### Traces Of Unaltered Outputs In Stitch/Strata
For me, this methodology of generating sample phrases, auditioning them and retaining the ones that I anticipated would be useful for composing with was a more fluid compositional workflow than having to step back into the mindset of an algorithm designer as I had been before in Max. The final work for Stitch/Strata still retains traces of the raw phrases, although many were additionally processed with playback rate modifications and rearranged or re-segmented manually. This section describes how some of them were incorporated into the final composition.

<!-- LINK: link to the main piece timecode -->
`search_small` in one iteration randomly selected samples with high spectral flatness, high centroid and low amplitude. This created a whispering texture which was used amongst several superimposed textural phrases at 3:03. The phrase itself can be heard in [AUDIO 7]({m.ss}#audio7).

<Waveform 
title="AUDIO 7"
file="/ss/whispering-texture.mp3"
peaks="/ss/whispering-texture.dat"
id="audio7"
/>

`jank` created a number of phrases that were glitch-like and robotic in character. I composed with these more sparingly and wove them amongst other phrases, at times allowing it to exhibit for longer periods of time. This can be heard in [AUDIO 8]({m.ss}#audio8) which relates to 4:38 in the piece.

<Waveform
title="AUDIO 8"
file="/ss/g.mp3"
peaks="/ss/g.dat"
id="audio8"
/>

In addition to this, the outputs of `jank` were used to create dynamically constrained and tense textures that were prolonged. These became formal reprieves throughout the latter half of the piece and can be heard at 4:53 for example. [AUDIO 9]({m.ss}#audio9) shows this in isolation. 

<Waveform 
title="AUDIO 9"
file="/ss/h.mp3"
peaks="/ss/h.dat"
id="audio9"
/>

`search_small` phrases were interspersed with other materials and heavily edited from their original form to be presented more intermittently in the overall texture. [AUDIO 10]({m.ss}#audio10) captures this, which can be heard at 4:47 in the piece.
<Waveform 
title="AUDIO 10"
file="/ss/e.mp3"
peaks="/ss/e.dat"
id="audio10"
/>

Outputs from `accum_phrase` were combined together to create unrelenting concatenations of vocal utterances. The repetition of short repeated phrases amongst the larger phrase itself can be heard. An example of this is in [AUDIO 11]({m.ss}#audio11). This was used as a long gesture that underpins the middle section of the piece at 3:15.

<Waveform 
title="AUDIO 11"
file="/ss/a+b.mp3"
peaks="/ss/a+b.dat"
id="audio11"
/>

Even given the radical paradigm shift towards Python, I returned to using the dynamic descriptor query approach, described in ["Selecting Segments"]({m.ss}#selecting-segments). This became a pivotal method for generating material that I then manually composed with and produced contrasting gestural material in comparison to the more static and iterated nature of the Python scripts output. The first section of the piece is entirely composed from long dynamic queries spanning over up to a minute. This created slowly evolving gestures where the change in query increases the spectral bandwidth and diversity of samples selected. This is demonstrated in [AUDIO 12]({m.ss}#audio12).

<Waveform
title='AUDIO 12'
file='/ss/ramping-1.mp3'
peaks='/ss/ramping-1.dat'
id='audio12'
/>

At 2:27, a dynamic query is used to create a rapidly changing texture encompassing highly noisy materials that reduce in amplitude over time. Refer to [AUDIO 13]({m.ss}#audio13).

<Waveform
title='AUDIO 13'
file='/ss/ramping-2.mp3'
peaks='/ss/ramping-2.dat'
id='audio13'
/>

A less linearly shaped query creates an oscillating gesture at 3:15. This is used to punctuate the beginning of a new section. Refer to [AUDIO 14]({m.ss}#audio14).

<Waveform
title='AUDIO 14'
file='/ss/ramping-3.mp3'
peaks='/ss/ramping-3.dat'
id='audio14'
/>

## Reflecting On Human And Computer Agency In Stitch/Strata
The compositional process that took Stitch/Strata to its final version was entirely different to what I imagined it would be at the start of the composition process. The challenges incited by the initial goals, aims and self-imposed constraints, to create a piece "in the computer", and the responses and solutions to those challenges made me reflect on my initial aims and questions for this piece as well as for the PhD research more widely. I was certain from the experience of writing this piece that working entirely in the computer and relinquishing creative control to the degree that I initially tried, particularly in the formal aspects of a work, was and is incompatible with my artistic goals and aesthetic preferences. However, this difficult process was formative and self-realisation. In many ways it outlined what aspects of composition I am willing to give away to the computer and at what stages I need to intervene with those processes to both feel like using the computer is a meaningful choice artistically, and that I am being satisfied in the process itself. I  arrived at a workflow where the balance of agency between myself and the computer was situated more evenly and elicited a creative dialogue that allowed me to infuse computer generated outputs with my own intuition and choices imposed on to that material. 

A main form of interaction that emerged between myself and the computer was the process of auditioning outputs from the computer. This fostered an awareness of the samples within the wider corpus of previously unknown materials. While I had some idea broad idea about the nature of the sounds that I was working with, once the original vocal samples had been segmented there was no way that I could have listened to the 3556 segments and operated on them meaningfully from that point without aid from the computer. The computer provided a structured exploration of those samples, grounded in filtering the larger corpus based on audio descriptors. This approach led me to thinking about groups of samples as blocks of material that could have highly unique and defined sonic properties, or as dynamic gestures shaped by their perceptual qualities. 

Aspects of the workflow are evident in the high-level formal decisions that I made and can be heard in the form of the final piece. 0:00 to 2:32 is built on the arrangement of dynamic queries to create the formal direction and structure. From 2:32 to 3:56, perceptually tight concatenations of samples, derived through descriptor queries and organised by different Python-based procedures are superimposed, with the superimpositions guided by my intuition and manual experimentation. 3:56 onwards is built on processes of intuitive manual segmentation and rearrangement of materials from the computer-generated concatenations materials. These different approaches, concepts and organisational principles would not have emerged without the computer offering initial materials for me to respond to.

This workflow that hybridises computer generation and my intuitive manipulation was more fruitful than the initial one where I assumed the role of "system designer", in which my agency is expressed through a layer of abstraction such as system parameters and constraints. Working *only* in that way did and does not engage me creatively because I find it hard to conceive of musical system that can encapsulate the ideas that I want to express, or which are flexible enough to change form throughout the creative process.

Furthermore, to work with in such an abstract fashion for me requires much better knowledge and visualisation about what kind of machine or program I am working towards. I generally began from the point of a creative *tabula rasa* that is built on through experiencing and experimenting with sound materials. As such, I need to formulate what the piece is both artistically and conceptually through creative action, and intermittently relying on the computer to guide that process is immensely helpful in facilitating that.

### State And The Digital Audio Workstation
The transition from attempting to model specific musical structures, toward building smaller ones and working with them intuitively shifted my workflow from Max to a combination of REAPER and Python. Instead of focusing on having the computer formalise most if not all aspects of a work in a single "execution", I moved toward a strategy in which the computer presented audio outputs as options or suggestions to me, which I could then modify, process, and change in place. 

One significant difference in compositional workflow caused by this shift in workflow was the way I interacted with the computer's outputs and reflected on them to develop the piece. Max does not inherently store or memorise anything to do with the state of a patch such as changes to parameters or sounds that are generated while it runs. Those capabilities have to be built and implemented on a case-by-case basis which can be technically complicated, especially as the patch itself changes in response to creative developments or as new bits of code are introduced. This lack of "state" in my Max programming was something that I found difficult or impossible to address at the same time as thinking about creative problems. The movement to Python and REAPER helped to alleviate some of these issues, and it was only in hindsight that I realised how important managing the intermediate state of creative outputs is to my compositional process.
 
In particular, the ability to store, delete, colour-code, label, annotate and position audio files on a timeline fitted with my need to keep ideas in states of temporary completeness. By taking the outputs of the Python processes and situating them in a REAPER session, they became part of a "sketchbook" that could be modified, added to and grew as the piece did. Furthermore, this sketchbook approach allowed for flexibility in the fixity of the work as it developed. When I was not sure of the compositional usefulness of a computer generated output, those outputs could be kept in the periphery of my work temporarily, without having to discard them entirely. Ideas that I found particularly compelling were easily retained alongside information denoting their importance or projected use. It also allowed me to move between different creative mindsets fluidly, whether those mindsets are focused on sound-design, low-level manipulation of samples, or zooming out to observe the high-level formal aspects of a work. The DAW in this sense acts as scaffolding  helps me to draw the computer into my practice when it is needed, rather than relying entirely on a system that I need to design from the ground up.

## Concluding Remarks
Stitch/Strata began with lofty compositional and creative aims that were challenged and eventually dismantled through the creative process itself. At the time, these changes to my creative practice and explorations of new ideas were both exciting as much as they were difficult to embrace. Reflecting on them now though, there were many aspects of the workflow that emerged that became fundamental in my approach to composing [Reconstruction Error]({m.re}) and [{m.emname}]({m.em}) as well as the development of [FTIS]({m.ftis}) which heavily leans on REAPER as an environment to explore computer generated outputs aurally and incorporate the contributions of the computer into my computer-aided practice.

The next project to be discussed is [Annealing Strategies]({m.as}). This work was perhaps subconsciously another attempt to see how I could relinquish control to the computer, albeit with a very different approach to the generation of sound and a deeper level of integration between the compositional material and the technical approach. In a number of ways this piece was more successful, but the workflow that I built up through the creative process confirmed for me that relinquishing a lot of my control and ability to intervene with the behaviour of the computer is not compatible with how I want to compose music in a computer-aided practice.

<NextSection 
next="Annealing Strategies"
link={m.as}
/>

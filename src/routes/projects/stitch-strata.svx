<script>
    import VideoMedia from "$lib/components/VideoMedia.svelte";
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
	import NextSection from "$lib/components/NextSection.svelte";
    import {metadata as m} from "../directory.svx";
    import Sage from "$lib/demos/Sage.svelte";
    import { sageUnshuffled, sageShuffled } from "$lib/components/data/ss.js";

</script>

# Stitch/Strata

Spiel about piece

The first performance of this piece was at the [2019 Electric Spring Festival](http://electricspring.co.uk/electric-spring-2019/) using the [Huddersfield Immersive Sounds System (HISS)](http://electricspring.co.uk/electric-spring-2019/). The recording of the piece that is submitted in the portfolio exists in two versions. A binaural version that attempts to capture the spatialisation from the first performance and a normal stereo version.

## Motivations and Influences

At the beginning of this PhD, I focused on the question of "how can the computer be creative in composition?". This question was a driving factor in what I wanted to explore and was a reflection of the curiosities spawned from a previous project, [Biomimicry]({m.biom}) in which the computer was seemingly creative and was able to construct sophisticated formal events in real-time.  I attribute many successful aspects of Biomimicry  in this regard, to the contributions from the improvisers. Primarily, the improviser drove formal events and the computer was only able to respond to the changes and structuring of time imposed by its human counterpart. Reflecting on this, my supervisor Dr. Alex Harker encouraged me to write a piece entirely "in the computer" and without any improvisers or instrumental component. The aim of this challenge was to remove the influence that a human could have on the computer in the performance of a piece and to encourage me to find computational strategies for creating sophisticated formal events.

As such, a major motivation for this project was to investigate how the computer could structure all formal levels of a work, from the selection of individual samples, their arrangement into gestures, their arrangement into sequences of gestures and so on and so forth until the work is additively built from a set of atomic units (samples) and a governing algorithm. Ideally, this would take my influence away from arranging materials at different levels of detail myself, to inventing procedures that the computer would then use to realise a work automatically. In this regard, a number of other practitioners were influencing me and driving this type of thinking. These are described in the next section, ["Other Practices"]({m.ss}#other-practices).

<!-- A motivation for me making the computer generate material is so that I do not have to conceive of all the material myself, and instead can create a strategy in which the computer can encompass variations and different forms of an idea. Within this problem though I want the computer to be able to be seeded with definitive qualities, such as a certain types of sound materials or constraints on how it uses these. Finding a strategy to represent both known and unknown qualities in the system design  is something that I wanted to achieve in this project, ideally so that I can create a form of interplay between aspects which are fixed and those which are not. -->

### Other Practices
Michael Young's "Neural Network Music" (NN Music) was particularly prominent in my set of influences. This is a real-time improvisational piece for an unspecified instrumental improviser and the computer. The piece is based around the computer as an improvisational partner that moves between a number of states. It decides which state to be in based on classifying the sound behaviours of the improvisational partner and the nature of their playing. The caveat of this is that the computer begins with a limited classification model that it then builds up over time in the course of the piece. The neural network that underpins how the computer responds to the performer is constantly retraining on the fly and developing a mapping between a set of actions it has stored in memory and the performance in the moment.

I found the technological approach of this work inspiring for two reasons. Firstly, much of the "composing" is done without necessarily making decisions about specific and detailed musical structures. Instead, a more general model for how sound should be made in real-time is devised and the results of this are somewhat open-ended. Secondly, the computer *forms its own connections* and uses those to make decisions about the structure of its responses to the performer. Those connections are not explicitly pre-composed or made ahead of time and instead are formed in the moment dynamically. 

At the time of composing [Stitch/Strata] I was also reading articles by [Adam Linson](http://percent-s.com/) and interested in his improvisational and creative coding practice. In particular, I found the way that he incorporates the computer into practice as an autonomous creative agent something that I could learn from and adapt to my own needs. Linson et. al (2015), for example, describes the design of "Odessa", a system that uses a subsumption architecture in order to build up an improvisation behaviour. The architecture is described:

<div class="bigquote">

*In Subsumption terminology, the “competing behaviors” of a system are organized into a network of interactive “layers” (Brooks 1999). Following this idea, and further informed by psychological research, Odessa’s behavioral layers consist of its basic agency to spontaneously produce output ("Play"), its ability to respond to musical input ("Adapt"), and its ability to resist by disregarding musical input, introducing silence, and initiating endings ("Diverge").* (Linson et. al., 2015, p. 4)

</div>

IMAGE 1 depicts how at a programmatic level these layers contain individual sound producing and interactive modules which are sensitive to the input of a live performer. By connecting these modules to each other across subsumption architecture, Linson et. al (2015, p. 13). hypothesise that this "would serve to produce cues that suggest intentionality". 

<ImageMedia2 
url="/static/ss/odessa.jpg"
caption='Odessa subsumption architecture taken from Linson et. al (2015). The boxes represent modules belonging to one of the three behavioural layers, "Play", "Diverge", "Adapt".'
figure="IMAGE 1"
/>

The subsumption architecture by Linson et. al. appealed to me in simplifying the problem of having the computer deal with time and form on multiple levels. I envisaged that I could experiment with something similar and construct layers as computational behaviours and then devise various mechanisms for how those layers would interact with each other. In a similar fashion to the resonance I found in Young's work, the computer would not have to be specifically programmed to deal with the minutia of musical decision making, rather, I could build a system that embodied a number of strategies for dealing with decision making, and the details would resolve themselves. 

These kinds of generalisations led to many failed attempts and created such a frustrating compositional process that I ended up rejecting the initial constraints of working "in the computer" and returned to me involving intuitive compositional processes enacted by me manually to finish the piece. This process is described next.

## Compositional Process
In this section I will describe the journey of the compositional process from the earliest experiments composing *with* the computer to the final result. Along the way I will detail some additional musical influences and reflect on the trajectory of this project.

### The Influence of Experimental Vocal Improvisation
Alongside the compositional influences described in [Motivations and Influences]({m.ss}#motivations-and-influences), listening had an impact on the sound materials that I gathered for composition. In this regard,  [Sage Pbbbt](https://sagepbbbt.com), whose enormous catalogue of vocal improvisation "daily sketches" became a trove of material that I would later draw on. These improvisations demonstrate the richness of extended vocal techniques and the diversity of sounds that can be produced. In connection with Sage's practice of extended vocal technique improvisation, I came into contact with the discography of [Phil Minton](https://www.philminton.co.uk) as well as his collaborative efforts with [Audrey Chen](http://www.audreychen.com) in *By The Stream*. Lastly, [Trevor Wishart's](http://www.trevorwishart.co.uk) piece [*Tongues of Fire*](https://www.youtube.com/watch?v=Ude4717dlsQ).

Extended vocal sounds had been a material that I wanted to work with in composition more deeply, so coming in contact with this rich world of experimental vocal improvisation created a confluence of sonic and technological interests. I myself am not a vocal improviser and needed some source materials to compose with and decided to use the entirety of the daily sketches by Sage Pbbbt due to the liberal [Creative Commons](https://creativecommons.org/licenses/by-nc/3.0/) license. My plans were to deconstruct this large body of work and use the computer aid me in selecting and arranging it.

### Building A Corpus Of Materials
I began by segmenting all of the recordings using REAPER's dynamic split tool and configuring it to segment based on the detection of transients. This choice was made because I wanted to extract individual utterances and phonetic-like sounds from composite gesture to be rearranged and re-concatenated. AUDIO 1 shows an example of this , by taking a portion of ["One Breath Poem Failure"](https://sagemusick.bandcamp.com/track/2016-02-16-one-breath-poem-failure), segmenting it and then recombining the segments randomly. The colours of each segment track the displacement of each utterance.

<Sage
    file1="/static/ss/gesture.mp3"
    peaks1="/static/ss/gesture.dat"
    file2="/static/ss/shuffled_gesture.mp3"
    peaks2="/static/ss/shuffled_gesture.dat"
    segs1={sageUnshuffled}
    segs2={sageShuffled}
/>

### Selecting Segments
Experimenting in this rapid way produced results that immediately sparked an interest in me for how I could recombine the individual segments of these gesture and to synthesise new materials by recycling items from this corpus of vocal utterances. The next challenge to face was building a system that would be responsible for organising phrases from these materials. I did not necessarily have a strong idea about what would constitute a phrase in terms of formal scale, but I knew that I wanted to explore recombining segments such that result would be perceptually "smooth", and not discontinuous and digital sounding. Given these loose constraints, I set out experimenting with a number of approaches in which individual segments would be concatenated into continuous *streams*.  

A strategy I explored early on was to employ audio descriptors in order to analyse the RMS, spectral centroid and spectral flatness measure of each sample. I anticipated that these three bits of data would allow me to differentiate each sound from each other by capturing their "brightness" (spectral centroid), "noisiness"  (spectral flatness) and intensity (RMS). I could then use the computer to find samples that were relatively homogenous in the percept. I could then combine samples that were similar enough according to a metric of distance. Using a Max patch, I was able to test this idea by querying a database of this descriptor information to return samples that fit a set of criteria. In [VIDEO 1]({m.ss}#vid1), I demonstrate this in practice, where I query for up to 50 samples from the corpus of segments based on three different constraints. Each slider corresponds to a different descriptor, so by changing the value in this interface I can modify the query.

1. Samples that have a spectral centroid greater than the given threshold
2. Samples that have a spectral flatness measure than a threshold
3. Samples that have an amplitude within 6dB of a given value

<VideoMedia id="vid1">
    <video slot="media" controls>
        <source src="/static/ss/descriptorspace.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/static/ss/descriptorspace.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 1: Querying the corpus of voice segments using audio descriptors.</p>
</VideoMedia>

Using the computer to query the corpus and find groups of samples matching a particular descriptor specification, I was presented with a number of possibilities that appealed to me aesthetically. In particular, results which produced texturally focused groupings were appealing to me, because by simply concatenating those sounds together it was possible to hear how they could form a detailed texture. Another aspect I did not intend to find, but found appealing, was the way that a morphology can be imposed on the concatenative process by changing the thresholds of the query dynamically. This can be seen specifically at 0:20 in [VIDEO 1]({m.ss}#vid1).

This descriptor processor underpinned much of the compositional process to follow. From this point I attempted to devise ways in which the computer could take these notions of forming material by querying by descriptor matching and use it in combination with another systematic layer that would construct form. This is discussed in ["Constructing Form Computationally"]({m.ss}#constructing-form-computationally)

 ### Constructing Form Computationally
In this phase of the composition process I faced a number of challenges when trying to devise systems for controlling form in combination with the descriptor matching paradigm. A number of strategies were explored in which I attempted to have the computer impose a level of control over what queries in order to dynamically create different material groups on-the-fly. 

One approach was to create predefined "shapes" that would dictate the overall quality of the gesture, without specifying any of the details in rendering it. This has a loose and sometimes tenuous connection to the perception of the audio which in some regards is very convincing, and others arbitrary. This type of approach is discussed in Harker (2012), in which he outlines how bespoke Max externals have been used toward similar ends in his compositional practice.

VIDEO 2 demonstrates one implementation, where the computer uses a parabolic shape to dynamically alter the amplitude and spectral centroid query values. This creates a smooth transition between the internal states of the gesture as different samples are selected. 

<VideoMedia id="vid2">
    <video slot="media" controls>
        <source src="/static/ss/dynamic-gesture.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/static/ss/dynamic-gesture.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 2: Dynamic gestures created by dynamically querying a corpus of vocal segments based on audio descriptors.</p>
</VideoMedia>

VIDEO 3 shows another idea in which a static state of very low amplitude is intermittently "escaped" from by updating a query for new samples in real-time. This creates a series of jumps between static states.

<VideoMedia id="vid3">
    <video slot="media" controls>
        <source src="/static/ss/static-jumps.webm" type="video/webm">  
        <p>
        Your browser doesn't support HTML5 video. Here is a <a href="/static/ss/static-jumps.webm">
        link to the video</a>instead.
        </p>  
    </video>
    <p slot="caption">VIDEO 3: A self-contained static musical behaviour is formed by creating an "anchor" in the descriptor space and momentarily departing and then returning to it.</p>
</VideoMedia>

These are just some of the strategies employed. Many are codified into the Max patches, or were left in half finished states and at various stages of development. Ultimately with this approach, I embraced what Harker (2012, p. 7) terms a "perceptual equivalence". Individual samples are interchangeable in regards to the query and variability is constrained to the dynamic shape rather than specific choices. As such, this strategy for having the computer select samples and arranging them revolves around communicating a generalised fixed structure which is detailed and resolved computationally.

Stacking these gestures and giving the computer to select between different "presets" of them, creating variation through recombining simple and effective archetypes.

- Automatic trajectories imposed by simulated annealing
    - initial_attempts > recordings > 5712433.txt (annealing)

<Waveform 
id="wave1"
title="AUDIO 1: Adversarial searching"
/>

Pitting two dynamic searches against each other. One that attempts to minimise the volume of the output by changing the centroid and one that tries to find the maximum centroid by minimising the volume. This adversarial process has no resolution but produces some interesting longer term morphologies and structures without having to specifically implement them. Part of this process was also a precursor to my engagement with Simulated Annealing in [Annealing Strategies]({m.as}).


- Using descriptor values and accumulators to generate passages with finite amounts of 'energy'
    - Lots of tuning
- sample selection
- z12 and shuffling samples.
    - Puckette (2015)
    - Just another level of shuffle, not necessarily interpreted as an something with order or a level of organisation. 

- Issues
    - Lacking formal direction and intention
    - Way too much time spent on low-level detail that had no relevance to the higher-level form (lots of friction)
    - Thinking about what I would 'want' to do, and an appropriate model for that. Lots of layers of abstraction to fight through to get to musical result when I already had a good idea about the types of sounds and organisations I wanted. Basically questioning why I would have the computer make certain decisions automatically?

More decisions were introduced, that on top of this came with their own set of technical requirements and concerns
Too much effort was put into finding generative or algorithmic models that reflected what I wanted to be composed in the first place.
There wasn't even a sense that these schemes were introducing an unintended or insightful aspect to the compositional process, they were just failures in replicating an existing way of thinking that I could have performed manually myself.

Ultimately, the frustrations of not meeting these challenges led to a radical change in the constraints I initially wanted to explore for Stitch/Strata.

None of these led to a piece, they were okay as experiments, but nothing spoke to me on an artistic or conceptual level about how the computer might build an entire piece in this fashion. 

Numerous meetings where the idea for this piece would pivot or I would pursue a new strategy beliving it would lead to a piece.

### From Modelling To Assistance
- I needed to make a drastic change to my workflow and "break" some boundaries I had put up.
    - Desired balance between human and computer agency (was one of these)

- How did I do this?
    - "Cold turkey" leave Max behind. Project structure was incredibly messy, bloated and I couldn't trace my compositional development. It was a mess of small experiments without being able to track how things had progressed. 
    - The way I wanted to work required creating a lot of abstraction around very trivial ideas, like looping over a container of audio files representing a gesture.
    - Pick up Python as a new tool
        - break the "rut"
        - Python is a different form of expression
    - Using Python, to iterating over possibilities and generating small structures
        - itertools to create patterned structures.
            - A better approach than Z12 for me.
    - Building smaller structures and controlling their order in time myself, or manually blending them was a more fruitful approach than finding a system to govern that type of high-level thinking.
    - Generating numerous sound files with variation very rapidly.
    - Working back in a DAW where things could be fixed to some degree and don't have to be 'programmed' in the same way.
        - Use the inherent "memory" of the DAW to create temporary stores of decision making. This acted like scaffolding to realise the piece

    - Examples
        - concat.py
        - /phrases/
        - 
## Reflecting On The Balance Of Human And Computer Agency
- The transition from attempting to model specific musical structures, toward building smaller ones and working with them intuitively shifted the balance of contribution made by the computer to the compositional process. Instead of focusing on having the computer formalise most if not all aspects of a work using detailed systems, I moved toward a strategy in which the computer presented options to me, which I could then modify, process, change in place all while leveraging the interface of the DAW. 

- The DAW reorients practice in many ways through its affordances as an environment for composition. 
    - memory bank for ideas which can be held in half-finished states.
    - Ordering of ideas can be changed 
    - Processes can be tested on specific musical ideas/objects/sequences
    - Creates opportunity to intervene in many ways and to step between generation and manipulation. 
 
### Moving Away From Modelling
- I dont know my own rules, patterns or restrictions and so modelling becomes too complex of a process in which I am trying to both codify a musical language
- I dont want to do automatic composition because I want to work non linearly at times, moving between different types of composing (sound-design, low-level manipulation of utterances or big picture). Only interacting with the musical result through an algorithm feels less conducive to this.
- - A better balance between "soundful practise" and technological practise.

- I want to feel like the time structure is uncovered/discovered rather than designed as part of the material.
    - Allude to annealing strategies
        - on the threshold here and less about specific codified structures and more about a shape or simple language to compose with

how do i take reponsiblity for what comes out - how does the computer side interact with my practice


<NextSection 
next="Annealing Strategies"
link={m.as}
/>

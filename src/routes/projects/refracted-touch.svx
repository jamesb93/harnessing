<script>
    import Caption from "$lib/components/Caption.svelte";
	import NextSection from "$lib/components/NextSection.svelte";
    import YouTube from "$lib/components/YouTube.svelte";
    import VideoMedia from "$lib/components/VideoMedia.svelte";
    import {metadata as m} from "../directory.svx";
</script>

# Refracted Touch
After composing both [Stitch/Strata]({m.ss}) and [Annealing Strategies]({m.as}) I felt as if the techniques and approaches that were explored in these works had little room to be developed. I discuss and reflect on this in ["Reflecting On The Balance Of Human And Computer Agency"]({m.ss}#reflecting-on-the-balance-of-human-and-computer-agency). At this time, the [ELISION Ensemble](https://www.elision.org.au) we're in Huddersfield as guest artists and I put myself forward to write a piece for [Daryl Buckley (Electric Guitar)](https://twitter.com/darylelision?lang=en) and Electronics.

## Motivations and Influences
A major part of my motivation for writing Refracted Touch was to take the opportunity of writing for a professional instrumentalist and performer, Daryl Buckley. I was not entirely sure what this creative work was going to be, so I thought that throwing myself into the deep end (what is a better way to express this) without too many preconceptions or plans would create an environment where I would be challenged and pushed.

Despite not having a clear or formal plan for what this work, I always envisaged this piece to be based on improvisation and real-time interaction between Daryl and an improvisatory system. I did not want to write a fixed piece of music with a score determining what Daryl would play as I felt that this would restrict how I could harness the computer to be dynamic in the creative process. I also felt that the last two pieces, [Stitch/Strata]({m.ss}) and [Annealing Strategies]({m.as}) had explored specific compositional workflows and that this would be a good opportunity to re-explore the area of live electronics.

As such, this piece is improvisational and grounded in Daryl interacting in real-time with the computer which dynamically generates responses depending on what he plays. The machine listening component of the piece is based mostly on detecting and measuring changes in amplitude, as well as recycling the sound of the guitar in a number of ways. On top of this dynamic behaviour is a loosely pre-composed structure that can be flexibly stretched or compressed by the performer by altering the behaviour of their playing between soft and loud extremes.

This is a unique piece in the set of works for this PhD, because to me it was the least successful creatively in terms of compositional workflow, but not necessarily the result. It also revealed to me that I no longer wanted to work in the paradigm of live interactive electronics. There are several reasons for this which will come to light by divulging different versions of the piece, which exists as a Max patch.

### Musical Influences
Daryl and I worked on this piece remotely up until the rehearsals just before the performance of the piece. Given that we both live in different countries, our main method of communicating and sharing ideas was for me to ask questions and for Daryl to provide recordings and short videos of techniques and sounds that he could produce. This was a large source of inspiration and guidance for me, as it outlined some constraints of what was possible, but also gave me an idea about what the electronics might do such that their interactions with Daryl would be suitable. I wanted Daryl's own personality, style and approach to be infused into the piece, rather than imposing a set of playing techniques and restrictions on to him at all times.

In addition to the footage Daryl provided, I listened and watched as much material of his playing that was available online, examples of which can be found in VIDEOs [1](), [2](), [3](), [4](), [5]().

<YouTube 
title="Matthew Sergeant in conversation with ELISION's Daryl Buckley (1 of 2)" 
url="https://www.youtube.com/embed/bo_n1JXGiMY" 
figure="VIDEO 1" 
/>

<YouTube 
title="Matthew Sergeant in conversation with ELISION's Daryl Buckley (2 of 2)" 
url="https://www.youtube.com/embed/S0WMNvkLODU"
figure="VIDEO 2" 
/>

<YouTube
title="The Wreck of Former Boundaries by Aaron Cassidy"
url="https://www.youtube.com/embed/bLKRJ2GfFEY"
figure="VIDEO 3"
/>

<YouTube
title="Dark Matter by Richard Barrett"
url="https://www.youtube.com/embed/ov9ZcOrONiY"
figure="VIDEO 4"
/>

<YouTube
title="Lichen by Matthew Sergent"
url="https://www.youtube.com/embed/b9s-8BPXd58"
figure="VIDEO 5"
/>

I also have some experience playing the guitar, but not extensively and would consider myself amateur. I experimented with extended techniques and preparations such as bowing with a cello bow, inserting nails and paper clips and attaching pegs to strings. Despite not being able to replicate Daryl's playing or recreate his expertise, it provided a way of having tangible experimentation outside of only using digital samples. This was helpful for testing and designing patches in a more real-world situation and some of the recorded material was used in the final piece.

With this method of composition and working almost entirely remotely, the piece remained in a highly speculative state for much of its development and was created by using recordings of myself and those extracted from these musical influences.

### Machine Listening for Formal Development
Creating form with the aid of the computer has been a focus in all of the projects and pieces so far. In Refracted Touch, I envisaged that the computer would be able to influence the form of the piece in a dynamic fashion such that there would be some loosely defined structure that could be stretched or compressed in real-time.

[Annealing Strategies]({m.as}) and [Stitch/Strata]({m.ss}) dealt with form in their own unique way and Refracted Touch required a different approach as I could not rely as heavily on my intuitive control, given that it is a piece which is realised in real-time. 

In order to address form, I looked to other works where live electronics and interactive systems were used to structure pieces in-the-moment and where the computer was responsible in some way for the structural and temporal aspects of a work. In this regard, Martin Parker's set of [gruntCount pieces](https://sumtone.bandcamp.com/album/gruntcount-improvised-pieces-for-player-and-computer) were influential. Parker describes the system as: 

<div class="bigquote">

*Each edition of gruntCount is personalised from the outset, with composer and performer working together to produce the elements of a system for creating well-­‐defined and structured musical pieces that invite liberal performer input, spontaneity and intuition. In the bass clarinet version (2012-­‐14), this preparatory stage involved a period of system "training", in which the composer engaged in real-­‐time free improvisations between himself and the player, creating at speed a unique set of interrelated DSP parameter presets... Having designed these settings, gruntCount’s compositional agenda proceeds with the plotting of various journeys or curves through the DSP settings. These curves may resemble a graph or automation curve, but in fact represent specific trajectories through a parameter space, which itself has nested settings within it. There is a formal design here, a quality and style, and yet the manner in which the piece is individuated is entirely defined by the live performer, whose physical efforts (or "grunts") move the assemblage forward.*  (Parker & Furniss, 2014, p. 1) 

</div>

The technicalities of this piece are straightforward to understand, and the complexity and richness of interactive behaviour is constructed by combining several simple principles. An onset detector generates "grunts", which progress the Max patch forward through a set of predefined states. The states are flexibly defined before running the patch. As such, there is some negotiation between what the performer and composer define, how that is navigated in real-time by triggering grunts as well as the sensitivity of the onset detection. This is relatively simple, but powerful in creating variation and expressivity in the moment. Parker gives a demonstration of how the patch works in [VIDEO 6]({m.rt}#vid6)

<VideoMedia id="vid6">
<iframe slot="media" src="https://player.vimeo.com/video/111283604" width="640" height="362" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen></iframe>
<p><a href="https://vimeo.com/111283604">gruntCount instruction video - June 2014</a> from <a href="https://vimeo.com/user5763633">Martin Parker</a> on <a href="https://vimeo.com">Vimeo</a>.</p>

<p slot="caption">VIDEO 6 Martin Parker demonstrating how to configure and run gruntCount.</p>
</VideoMedia>

Similary, the work of Owen Green from his PhD thesis was influential. In particular, I drew from and infused signal processing concepts informed by two pieces, [*Danger in the Air*](http://owengreen.net/portfolio/danger-in-the-air/) and [*Cardboard Cutout*](http://owengreen.net/portfolio/cardboard-cutout/). *Danger in the Air*, features live electronic components that adaptively respond to the input of the performer and occupy a range of agent-like behaviours from sound processing module to co-player. From this work I was inspired by the way that Green made the electronic processing transform over the duration of the piece by making the real-time granular synthesiser's parameters adapt to the sound of the input through audio description and digital signal processing. 

*Cardboard Cutout*, uses signal decomposition to separate harmonic from percussive components from the composite input signal. After this stage, each of these layers is processed with granulation. Like *Danger in the Air*, the granulation process adapts to the signal. I took a similar approach in Refracted Touch, although I separate harmonic and percussive components from each other in order to process them uniquely and to help improve an onset detection algorithm. Furthermore, Green's approach to time and form in this work is constructed as a negotiation between pre-composed sections and the behaviour of the performer which drives a sequencer that moves between those sections. Green describes his approach as follows:

<div class="bigquote">

*The modulation of the various processes at work in the patch is driven, in large part, by a sequencer that moves through a series of pre-defined steps (see Figure 8.3 on p. 202 in the technical documentation below for details of this).The progression of the sequence is still driven by player input, however, so whilst the ordering and certain temporal aspects of the electronic processing are determinate, their precise pacing is adaptive.* (Green, 2013, p. 195)

</div>

In the later versions of the Refracted Touch patch, I implemented a similar approach to form, where pre-composed states are progressed through according to the accumulated "energy" of the performer's input signal. 

## The Patch
Refracted Touch was developed by iteratively developing a patch and by using pre-recorded materials derived from my own guitar playing and from Daryl. Each version captures my compositional thinking at the time of its creation. This section will briefly describe what each patch version does and situates it in the development of the piece. As such, I will separate this section into two sub-sections, ["Earlier Versions"]({m.rt}#earlier-versions), which describes all but the final version and ["The Final Version"]({m.rt}#final-version) which shows in more detail the final form of the patch and thus piece.

### Earlier Versions
#### Version 1
I began composing Refracted Touch by collecting and making sounds, recording them and then trying to build signal processing modules around these that produced aesthetically pleasing results. I have provided some context for other interactive electronic works, in ["Machine Listening for Formal Development"]({m.rt}#machine-listening-for-formal-development) that influenced me, and much of my thinking and experimentation at this early stage of the piece was based on reimplementing and trying out ideas that I had derived from looking at their code and listening to their work.

The first module I created was a harmonic percussive source separator in [FrameLib](https://github.com/AlexHarker/FrameLib). I started with this module as an intuitive choice, based on the assumption that certain aspects of amplified slide guitar playing from Daryl could be separated into novel components. For example, the sound of striking strings with metallic objects has an articulated attack that is mostly percussive, followed by the harmonic resonance of the amplified guitar. I wanted to see if these elements could be separate from each other, and if this process could render new material or function as an effect. [VIDEO 7]({m.rt}#video-7) gives an example of this process on some sampled slide guitar techniques with heavily detuned strings.

<VideoMedia>
    <video slot="media" controls loop>
        <source src="/rt/hpss-demo.mp4" type="video/mp4">  
        <p>Your browser doesn't support HTML5 video. Here is a <a href="/rt/hpss-demo.mp4">link to the video</a> instead.</p>  
    </video>
    <p slot="caption">VIDEO 7: Demonstration of the harmonic percussive separator process.</p>
</VideoMedia>

In particular, I found the percussive extraction the most creatively fruitful. Without the harmonic component to support the sound, the expressiveness and articulation of attacks is transformed and the identity of the sound is changed radically. 

Both the percussive and harmonic components are kept in constantly running buffers such that the last few seconds of sound is continually being separated and maintained in memory. These buffers of source separated sounds are used by other sound producing modules such as:

*Short Resonators (demonstrated in [VIDEO 8]({m.rt}#vid8)*
- 32 [CNMAT `resonators~`](https://cnmat.berkeley.edu/content/resonators) in parallel excited by a noise source.
- Each resonator can be set by providing a list of three values, the frequency, the gain and a triplicate which determines the decay.
- The harmonic component is analysed by the `sigmund~` object to produce a set of data describing the sinusoidal components. This gives the frequency and relative gains of those sinusoids which is then used to control the `resonators~`.
- An amplitude onset detector, triggers an envelope that allows the `resonators~` to sound momentarily when the guitar is hit.
- The intended effect is that the `resonators` will resonate "in tune" with the guitar, albeit imperfectly due to the discrepancies between the `sigmund~` analysis, creating a hybridised sound between the guitar attacks and the parallel filter bank resonance.

<VideoMedia id="vid8">
    <video slot="media" controls loop>
        <source src="/rt/resonator-demo.mp4" type="video/mp4">  
        <p>Your browser doesn't support HTML5 video. Here is a <a href="/rt/resonator-demo.mp4">link to the video</a> instead.</p>  
    </video>
    <p slot="caption">VIDEO 8: Demonstration of the "resonators" module.</p>]
</VideoMedia>

<!-- TODO: Fix this....? -->
 <br>
<i>Metallic Feedback</i>

- This module is composed from two identical feedback comb filters that are connected in series.
- They have two parameters: a delay time and a feedback amount.
- They are self-modifying, as the output gets louder the feedback amount is lowered. The feedback will never exceed 1.0 and attenuating it in this way helps to control it and stop it from becoming overpowering all the time as well as creating slight variation in response to the input signal.
- The delay time is modulated to a random value between 1 and 7.9 milliseconds for the first metallic feedback module and 7.34 and 9 milliseconds for the second one. The different delay times and the way that they are connected in series creates complex morphing properties to the processing ranging from tight feedback "plucks" to longer metallic resonances.
- The output of the short resonators is connected to the input of the first feedback comb filter making an already resonant and tonal sound become temporally stretched, metallic and producing the effect that the material is being recycled.
    
*Granular Synthesis* 

- The  portion of the most recent input from the percussive component is used as the source for a granular synthesis module. This enables the granular synthesis to be dynamic when generating grains, as well as creating a feeling that material is being recycled by the computer. The parameters for this are manually fixed to create a textural companion to the input sound. A demonstration is provided in [VIDEO 10]({m.rt}#vid10)

<VideoMedia id="vid10">
    <video slot="media" controls loop>
        <source src="/rt/gran-demo.mp4" type="video/mp4">  
        <p>Your browser doesn't support HTML5 video. Here is a <a href="/rt/gran-demo.mp4">link to the video</a> instead.</p>  
    </video>
    <p slot="caption">VIDEO 10: Demonstration of the granular synthesis module.</p>
</VideoMedia>

*Cubic Non-Linear Distortion*
- This module is a cubic non-linear distortion module that in addition to adding harmonics to the input signal can alter the dynamics of the input sound.

The gen code for this is shown in CODE 1 and a demonstration can be found in [VIDEO 11]({m.rt}#vid11)

```js
cnld(xin, offset, drive) { /* cubic non-linear distortion */

	offset = clip(offset, 0., 1.);
	drive = clip(drive, 0., 1.);

	pregain = pow(10., (2. * drive));
	x = clip(((xin + offset) * pregain), -1., 1.);
	cubic = x - (x*x*x)*(0.3333333333);

	return dcblock(cubic);
}

/* + 1x or 2x more dcblocker after downsampling process */

out1 = cnld(in1, in2, in3);
```

<Caption>
    <p slot="text">CODE 1: Cubic Non-Linear Distortion</p>
</Caption>

<!-- WIDGET: Sound Example -->

<VideoMedia id="vid11">
    <video slot="media" controls loop>
    <source src="/rt/cnld-demo.mp4" type="video/mp4"> 
    <p>Your browser doesn't support HTML5 video. Here is a <a href="/rt/cnld-demo.mp4">link to the video</a> instead.</p> 
    </video>
    <p slot="caption">VIDEO 11: Cubic non-linear distortion module with two different test sounds.</p>
</VideoMedia>

**Temporal Changes**

Alongside the sound processing modules, I began creating small sub-patches that would be responsible for creating change in the patch over time. These patches create events, or drive modifications to parameters in response to the input signal, in order to make overall changes flexible in nature while giving me the ability to specify what the changes themselves would be. I decided upon an approach of *accumulation*, whereby the energy of the harmonic and percussive components would accumulate separately up to a specified limit. Once the limit has been reached the accumulator creates a generic event, that can be mapped elsewhere in the patch and the internal value is reset to 0. From this process different events can be generated, or the accumulation process itself can be mapped as a parameter to other processes.

<!-- WIDGET: basic accumulator example -->

For example, the envelope length of the resonator module is based on the interval between resets of the harmonic components accumulator. The more harmonic energy that is put into the system, the longer the envelopes become. My thinking in this regard was that this would make the resonators playable over longer scales, as well as making them highly responsive to what the performer decided to do. When the same harmonic accumulator resets, the resonator itself is toggled between the on and off status. This was designed such that there would be times where the resonator was dormant. Although these were never actualised, notes from the patch itself were recorded documenting some imagined applications. In these notes the "timers" refer to the interval between resets, the "bangs" refer to those reset events and the "raw signal" refers to the accumulation value itself.

```
The timers could be used for....

--------------------------------------

1. Controlling the rate of something
2. Establishing envelope lengths
```

```
The bangs could be used for...

--------------------------------------

1. Engaging long form changes
2. Turning elements of a system on/off
3. Creating harmonic swells (resonators~)
```

```
The raw signal could be used for...

--------------------------------------

1. Pushing an envelope forward
2. Scrubbing through a buffer
3. Alternating between resonator banks
4. Pushing the prevalence of certain systems up/down (long form stuff)
```

The motivation for this, was so that the computer would at times impose formal development and change on to an improvisation, while also remaining to an extent sensitive to the performer. The performer could for example, halt the accumulators by reducing the activity of their playing potentially creating a sense of static behaviour. However, the accumulators would still be functioning in the background, albeit slowly and so change would not be completely avoided. Conversely to this, the performer may decide that they want the computer to rapidly alter its state, or to create bombastic and fleeting modifications to which they might respond. At the same time, they might want to play quietly and without much activity, so these two aspects cannot be easily mediated. Such a tension as this, creates a negotiation between the performer's playing activity and the ability for the performer to effect change in the improvisational partner, which I thought may create a type of "game" in which the performer and computer are antagonised by each other.

This was only the first version of the patch, and so much of what it contains were nascent experiments that were not cohesive as a whole. My main method of experimentation was to play different components in isolation rather than having a "start" button that would then play the piece (as was the case in the final performance). As such, the patch is perhaps more useful as a starting point that situates this project amongst a set of early influences and approaches. From this version of the patch, it can be observed how certain elements were retained through the development process, and what this may suggest about my preoccupations compositionally and sonically. 

#### Version 2
Many aspects from the first version are left relatively untouched in this second patch version. On reflection, this patch was a "stepping stone" to the later versions, as there are several unfinished components. Despite the incomplete nature of this version, there are some key developments that were used in the final version and the performance.

A significant change was made to the granular synthesis module. I found that although I liked the notion of granulating the percussive component of the input signal, the result was often overpowering and homogenous, especially with certain settings that favoured longer grain sizes. To mitigate this, and to increase the expressiveness of the granular synthesis module, I began creating small mechanisms that would modulate the parameters in response to the input signal.

For example, instead of constantly triggering the generation of new grains, the interval between this is determined by the amplitude of the input signal. Similarly, the time offset of grain generation (where in the buffer the grain starts) is dependent on a phasor signal that increases in speed with amplitude. Transposition is also modulated in this same way. IMAGE 1 describes the signal flow of this

A new module named "low" is in this version. This module focuses on responding to moments when the guitarist is performing gestures and techniques that produce low frequency sounds. This is detected using the `energy_ratio` descriptor from Alex Harker's `descriptorsrt~` object like an onset detector, by setting a threshold for the energy between 0hz and 150hz. When this energy is exceeded an onset occurs. The result of these onsets 

It plays harmonic sections slowed down depending on the input of the 

Increase in detail for the self-modulating aspects of the patch.

Accumulators become a generalised part of the patch that aren't necessarily attached to a single module or parameter. The percussive and harmonic components have their own ensemble of four accumulators. Each sequential accumulator takes an increasing amount of energy to reset. The interval between resets for each scale is recorded. Interestingly, this patch is the most ordered and structured with clear intent. Despite this, the recorded intervals are not used anywhere else in the patch. There is no documented reason for this and I cannot recall from memory but my assumption would be that I found the other modules more interesting to iterate on and decided to start a new version before experimenting with the accumulators.

#### Version 3/4

Version 3 and 4 are incredibly similar, with the later version mostly implementing fixes to problems and changing the cosmetic aspects of the patch. As such, I've decided to group them together.

**THE DEVELOPMENT TOWARDS CONCEPTUAL GROUPS OF MODULES AND STATES**

#### rt.bits
- Rack Sounds
-Concatatenative Synthesis

#### rt.harmfoc
- Spectral Freeze of harmonic material

#### rt.low
- Grind
- Fuzz
- Fb

#### rt.disruptor
- Cubic Non-Linear Distortion
- Granular Synthesis
- Fourses Synthes


At this point I had finalised choices in regards to the sound processing techniques that I wanted to use and was certain about those aspects which would remain in the patch. However, high-level form was a concern to me, and in this regard the patch contained no mechanisms for creating and managing long term form. I did not have strong ideas about techniques that I wanted to use, and again I think that this difficulty emphasises an incompatibility with the way that I want to use technology in composition. A similar "friction" arose in [Stitch/Strata]({m.ss}) and is discussed in ["From Modelling to Assistance"]({m.ss}#from-modelling-to-assistance). 

Much of my thinking up to this point was that the formal aspects of the work would be governed by the state of the modules. Switching them on and off, or modifying parameters were to me the only way to shape the nature of the electronics, subsequently influencing the performer and thus the overall piece. Each module had been designed and conceived of with certain material types in the first place, so I theorised that a "state" detector that could alter the internal components of the patch in response to the input of the guitar signal could be a way that the machine could be dynamic, while also being cohesive with the performer. In addition to this, I envisaged that this strategy might introduce a level of flexibility for the performer and provide them with a way to interact with the computer in order to influence the form of the piece. Depending on their in-the-moment assessment and listening, the performer could decide to maintain a single idea for as long as needed or explore the localised behaviours of a state before moving on at their own pace. From the inverse perspective, I also imagined that this would give the computer some ability to be more divergent in the construction of form in real-time by creating a kind of "game", whereby the performer is working either with or against the behaviour of that classification process.

To implement this, I used the [mubu](https://ismm.ircam.fr/mubu/) Max package, specifically the `mubu.gmm` object. This object uses a gaussian mixture model to compare live input data against a set of trained exemplars and produces a series of values that determine the confidence it has that the incoming data is represented by the trained exemplars. I used mel-frequency cepstrum coefficients derived by analysing audio files of extended techniques played provided by Daryl to create some fixed musical behaviours as exemplars. 

<VideoMedia>
 <video slot="media" controls loop>
 <source src="/rt/gmm-demo.mp4" type="video/mp4"> 
 <p>Your browser doesn't support HTML5 video. Here is a <a href="/rt/gmm-demo.mp4">link to the video</a> instead.</p> 
 </video>
 <p slot="caption">VIDEO 1: Gesture Recognition with mubu.gmm.</p>
</VideoMedia>

Mapping to modules by using a 

<!-- DEMO: recognition process -->

Temperamental inbetween states and hard to manually explore that concept as a performer. While there might be a data representation which is indicative of an inbetween-ness, the physical reality of performing that might be more difficult or impossible to rectify. I wanted to work more closesly with that idea, rather than discerete states, but realised through testing this idea that it was more complex than it initially seemed and perhaps was an ill-proposed problem.

Not enough variation between the ideas I had.

In experiments I found that the patch felt like a follower, more than having its own behaviour and conitrbution.

What I found through this process though was the idea that the piece could be constructed as "panels" in which a specific idea is explored for a flexible amount of time with some constraints. 

balancing generalisation and specific control.

### Final Version
The final version of the patch in many aspects was not much different from version 4. Many changes were not musical in nature, and were changes to ensure that the patch would not break in performance. The most significant aspect that came to fruition in the final version was a sub-process that dealt with time and form through the aforementioned states. In previous versions of the patch the notion of states had been present as an idea, and some experiments had taken place in using these states as a way of pushing the patch and thus the piece into new sonic and behavioural territories automatically. There was some success in this approach, but I found myself increasingly worried towards the performance date that controlling this aspect would be necessary for both rehearsing the piece in a meaningful way, and as a way of guaranteeing to an extent that certain moments and things would emerge in the performance despite being highly improvised.

In the final version of the patch the energy accumulator approach is taken to the extreme and used as a way of determining the duration that a given "state" will be engaged for in the patch. 
 
 These "states" are predetermined by me beforehand and control which sound producing modules are active and the parameters across those modules. Importantly too, I introduced a web-based interface for which instructions could be delivered to the performer. My thinking with this, would be that the states would not just provoke aural responses from him, but that he would also be given visual feedback on the progress through a particular state, amongst a set of guiding instructions on techniques to use, what dynamic range to occupy and what he should focus on listening to in the computers output.

Material was only immediate in the granulation. I wanted it to sustain and so I introduced a feedback component to the granulation which made it behave less dependently on the input signal

State recognition took on a new more deterministic form.

Higher level of pre-composition, because it was difficult to experiment without Daryl there physically. Try to keep things with a relatively high chance to succeed and not have to re-work it only a day or so before.




<!-- Here is where you would talk about the recording of the piece, and point to each section as a breakdown and what happened in the music... -->

This approach represents is a major digression from previous approaches and the computer has little role in influencing the work. Instead, it is more like a static object that the guitarist must be aware of. If a section only has a limited amount of energy to spare in the accumulator then playing quietly is the only way to make sure that state lasts for an appropriate amount of time. Within each state there is opportunity for the guitarist and the computer to interact with each other and many of the reactive features in each sound producing module is retained from the previous versions. For example, the granular synthesis module is fea

## Reflection

- Much of my thinking in machine listening was based on modules being responsive to very specific aspects of playing, while Daryl's style and approach is to explore broad sonic territories with more defined gestural behaviours. In other words, I had implemented lots of ways to measure things I might be interested in, not necessarily what Daryl was interested in and even trying to measure gestural aspects may have been impossible or another approach not explored entirely.

- Hard to understand the long term side effects of setting up certain interactions

- Operating on recorded materials to make a real-time work is not conducive 

- Issues of workflow

- Working with a performer introduces a number of concerns that make it difficult to focus on computer agency.
 - Something that requires a lot of time and effort

- Controlling the material is much harder, so you have to "omnivorous" sound modules that will accept almost anything and do something "good" with it, or constrain the material and really limit what the performer can do.

<!-- WIDGET: give examples -->
- After listening back to the rehearsals, the dress rehearsal and gig recording I realised that each recording had several moments where the electronics and Daryl coalesced to create sophisticated musical structures that I could not possibly plan and construct manually in the DAW. Despite this, these moments were not controllable or re-creatable due to the sheer number of parameters and constraints which give rise to such things as well as the contingent nature of the processes which modulated the electronics on a local scale. I realised from this that while I want the computer to influence the emergence of such serendipitous musical results, I need to be able to store these and modify them and to have methods of iterating on points of progress in the compositional progress. This methodology of live electronics and improvisation is not a way forward if I want to do that.



- I would probably constrain the material and limit what the computer can do to make a shorter and more focused piece.
 - While going back to the max patches, video documentation and original source material that inspired me with Daryls playing I now see what this material might have been (2.wav)

### Difficulties Modelling Form in Interaction
### Intervention is Essential

<NextSection 
next="Reconstruction Error"
link={m.re}
/>
 
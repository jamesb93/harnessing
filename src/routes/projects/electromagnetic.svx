<script>
	import NextSection from "$lib/components/NextSection.svelte"
    import {metadata as m} from "../directory.svx"
    import VideoMedia from '$lib/components/VideoMedia.svelte';
    import VideoMedia2 from '$lib/components/VideoMedia2.svelte';
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
    import YouTube from '$lib/components/YouTube.svelte';
    import Album from '$lib/components/Album.svelte';
    import Code from '$lib/components/Code.svelte';
    import Waveform from '$lib/components/Waveform.svelte'; 

    import { 
        interAlbum, 
        kindleModes, 
        xiaomiModes,
        xboxModes,
        classifier
    } from '$lib/data/interferences.js';
</script>

# Interferences

<Album
tracks={interAlbum}
title='Interferences'
id='audio1'
figure='AUDIO 1'
/>

Interferences is the fifth and final project in this portfolio. It is a pair of two medium length works presented as an EP. There is a git repository which contains all of the Python code and Max patches which were used. This can be found at: https://github.com/jamesb93/interferences

## Motivations and Influences
A major motivation in this project was to develop the workflow I had established in [Reconstruction Error]({m.re}) and to build on my creative coding practice with Python, Max, REAPER and [FTIS]({m.ftis}). There were two aspects to this motivation. Firstly, I wanted to recreate the experience of navigating through a corpus of unknown materials led by the computer through *structured listening* as I had done in [Reconstruction Error]({m.re}). Although I only utilised a small portion of the databending corpus, I wanted to generate a new corpus with its own origins, whilst still situating my work within a digital and synthetic aesthetic. At the time, I had been interested in [circuit bending](https://en.wikipedia.org/wiki/Circuit_bending) and the vast number of sounds that breaking apart and reconstructing old electronics could generate. The aesthetic of these sounds suits my sensibilities well, and I imagined that collecting a number of them could be a path forward to creating the corpus that I would need for composition. Tangential to this research, I came across a video of Nicolas Collins using an induction microphone to amplify the sounds of electromagnetic interferences generated by everyday appliances and objects. This video can be seen in [VIDEO 1]({m.em}#vid1). It struck me that instead of circuit bending, this might be a suitable technique for creating a corpus with the properties I desired. Following this hunch I obtained an [Elektrosluch Mini induction microphone](https://store.lom.audio/products/elektrosluch-mini-city-diy-kit-1?variant=4542176460832) capable of recording electromagnetic waves. I describe how this device was instrumental in the compositional process further in ["Recording Objects With Transduction"]({m.em}#recording-objects-with-transduction).

<YouTube 
url="https://www.youtube.com/embed/4T7qkYY7LZM"
caption='Nicolas Collins demonstrating induction to record electromagnetic fields generated by everyday electrical appliances.'
figure='VIDEO 1'
id='vid1'
/>

Secondly, I wanted to iterate on using the connected process of segmentation, analysis, dimension reduction and clustering and apply it more fluidly to compositional materials. The technology implemented in [Reconstruction Error]({m.re}) was very successful to me creatively, but was largely inflexbile and to perform the same tasks on new material would require rewriting the scripts. Throughout this project, [FTIS]({m.ftis}) supported this venture, and made it possible create  scripts that supported experimentation and data manipulation. This aspect is discussed in more detail in ["Pathways Through The Corpus"]({m.em}#pathways-through-the-corpus).


## Composition Process

The composition process can roughly be divided into  two phases of composition. The first phase involved a lot of experimentation with [FTIS]({m.ftis}) implementing various ways navigating through the corpus using machine learning and listening.  In the second phase, two pieces emerged out of these experiments and instead of using the computer to explore more broadly, I used it to search for and hone in on specific materials within the corpus of electromagnetic sounds. 

### Recording Objects With Induction

<!-- REVIEW: do i need to give the corpus here for listening maybe? -->
As discussed in ["Motivations And Influences"]({m.em}#motivations-and-influences), I procured an electromagnetic induction microphone and wanted to use it to generate a corpus of digital and synthetic type sounds. To do this, I took electronic objects from around the home and recorded them, at times while operating them simultaneously. I was surprised by the diversity of sounds this produced. Despite the devices seeming dormant, the induction recording technique could detect and uncover their invisible mechanisms and operation. I recorded a number of devices, including my computer keyboard, mouse, a mobile phone, a sound card, a laptop and an e-reader. Each device produced its own characteristic sounds - and some of these objects offered me the ability to trigger certain sonic behaviours. For example, [AUDIO 2]({m.em}#aud2) captures the sound of the e-reader in a dormant state. The change at 0:02 is triggered by switching the aeroplane mode on or off. This user interaction causes the components in the e-reader to operate differently, which is captured by the induction microphone. Eventually the initial static state is returned to. This can be heard at 1:59 lasting until the end of the clip.

<Waveform 
file='/inter/kindle-gesture.mp3'
peaks='/inter/kindle-gesture.dat'
id='aud2'
title='Induction recording of e-reader while switching the aeroplane mode on and off'
caption='AUDIO 2'
segments={kindleModes}
/>

Other objects produced their own characteristic material. I found that across all of the samples, my interpretation would always result in some kind of distinction between *active* and *passive* states.  For example, [AUDIO 3]({m.em}#aud3) is extracted from the electromagnetic interferences of a mobile phone. I left the microphone to record this device for some time. To me, I could divided the sounds of this into these two classes, where 0:00 to 0:40 is *static*, and the remainder of the recording is *active*.

<Waveform
file='/inter/xiaomi-gesture.mp3'
peaks='/inter/xiaomi-gesture.dat'
id='aud3'
title='Induction recording of mobile phone without user interaction'
caption='AUDIO 3'
segments={xiaomiModes}
/>

As I recorded and browsed through these sounds, I was reminded of the quasi-documentary work of Australian field-recording artist Jay-Dea Lopez and the way he taps into "unseen" sources using contact microphones attached to monolithic infrastructural objects in the outback. In particular, two blog posts of his, ["Low Frequencies"](https://soundslikenoise.org/2019/09/07/low-frequencies/) and ["Coil pickups and microsounds"](https://soundslikenoise.org/2020/08/16/coil-pickups-and-microsounds) contain these types of material. Similarly, I felt a certain connection between my experiments and the morphologies and textures present in Bernhard Gunter's *Untitled I/92* ([VIDEO 2]({m.em}#vid2)), and Alvin Lucier's *Sferics* ([VIDEO 3]({m.em}#vid3)). Both of these pieces to me encapsulate the essence and character of what I was discovering with the electromagnetic microphone - delicate and fragile sound worlds full of intimate clicks, pops and spectrally constrained fragments. Furthermore, these external musical references started to funnel into my conceptualisation of the initial material as that which can occupy two distinct states - busy and rapidly changing or unchanging and glacial in nature. This became an essential starting point in the technological process of dissecting these sounds in order to become more familiar with them.

<YouTube 
url="https://www.youtube.com/embed/QZfMp6tredw"
caption="Untitled I/92  from Un Peu De Neige Salie by Bernhard Gunter"
figure='VIDEO 2'
id='vid2'
/>

<YouTube 
url="https://www.youtube.com/embed/rxUvMl_IxoQ"
caption="Sferics by Alvin Lucier"
figure="VIDEO 3"
id="vid3"
/>

### Static Versus Gestural Material
I began by trying to dissect the corpus into two groups based on classifying whether they were *static* or *active*. To do this, I first had to segment the corpus, and then find a way of crudely sorting it into either of these two classifications.

#### Segmentation
The goal of segmentation was to automatically divide each sound into *salient* sections. *Salience*, in this context, was not yet based on creating divisions into *static* and *active* classes yet, rather to achieve an automatic division of each file into perceptually heterogenous sounds. After this process, I planned to apply a classification or clustering analysis so that the computer could discern which group each segment might belong to. There are complicated aspects to this though as the time span of static and active states, in my perception, varied a lot depending on the file. [AUDIO 2]({m.em}#aud2) only shows a portion of that e-reader recording, and the static and active states last for up to minutes between alternations. [AUDIO 4]({m.em}#aud4) shows another recording taken from a wireless game controller, which demonstrates similar alternations between stasis and activity. These changes occur on a much smaller time scale, in the region of seconds rather than minutes. In this context, a frame-by-frame analysis was likely to be unsuitable to this range of sounds and hard to tune appropriately to capture the sparse and specific moments in time in longer samples, while also being sensitive to the shorter-term changes.

<Waveform 
file='/inter/xbox-gesture.mp3'
peaks='/inter/xbox-gesture.dat'
caption='AUDIO 4'
title='Shorter induction recording taken from a game controller.'
id='aud4'
segments={xboxModes}
/>

To tackle this, I created a bespoke algorithm influenced by Lef√®vre, S., & Vincent, N. (2011) which performs segmentation by sampling short sequences within a sound and clustering those sequences together. In my algorithm, I perform a two phase process. The first phase, *over-segments* a sample into numerous small divisions. The second phase applies audio descriptor analysis to each segment and then groups each one into a user-specified number of clusters. From this, the algorithm recursively operates on a specified number of windows of clustered segments and merges contiguous segments with the same cluster label. It starts at the first segment in time and performs this by shifting forward in time, merging inside the loop. As such, it gradually creates larger and larger combined segments until consecutive segments all belong to different clusters.

This approach avoids the situation where I have to find a suitable combination of settings as there are fewer parameters to deal and when it is required they are much more general in scope. It also avoids some of the issues with frame-by-frame approaches to segmentation, which often only target short windows of time and do not respect the longer term implications of changes in the signal necessarily. By using a shifting window that merges while it slides across the segments, it gives the algorithm a "lookahead" and "look behind" that constantly updates as it goes.

I experimented with my implementation in a number of ways, by changing the initial over-segmentation algorithm, the audio descriptor analysis and then the window size and number of clusters. I tested this visually and aurally by running the algorithm with these underlying components being modified and exchanged and rendering the results as a REAPER session. Overall, I found that the choice of segmentation algorithm did not really matter and had little effect on the final result. What had the most significance though was the initial granularity of the segments. If the segments were too large the algorithm would usually find contiguous segments to be quite similar as the analysis often regressed to the mean. It was important to begin a high level of differentiation between segments to avoid this. The window size and number of clusters had a bigger effect on the results. In preliminary tests I found that they could diverge significantly in unexpected ways. Unlike changing a threshold for an amplitude-based slicer for example, which as it is lowered one expects to see more clusters emerge, the combination of window size and number of clusters presented a non-linear spectrum of segmentations. [IMAGE 1]({m.em}#img1) depicts such a REAPER session, where each track is the result of a different combination of these two parameters. The session itself can also be found in the repository here: [session.rpp](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation_scripts/2020-06-18%2004-09-54.718681/session.rpp) and the code for these experiments can¬†be located here: [clustered_segmentation.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation_scripts/clustered_segmentation.py).

<ImageMedia2 
url='/inter/clusterd-segmentation.jpg'.
figure='IMAGE 1'
caption='Clustered segmentation results rendered as a REAPER session.'
id='img1'
/>

Some of the results from these experiments were not satisfactory, while others created results which were much more suitable. I decided to make this algorithm into a [FTIS]({m.ftis}) analyser. The concept of analysers is described in ["Analysers"]({m.ftis}#analysers), a part of the technical implementation section. I then ran the algorithm on all of the induction recordings using this script: [segmentation.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation.py). The results from this were satisfactory to start, but not necessarily perfect. This was not an issue though as I planned to do further segmentation later to further separate the material. At this point it only had to be a general division of the sounds in time to facilitate clustering static and active sounds.

#### Classification Through Clustering
I devised a strategy for classifying each of these newly generated segments into two distinct groups. I did this by calculating the statistics of spectral flux across windowed frames in each sample and then clustering each this data using the Agglomerative Clustering algorithm. The code for this can be found here: [split_by_activity.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/split_by_activity.py). To me this process was perceptually successful and potent for dividing sounds into static and active groups. A selection of different results from this process are presented in [AUDIO 5]({m.em}#aud5).

<Album 
tracks={classifier}
id='aud5'
title='Several classification results.'
figure='AUDIO 5' 
/>

### Pathways Through The Corpus
Once I had produced this classification of the segmentation, I wanted to begin exploring this data in order to spark compositional action. As such, this next section describes different *pathways* through the corpus of segmented and classified sounds I created, and experiments which led to the compositional works. In hindsight, the technological experimentation in this project was much more bound up in *soundful* forms of practice. Largely, this is because [FTIS]({m.fits}) enabled a workflow where I could fluidly move between REAPER, Max and scripting different machine learning and content-aware programs.

To begin with I was interested mostly in the sounds which were classified as *active*

- micro-clustering
    - HDBSCluster Exploration
        - Mechanised nature produced by looping, while still maintaining a sense that it could have been generated from machinery or by electromagnetic interference.
        - Cluster 37 note??
        - Cluster 18 (quiet sounds) > sorting by quietness > hinting at rhythmic pulsation at times but never committing to it fully > convolution anchors
        - Cluster 33
- Sorting by quietness
    - Using FTIS's corpus filtering to create pockets of material
    - Don't have to deal with a whole corpus, start with a subsection and then operate on that. very powerful and flexible compared to 
    - This would be instrumental later in finding material for Anchors

- REAPER exporting and "sketching"
    - cluster 67
    - idea2-loops-iteration.RPP
    
#### Mixing Sounds From [Reconstruction Error]({m.re}) MULTICORPUS

Felt like I was had explored a lot of the corners of the corpus and the material was unsurprising. I could discern what I liked quite rapidly and knew what it belonged to, whereas in [Reconstruction Error]({m.re}), I always felt like there was more corpus to dive into when I ran out of ideas or felt stuck compositionally. 

As such

- This demonstrated the expressive power of FTIS, in that I could rapidly combine corpora and begin making connections between different parts of my practice. (Quietest.RPP for example)


Som other tangential approaches, although mostly dead ends. I kinda just found something that worked... the dead ends are important. It's not about finding one right approach, but honing in on what is important to me through discovery ...

### Anchors (placeholder)
The connection here is the multicorpus approach.


Developing anchors using a searching process and composing from there. A main source of inspiration is the juxtaposition and superimposition of stationary textures. Many of the initial discoveries were made by looking suitable for convolution, ideally pitched and quasi-textural in nature. There are also numerous notes above that show the development of the source material.

convolution_base_materials.py - take static sounds and create three clusters out of this

convolution_three_anchors.py - took three sounds 
convolution_find_tuned.py - find more pitchy sounds using CQT and kdtree to branch outward from known samples to get 200 more samples.

https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/MultiCorpus/scripts/convolution_three_anchors.py

Sorting these 200 samples by centroid using ReaCoMa

A mid-way version of the piece emerged from intuitively moving these samples around. I tried to create complex 

<ImageMedia2 
url='/inter/anchors-mid-way-session.png'
/>

<Waveform 
file='/inter/28-10-20.mp3'
peaks='/inter/28-10-20.dat'
/>

For this particular track 'anchors', I want the form to almost feel shapeless in the long term. I don't want it to feel like its necessarily pushing toward a climatic point, rather that the music exists in a holding pattern and has ebbs and flows of material within that pattern. This is also part of making the material work musically, I have a quasi-conceptual idea of the sounds being reconstructed into the machines from which they were extracted. Part of frustrations in databending was the fact that the forms felt quite similar because I was focusing on stringing together various meso-form ideas without much consideration for how they would exist as a long term structure. By the time I got to the natural conclusion of each piece's materials in the composition process I had already crystallised a structure that wasn't that malleable.

The Gunter meanders and hovers and restrains from developing intentionally to heighten the suspense.



### EEE (placeholder)
- Take the KNN approach and apply it so other material. Building up a piece around a single sample.
- [isolate_static.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/MultiCorpus/scripts/isolate_sample.py)
- Finding what material from the databending corpus was close to 06-Kindle Off-200513_1547-glued-04.wav
- Focus on disintegrating the kindle sample and finding connections to the disintegrated parts
- 07-200622_1602.wav composing around the morphology

For this particular track 'anchors', I want the form to almost feel shapeless in the long term. I don't want it to feel like its necessarily pushing toward a climatic point, rather that the music exists in a holding pattern and has ebbs and flows of material within that pattern. This is also part of making the material work musically, I have a quasi-conceptual idea of the sounds being reconstructed into the machines from which they were extracted. Part of frustrations in databending was the fact that the forms felt quite similar because I was focusing on stringing together various meso-form ideas without much consideration for how they would exist as a long term structure. By the time I got to the natural conclusion of each piece's materials in the composition process I had already crystallised a structure that wasn't that malleable.

## Reflection

Frustration drives technological development - and technological development drives compositional innovation and breakthroughs. They feed off each other. Anchors only came about because of the multi-corpus approach, which was enabled by extending FTIS. I was thinking about gathering material in a specific way (*branching* out from a single compositional anchor towards similar sounds) which gave rise to the knn method of searching. This sparked the idea for the Kindle active gesture to be used, which I had an appreciation before at the verys tart of listening but didn't know how to incorporate it.

Aestheticise the unheard and procedural sounds of technology. Kindle on/off thing.

- Mouse-2.wav sample becoming a pitched motif.
 - Xiaomi sample in Anchors
- The way that my listening is structured through technology leads me to these kinds of concepts.



### Using the computer to work from more knowns in composition
- The electromagnetic stuff creates a foundation, a base from which I created material links elsewhere and drew into single compositions.
### Towards a stable workflow

Would have I been able to segment the material at the scale it existed at, analyse it and do the other various processes without the computer?

<NextSection 
next="Technical Implementation and Software"
link={m.ti}
/>
    
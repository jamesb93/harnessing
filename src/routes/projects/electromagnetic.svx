<script>
	import NextSection from "$lib/components/NextSection.svelte"
    import {metadata as m} from "../directory.svx"
    import VideoMedia from '$lib/components/VideoMedia.svelte';
    import VideoMedia2 from '$lib/components/VideoMedia2.svelte';
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
    import YouTube from '$lib/components/YouTube.svelte';
    import Album from '$lib/components/Album.svelte';
    import Code from '$lib/components/Code.svelte';
    import Waveform from '$lib/components/Waveform.svelte'; 

    import { 
        interAlbum, 
        kindleModes, 
        xiaomiModes,
        xboxModes,
        classifier,
        loopExperiments
    } from '$lib/data/interferences.js';
</script>

# Interferences

<Album
tracks={interAlbum}
title='Interferences'
id='audio1'
figure='AUDIO 1'
/>

Interferences is the fifth and final project in this portfolio. It is a pair of two medium length works presented as an EP. There is a git repository which contains all of the Python code and Max patches which were used. This can be found at: https://github.com/jamesb93/interferences

## Motivations and Influences
A major motivation in this project was to develop the workflow I had established in [Reconstruction Error]({m.re}) and to build on my creative coding practice with Python, Max, REAPER and [FTIS]({m.ftis}). There were two aspects to this motivation. Firstly, I wanted to recreate the experience of navigating through a corpus of unknown materials led by the computer through *structured listening* as I had done in [Reconstruction Error]({m.re}). Although I only utilised a small portion of the databending corpus, I wanted to generate a new corpus with its own origins, whilst still situating my work within a digital and synthetic aesthetic. At the time, I had been interested in [circuit bending](https://en.wikipedia.org/wiki/Circuit_bending) and the vast number of sounds that breaking apart and reconstructing old electronics could generate. The aesthetic of these sounds suits my sensibilities well, and I imagined that collecting a number of them could be a path forward to creating the corpus that I would need for composition. Tangential to this research, I came across a video of Nicolas Collins using an induction microphone to amplify the sounds of electromagnetic interferences generated by everyday appliances and objects. This video can be seen in [VIDEO 1]({m.em}#vid1). It struck me that instead of circuit bending, this might be a suitable technique for creating a corpus with the properties I desired. Following this hunch I obtained an [Elektrosluch Mini induction microphone](https://store.lom.audio/products/elektrosluch-mini-city-diy-kit-1?variant=4542176460832) capable of recording electromagnetic waves. I describe how this device was instrumental in the compositional process further in ["Recording Objects With Transduction"]({m.em}#recording-objects-with-transduction).

<YouTube 
url="https://www.youtube.com/embed/4T7qkYY7LZM"
caption='Nicolas Collins demonstrating induction to record electromagnetic fields generated by everyday electrical appliances.'
figure='VIDEO 1'
id='vid1'
/>

Secondly, I wanted to iterate on using the connected process of segmentation, analysis, dimension reduction and clustering and apply it more fluidly to compositional materials. The technology implemented in [Reconstruction Error]({m.re}) was very successful to me creatively, but was largely inflexbile and to perform the same tasks on new material would require rewriting the scripts. Throughout this project, [FTIS]({m.ftis}) supported this venture, and made it possible create  scripts that supported experimentation and data manipulation. This aspect is discussed in more detail in ["Pathways Through The Corpus"]({m.em}#pathways-through-the-corpus).


## Composition Process

The composition process can roughly be divided into  two phases of composition. The first phase involved a lot of experimentation with [FTIS]({m.ftis}) implementing various ways navigating through the corpus using machine learning and listening.  In the second phase, two pieces emerged out of these experiments and instead of using the computer to explore more broadly, I used it to search for and hone in on specific materials within the corpus of electromagnetic sounds. 

### Recording Objects With Induction

<!-- REVIEW: do i need to give the corpus here for listening maybe? -->
As discussed in ["Motivations And Influences"]({m.em}#motivations-and-influences), I procured an electromagnetic induction microphone and wanted to use it to generate a corpus of digital and synthetic type sounds. To do this, I took electronic objects from around the home and recorded them, at times while operating them simultaneously. I was surprised by the diversity of sounds this produced. Despite the devices seeming dormant, the induction recording technique could detect and uncover their invisible mechanisms and operation. I recorded a number of devices, including my computer keyboard, mouse, a mobile phone, a sound card, a laptop and an e-reader. Each device produced its own characteristic sounds - and some of these objects offered me the ability to trigger certain sonic behaviours. For example, [AUDIO 2]({m.em}#aud2) captures the sound of the e-reader in a dormant state. The change at 0:02 is triggered by switching the aeroplane mode on or off. This user interaction causes the components in the e-reader to operate differently, which is captured by the induction microphone. Eventually the initial static state is returned to. This can be heard at 1:59 lasting until the end of the clip.

<Waveform 
file='/inter/kindle-gesture.mp3'
peaks='/inter/kindle-gesture.dat'
id='aud2'
title='Induction recording of e-reader while switching the aeroplane mode on and off'
caption='AUDIO 2'
segments={kindleModes}
/>

Other objects produced their own characteristic material. I found that across all of the samples, my interpretation would always result in some kind of distinction between *active* and *passive* states.  For example, [AUDIO 3]({m.em}#aud3) is extracted from the electromagnetic interferences of a mobile phone. I left the microphone to record this device for some time. To me, I could divided the sounds of this into these two classes, where 0:00 to 0:40 is *static*, and the remainder of the recording is *active*.

<Waveform
file='/inter/xiaomi-gesture.mp3'
peaks='/inter/xiaomi-gesture.dat'
id='aud3'
title='Induction recording of mobile phone without user interaction'
caption='AUDIO 3'
segments={xiaomiModes}
/>

As I recorded and browsed through these sounds, I was reminded of the quasi-documentary work of Australian field-recording artist Jay-Dea Lopez and the way he taps into "unseen" sources using contact microphones attached to monolithic infrastructural objects in the outback. In particular, two blog posts of his, ["Low Frequencies"](https://soundslikenoise.org/2019/09/07/low-frequencies/) and ["Coil pickups and microsounds"](https://soundslikenoise.org/2020/08/16/coil-pickups-and-microsounds) contain these types of material. Similarly, I felt a certain connection between my experiments and the morphologies and textures present in Bernhard Gunter's *Untitled I/92* ([VIDEO 2]({m.em}#vid2)), and Alvin Lucier's *Sferics* ([VIDEO 3]({m.em}#vid3)). Both of these pieces to me encapsulate the essence and character of what I was discovering with the electromagnetic microphone - delicate and fragile sound worlds full of intimate clicks, pops and spectrally constrained fragments. Furthermore, these external musical references started to funnel into my conceptualisation of the initial material as that which can occupy two distinct states - busy and rapidly changing or unchanging and glacial in nature. This became an essential starting point in the technological process of dissecting these sounds in order to become more familiar with them.

<YouTube 
url="https://www.youtube.com/embed/QZfMp6tredw"
caption="Untitled I/92  from Un Peu De Neige Salie by Bernhard Gunter"
figure='VIDEO 2'
id='vid2'
/>

<YouTube 
url="https://www.youtube.com/embed/rxUvMl_IxoQ"
caption="Sferics by Alvin Lucier"
figure="VIDEO 3"
id="vid3"
/>

### Static And Active Material
I began by trying to dissect the corpus into two groups based on classifying whether they were *static* or *active*. To do this, I first had to segment the corpus, and then find a way of crudely sorting it into either of these two classifications.

#### Segmentation
The goal of segmentation was to automatically divide each sound into *salient* sections. *Salience*, in this context, was not yet based on creating divisions into *static* and *active* classes yet, rather to achieve an automatic division of each file into perceptually heterogenous sounds. After this process, I planned to apply a classification or clustering analysis so that the computer could discern which group each segment might belong to. There are complicated aspects to this though as the time span of static and active states, in my perception, varied a lot depending on the file. [AUDIO 2]({m.em}#aud2) only shows a portion of that e-reader recording, and the static and active states last for up to minutes between alternations. [AUDIO 4]({m.em}#aud4) shows another recording taken from a wireless game controller, which demonstrates similar alternations between stasis and activity. These changes occur on a much smaller time scale, in the region of seconds rather than minutes. In this context, a frame-by-frame analysis was likely to be unsuitable to this range of sounds and hard to tune appropriately to capture the sparse and specific moments in time in longer samples, while also being sensitive to the shorter-term changes.

<Waveform 
file='/inter/xbox-gesture.mp3'
peaks='/inter/xbox-gesture.dat'
caption='AUDIO 4'
title='Shorter induction recording taken from a game controller.'
id='aud4'
segments={xboxModes}
/>

To tackle this, I created a bespoke algorithm influenced by Lefèvre, S., & Vincent, N. (2011) which performs segmentation by sampling short sequences within a sound and clustering those sequences together. In my algorithm, I perform a two phase process. The first phase, *over-segments* a sample into numerous small divisions. The second phase applies audio descriptor analysis to each segment and then groups each one into a user-specified number of clusters. From this, the algorithm recursively operates on a specified number of windows of clustered segments and merges contiguous segments with the same cluster label. It starts at the first segment in time and performs this by shifting forward in time, merging inside the loop. As such, it gradually creates larger and larger combined segments until consecutive segments all belong to different clusters.

This approach avoids the situation where I have to find a suitable combination of settings as there are fewer parameters to deal and when it is required they are much more general in scope. It also avoids some of the issues with frame-by-frame approaches to segmentation, which often only target short windows of time and do not respect the longer term implications of changes in the signal necessarily. By using a shifting window that merges while it slides across the segments, it gives the algorithm a "lookahead" and "look behind" that constantly updates as it goes.

I experimented with my implementation in a number of ways, by changing the initial over-segmentation algorithm, the audio descriptor analysis and then the window size and number of clusters. I tested this visually and aurally by running the algorithm with these underlying components being modified and exchanged and rendering the results as a REAPER session. Overall, I found that the choice of segmentation algorithm did not really matter and had little effect on the final result. What had the most significance though was the initial granularity of the segments. If the segments were too large the algorithm would usually find contiguous segments to be quite similar as the analysis often regressed to the mean. It was important to begin a high level of differentiation between segments to avoid this. The window size and number of clusters had a bigger effect on the results. In preliminary tests I found that they could diverge significantly in unexpected ways. Unlike changing a threshold for an amplitude-based slicer for example, which as it is lowered one expects to see more clusters emerge, the combination of window size and number of clusters presented a non-linear spectrum of segmentations. [IMAGE 1]({m.em}#img1) depicts such a REAPER session, where each track is the result of a different combination of these two parameters. The session itself can also be found in the repository here: [session.rpp](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation_scripts/2020-06-18%2004-09-54.718681/session.rpp) and the code for these experiments can be located here: [clustered_segmentation.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation_scripts/clustered_segmentation.py).

<ImageMedia2 
url='/inter/clusterd-segmentation.jpg'.
figure='IMAGE 1'
caption='Clustered segmentation results rendered as a REAPER session.'
id='img1'
/>

Some of the results from these experiments were not satisfactory, while others created results which were much more suitable. I decided to make this algorithm into a [FTIS]({m.ftis}) analyser. The concept of analysers is described in ["Analysers"]({m.ftis}#analysers), a part of the technical implementation section. I then ran the algorithm on all of the induction recordings using this script: [segmentation.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation.py). The results from this were satisfactory to start, but not necessarily perfect. This was not an issue though as I planned to do further segmentation later to further separate the material. At this point it only had to be a general division of the sounds in time to facilitate clustering static and active sounds.

#### Classification Through Clustering
I devised a strategy for classifying each of these newly generated segments into two distinct groups. I did this by calculating the statistics of spectral flux across windowed frames in each sample and then clustering each this data using the Agglomerative Clustering algorithm. The code for this can be found here: [split_by_activity.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/split_by_activity.py). To me this process was perceptually successful and potent for dividing sounds into static and active groups. A selection of different results from this process are presented in [AUDIO 5]({m.em}#aud5).

<Album 
tracks={classifier}
id='aud5'
title='Several classification results.'
figure='AUDIO 5' 
/>

### Pathways Through The Corpus
Once I had produced this classification of the segmentation, I wanted to begin exploring this data in order to spark compositional action. As such, this next section describes different *pathways* through the corpus of segmented and classified sounds I created, and experiments which led to the compositional works. In hindsight, the technological experimentation in this project was much more bound up in *soundful* forms of practice. Largely, this is because [FTIS]({m.fits}) enabled a workflow where I could fluidly move between REAPER, Max and scripting different machine learning and content-aware programs.

To begin with I was interested mostly in the sounds which were classified as *active* and how these could be broken down further into individual impulses, and *micro-gestures*. For example, I imagined that [AUDIO 6]({m.em}#aud6), a segment from *Kindle_04_08.wav*, might be restructured to be less chaotic and more ordered - or perhaps a rhythmic structure could be imposed on to individual segments extracted from this. 

<Waveform 
file='/inter/kindle-04-8-seg.mp3'
peaks='/inter/kindle-04-8-seg.dat'
id='aud6'
figure='AUDIO 6'
title='Kindle_04_08.wav segment'
/>

I used [FTIS]({m.ftis}) to segment the *active* sounds and then clustered these segments based on their perceptual similarity. This was orchestrated with two separate [FTIS]({m.ftis}) scripts for. This [segmentation script](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/micro_segmentation.py) uses the clustered segmentation algorithm to create very granular slices. This [clustering script](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/micro_clustering.py) calculates MFCC descriptors for each segment and clusters them into an unspecified number of clusters using the HDBSCAN algorithm. I explored the results aurally in Max, similar to how I had explored the corpus in [Reconstruction Error]({m.re}). From this exploration I was presented with perceptually homogenous groups in terms of texture and morphology. [VIDEO 4]({m.em}#vid4) I captures some of this audition-based exploring.

<VideoMedia2 
url='/inter/exploring-clusters.mp4'
caption='Exploring segmented and clustered active material'
figure='VIDEO 4'
id='vid4'
/>

This Max patch used for exploring looped a selected sample from a cluster. As I navigated between clusters and samples within them, the intricacy and fragility of certain clusters was foregrounded. Curiously too, some material that I perceived as static had been included in these clusters. The nature of the looping playback, combined with already mechanised and artificial nature of these sounds was evocative and I was drawn to different groups - imagining composing with the segments to form my own artificial machine-like sounds composites. Clusters 1, 2, and 37, for example, produced structured rhythmic sounds articulated by sparse and subtle transients. When I was changing the sample while exploring, the rhythmic patterns of these articulations could be altered.  I imagined that combining several of these could be useful in constructing layers of  intricate quasi-rhythmic patterns. Clusters 0, 11 and 18 contained a large amount of drone-like static material with a prominent pitch content. To an extent, I was thankful this material had been poorly grouped into the *active* classification causing it to be discovered in this exploring process. The playback of short segments of such material highlighted how it could be transformed from its originally directionless form into something else perhaps.

For me, this was a starting point at which I began to conceive of the material in the corpus as two distinct types that I could treat discretely in composition. I imagined those sounds with more rhythmic and iterated qualities could form a piece in this EP, while those with drone-like qualities might form another. This sparked further utilisation of [FTIS]({m.ftis}) to further hone in on these two types of material and to become more familiar with them. These experiments did not necessarily lead to concrete pieces or incremental stages of the final compositions, rather, they acted as different limited *tests* designed to provoke me through further listening.

#### Ad-Hoc Experiments

One such experiment was to run similar clustering processes on a subset of the corpus, derived by filtering each item according to its average loudness. The aim of this was to hone in on those faint, drone-like sounds and to purify the corpus towards only those sounds and away from those more rhythmic sounds. This was facilitated by extending [FTIS]({m.ftis}) and the implementation of a *Corpus* object, a programming construct which contains both the data (a collection of samples and audio descriptor analysis), and a series of methods to operate on that data. Up to this point, *Corpus* objects only held the names of files in their data structure as a way of establishing where a corpus resided on a hard disk. I then added a number of filtering methods which allowed me to sieve the corpus in some blunt ways. These methods were composable, and could be combined in order to carve away samples from the overall corpus and to return a new corpus. [CODE 1]({m.em}#code1), for example, demonstrates filtering a corpus by the loudness of each item so that only the bottom 10% of samples would remain. Any number of those methods can be chained to increasingly filter what is returned by each filter method.

<Code
caption='Corpus filtering by loudness example using the loudness() method of a Corpus() object.'
figure='CODE 1'
id='code1'
>

```py
from ftis.corpus import Corpus
corpus = Corpus('path/to/corpus')
corpus.loudness(max_loudness=10)
```

</Code>

The extension to the *Corpus* object was effective for capturing the quietest and most delicate sounds, although it did not specifically isolate those that were more drone-like as I had intended. I embraced this and combined the materials into a short sketch which can be heard in [AUDIO 7]({m.em}#aud7), playing with these sounds culminated something representing an early draft of a piece which became instrumental as a catalyst for  the creation of [Anchors]({m.em}#anchors).

<Waveform 
file='/inter/short-quiet-idea.mp3'
peaks='/inter/short-quiet-idea.dat'
title='Short draft of a piece composed with sounds derived by corpus filtering'
caption='AUDIO 7'
id='aud7'
/>

Another ad-hoc experiment incorporated the type of approach developed in [Reconstruction Error]({m.re}), by which samples explored aurally in Max were rapidly exported to REAPER to be composed with intuitively. Taking material from clusters such as 1, 2, and 37 I constructed a series of different loop-based palettes as I had been roughly experimenting with in Max. This was more decisive and informed while performing it REAPER, and I aimed to create several iterated structures that were cyclical in nature, but also could have been perceived as relatively static simultaneously. Several of these experiments can be heard in [AUDIO 8]({m.em}#aud8). These experiments were less evocative for me, and I felt like the underlying concept of what I was trying to do was not explored in as much detail as it demanded. That said, it did inform some of the way that material was constructed in [EEE]({m.em}#eee), in terms of which materials I incorporated and how they were structured through intuitive means.

<Album 
tracks={loopExperiments}
id='aud8'
title='Various loop experiments using clusters such as 1, 2, and 37'
figure='AUDIO 8'
/>

#### Mixing Sounds From [Reconstruction Error]({m.re})

These experiments were useful in acquainting me with the broader nature of the corpus and were meaningful as a way of stepping back into a compositional mindset after much technical work had taken place. While I was doing them though, I had a constant feeling that the amount of variation in the corpus was not enough for me to carry forward the initial compositional sketches into something more fully fledged. Specifically, the shorter segments lost a lot of their significance when pulled apart from the original temporal structures with other segments. This was unlike [Reconstruction Error]({m.re}), in which I found a lot of value in dissecting sounds into atomic units and composing with them in that deconstructed state. In response to this I perceived that there was a certain overlap and kinship between the sound world of the two projects and that allowing myself to draw in previously used material might expand the possibilities. This was not easy to do though however, as [FTIS]({m.ftis}) was built with the mindset that a project only has one corpus which was not the case for me.

This frustration led me to again develop and extend the functionality of [FTIS]({m.ftis}) to service these emerging compositional aims. Up till now, a *Corpus* object represented a single folder of samples or a single sample on the computer. Using operator overloading I made it possible to combine corpora programmatically. Using this new functionality I was able to take any number of corpora and treat them as a single corpus, or as I termed it internally a *multi-corpus*. [CODE 2]({m.em}#code2) shows an example of how this is orchestrated as [FTIS]({m.ftis}) code.

<Code 
id='code2'
caption='Adding two FTIS Corpus objects together using operator overloading. the variable multi_corpus is a new corpus as a result of adding corpus_one to corpus_two.'
figure='CODE 2'
>

```py
from ftis.corpus import Corpus
corpus_one = Corpus('path/to/corpus')
corpus_two = Corpus('path/to/corpus')
multi_corpus = corpus_one + corpus_two
```

</Code>

 A number of experiments emerged from this which ultimately 




### Anchors (placeholder)
The connection here is the multicorpus approach.

Developing anchors using a searching process and composing from there. A main source of inspiration is the juxtaposition and superimposition of stationary textures. Many of the initial discoveries were made by looking suitable for convolution, ideally pitched and quasi-textural in nature. There are also numerous notes above that show the development of the source material.

convolution_base_materials.py - take static sounds and create three clusters out of this

convolution_three_anchors.py - took three sounds 
convolution_find_tuned.py - find more pitchy sounds using CQT and kdtree to branch outward from known samples to get 200 more samples.

https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/MultiCorpus/scripts/convolution_three_anchors.py

Sorting these 200 samples by centroid using ReaCoMa

A mid-way version of the piece emerged from intuitively moving these samples around. I tried to create complex 

<ImageMedia2 
url='/inter/anchors-mid-way-session.png'
/>

<Waveform 
file='/inter/28-10-20.mp3'
peaks='/inter/28-10-20.dat'
/>

For this particular track 'anchors', I want the form to almost feel shapeless in the long term. I don't want it to feel like its necessarily pushing toward a climatic point, rather that the music exists in a holding pattern and has ebbs and flows of material within that pattern. This is also part of making the material work musically, I have a quasi-conceptual idea of the sounds being reconstructed into the machines from which they were extracted. Part of frustrations in databending was the fact that the forms felt quite similar because I was focusing on stringing together various meso-form ideas without much consideration for how they would exist as a long term structure. By the time I got to the natural conclusion of each piece's materials in the composition process I had already crystallised a structure that wasn't that malleable.

The Gunter meanders and hovers and restrains from developing intentionally to heighten the suspense.



### EEE (placeholder)
- Take the KNN approach and apply it so other material. Building up a piece around a single sample.
- [isolate_static.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/MultiCorpus/scripts/isolate_sample.py)
- Finding what material from the databending corpus was close to 06-Kindle Off-200513_1547-glued-04.wav
- Focus on disintegrating the kindle sample and finding connections to the disintegrated parts
- 07-200622_1602.wav composing around the morphology

For this particular track 'anchors', I want the form to almost feel shapeless in the long term. I don't want it to feel like its necessarily pushing toward a climatic point, rather that the music exists in a holding pattern and has ebbs and flows of material within that pattern. This is also part of making the material work musically, I have a quasi-conceptual idea of the sounds being reconstructed into the machines from which they were extracted. Part of frustrations in databending was the fact that the forms felt quite similar because I was focusing on stringing together various meso-form ideas without much consideration for how they would exist as a long term structure. By the time I got to the natural conclusion of each piece's materials in the composition process I had already crystallised a structure that wasn't that malleable.

## Reflection

Something as simple programatically as allowing the composability of Corpus objects engendered a whole different mode of interaction with FTIS, and thus lead to pieces based on the exploration of highly specific material types.

Also, delving donw multiple levels of hierarchy. Starting very broadly, what is in the corpus, what is in the group, what is in the sample, how do the samples fit together... This was a new way of hierarchically thinking about structure and form that developed from this increasingly atomic way of dealing with sounds with computer-aid.

Material hierarchies driven by FTIS
    - Cluster exploration in microsegmentation
    - KNN searches
    - The electromagnetic stuff creates a foundation, a base from which I created material links elsewhere and drew into single compositions.

Frustration drives technological development - and technological development drives compositional innovation and breakthroughs. They feed off each other. Anchors only came about because of the multi-corpus approach, which was enabled by extending FTIS. I was thinking about gathering material in a specific way (*branching* out from a single compositional anchor towards similar sounds) which gave rise to the knn method of searching. This sparked the idea for the Kindle active gesture to be used, which I had an appreciation before at the verys tart of listening but didn't know how to incorporate it.

Aestheticise the unheard and procedural sounds of technology. Kindle on/off thing.

- Mouse-2.wav sample becoming a pitched motif.
 - Xiaomi sample in Anchors
- The way that my listening is structured through technology leads me to these kinds of concepts.

- For me, this was a highly successful pilot project, where FTIS was at the basis of my computer-aided compositional practice. The computer was a partner through corpora exploration and structured my engagement with sounds

### Towards a stable workflow

Would have I been able to segment the material at the scale it existed at, analyse it and do the other various processes without the computer?

<NextSection 
next="Technical Implementation and Software"
link={m.ti}
/>
    
<script>
	import NextSection from "$lib/components/NextSection.svelte"
    import {metadata as m} from "../directory.svx"
    import VideoMedia from '$lib/components/VideoMedia.svelte';
    import VideoMedia2 from '$lib/components/VideoMedia2.svelte';
    import ImageMedia2 from '$lib/components/ImageMedia2.svelte';
    import YouTube from '$lib/components/YouTube.svelte';
    import Album from '$lib/components/Album.svelte';
    import Code2 from '$lib/components/Code2.svelte';
    import Waveform from '$lib/components/Waveform.svelte'; 
    import KDTree from '$lib/demos/kdtree/KDTree.svelte';

    import { 
        interAlbum, foundations, rhythmicConstructs,
        eeeSegments, 
        anchors, anchorsSegments,
        kindleModes,xiaomiModes,xboxModes,
        classifier,loopExperiments,baseSketchSegs
    } from '$lib/data/interferences.js';
</script>

# Interferences

<Album
tracks={interAlbum}
title='Interferences'
id='audio1'
figure='AUDIO 1'
/>

Interferences is the fifth and final project in this portfolio. It is a set of two works presented as an EP. There is a git repository which contains all of the Python code and Max patches which were used. This can be found at: https://github.com/jamesb93/interferences

## Motivations and Influences
A primary motivation in this project was to develop the workflow established in [Reconstruction Error]({m.re}), and to build on my creative coding practice with Python, Max, REAPER and [FTIS]({m.ftis}). There were two aspects to this motivation. Firstly, I wanted to recreate the experience of navigating through a corpus of unknown materials led by the computer through *structured listening*. Although I only utilised a small portion of the databending corpus, I wanted to generate a new corpus with its own origins, whilst still situating my work within a digital and synthetic aesthetic. At the time, I was interested in [circuit bending](https://en.wikipedia.org/wiki/Circuit_bending) and the vast number of sounds that breaking apart and reconstructing old electronics could generate. The aesthetic of these sounds suits my sensibilities well, and I imagined that collecting a number of them could be a path forward to creating the corpus that I needed. Tangential to this research, I came across a video of Nicolas Collins using an induction microphone to amplify the sounds of electromagnetic interferences generated by everyday appliances and objects. This video can be seen in [VIDEO 1]({m.em}#vid1). It struck me that instead of circuit bending, this might be a suitable technique for creating a large corpus with the properties I desired. Following this hunch, I obtained an [Elektrosluch Mini induction microphone](https://store.lom.audio/products/elektrosluch-mini-city-diy-kit-1?variant=4542176460832) capable of recording electromagnetic waves. I describe how this device was instrumental in the compositional process further in ["Recording Objects With Transduction"]({m.em}#recording-objects-with-transduction).

<YouTube 
url="https://www.youtube.com/embed/4T7qkYY7LZM"
caption='Nicolas Collins demonstrating induction to record electromagnetic fields generated by everyday electrical appliances.'
figure='VIDEO 1'
id='vid1'
/>

<!-- REVIEW -->
Secondly, I wanted to iterate on the pipeline of segmentation, analysis, dimension reduction and clustering and apply it more fluidly to new compositional materials. The technology implemented in [Reconstruction Error]({m.re}) was successful for enabling me to be creative, but was largely inflexbile and to perform the same tasks on new material would require rewriting the scripts. Thus, [FTIS]({m.ftis}) supported this goal, and made it possible create  scripts that supported experimentation and data manipulation. This aspect is discussed in more detail in ["Pathways Through The Corpus"]({m.em}#pathways-through-the-corpus).

## Composition Process
The composition process can roughly be divided into two phases. The first phase involved experimentation with [FTIS]({m.ftis}) and devising various ways to navigate through the corpus with the aid of the computer.  The second phase involved a more direct level of composition highlighting the two EP pieces which emerged in response to the phase one experiments. In this second phase, I used the computer to search and hone in on specific materials in the electromagnetic sounds corpus. This contrasts to the broader exploration undertaken in the first phase.

### Recording Objects With Induction
As outlined in ["Motivations And Influences"]({m.em}#motivations-and-influences), I procured an electromagnetic induction microphone and wanted to use it to generate a corpus of digital and synthetic type sounds. To do this, I took electronic objects from around the home and recorded them, at times operating those objects simultaneously. I was surprised by the diversity of sounds this produced. Despite devices in my environment seeming dormant, the induction microphone could detect and uncover their invisible electronic mechanisms and operation. I recorded a number of devices, including my computer keyboard, mouse, a mobile phone, a sound card, a laptop and an e-reader. Each device produced its own characteristic outputs and some of these objects offered me the ability to trigger certain sonic behaviours. For example, [AUDIO 2]({m.em}#aud2) captures the sound of an e-reader left untouched. The change at 0:02 is triggered by switching the "aeroplane mode" on and off. This user interaction causes the components in the e-reader to operate differently, which is captured by the induction microphone. Eventually the initial static state is returned to. This can be heard at 1:59 lasting until the end of the clip.

<Waveform 
file='/inter/kindle-gesture.mp3'
peaks='/inter/kindle-gesture.dat'
id='aud2'
title='Induction recording of e-reader while switching the aeroplane mode on and off'
caption='AUDIO 2'
segments={kindleModes}
/>

Other objects produced their own characteristic material although I found that across all of the objects I had listened to, my interpretation would always distill toward a distinction between *active* and *passive* musical states.  For example, [AUDIO 3]({m.em}#aud3) is extracted from the electromagnetic interferences of a mobile phone. In my perception, there are two sonic classes at work here, where 0:00 to 0:40 is *static*, and the remainder of the recording is *active*.

<Waveform
file='/inter/xiaomi-gesture.mp3'
peaks='/inter/xiaomi-gesture.dat'
id='aud3'
title='Induction recording of mobile phone without user interaction'
caption='AUDIO 3'
segments={xiaomiModes}
/>

As I recorded and browsed through these sounds, I was reminded of the work of Australian field-recording artist Jay-Dea Lopez and the way he taps into similar sources using contact microphones attached to monolithic infrastructural objects in the outback. In particular, three blog posts of his, ["Low Frequencies"](https://soundslikenoise.org/2019/09/07/low-frequencies/), ["Coil pickups and microsounds"](https://soundslikenoise.org/2020/08/16/coil-pickups-and-microsounds) and ["Raising the Inaudible to the Surface"](https://soundslikenoise.org/2014/08/31/listening-to-the-inaudible-field-recording-and-the-pursuit-of-the-microsound/) contain these types of material. Similarly, I felt a certain connection between my experiments and the morphologies and textures present in Bernhard Gunter's *Untitled I/92* ([VIDEO 2]({m.em}#vid2)), and Alvin Lucier's *Sferics* ([VIDEO 3]({m.em}#vid3)). To me, both of these pieces shared some of the characteristics of what I was discovering with the electromagnetic microphone — delicate and fragile sound worlds full of intimate clicks, pops and spectrally constrained sonic fragments. Furthermore, these external influences started to funnel into my conceptualisation of the initial material as that which can occupy two distinct states — busy and rapidly changing or unchanging and glacial in nature. This became an essential starting point in the technological process of using machine listening to navigate through this corpus of sounds framed by the opposition of *static* and *active* states.

<YouTube 
url="https://www.youtube.com/embed/QZfMp6tredw"
caption="Untitled I/92  from Un Peu De Neige Salie by Bernhard Gunter"
figure='VIDEO 2'
id='vid2'
/>

<YouTube 
url="https://www.youtube.com/embed/rxUvMl_IxoQ"
caption="Sferics by Alvin Lucier"
figure="VIDEO 3"
id="vid3"
/>

### Static And Active Material
I began by trying to dissect the corpus into two groups based on classifying whether they were *static* or *active*. To do this, I first had to segment the corpus, and then find a way of crudely classifying those segments into either of these two classification labels.

#### Segmentation
The end goal for segmentation in this project was to automatically divide each sound into *static* and *active* components. I began first by splitting into rough homogenous regions of spectral activity. I planned to apply a classification or clustering analysis to the segments made by this process so that the computer could discern which group each segment might belong to. There were complicated aspects to this though as the time span of static and active states to my perception varied a lot depending on the file. [AUDIO 2]({m.em}#aud2) only shows a portion of what was created from recording an e-reader, and the static and active states last for up to minutes between alternations. [AUDIO 4]({m.em}#aud4) shows another recording taken from a wireless game controller, which demonstrates similar oscillations between *static* and *active* states, although these changes occur on over seconds rather than minutes. Given these disparities in time scale, a frame-by-frame analysis was likely to be unsuitable and hard to tune appropriately to capture the sparse and specific moments in time in longer samples, while also being sensitive to the shorter-term changes.

<Waveform 
file='/inter/xbox-gesture.mp3'
peaks='/inter/xbox-gesture.dat'
caption='AUDIO 4'
title='Shorter induction recording taken from a game controller.'
id='aud4'
segments={xboxModes}
/>

To tackle this, I created a bespoke algorithm influenced by Lefèvre, S., & Vincent, N. (2011) which performs segmentation by sampling short sequences within a sound and clustering those sequences together. The algorithm is structured as a two phase process. The first phase, *over-segments* a sample into numerous small divisions. These divisions are not necessarily perceptually meaningful, rather, the process aims to divide the signal excessively so that these small divisions can be recombined later in the second phase. The second phase applies audio descriptor analysis to each segment and then groups each one into a user-specified number of clusters. This *number of clusters* parameter could determine how specific the algorithm could be in classifying the segments. From this point, the algorithm recursively operates on a windows of clustered segments and merges contiguous segments with the same cluster label. This window is also configurable. It starts at the first segment in time and performs this by shifting forward in time, merging inside the loop. As such, it gradually creates larger and larger combined segments until consecutive segments all belong to different clusters. [IMAGE 1]({m.em}#img1) is a visual representation of this algorithm.

<ImageMedia2 
url='/inter/clusterseg-explanation.svg'
caption='Visual depiction of "cluster segmentation" algorithm'
figure='IMAGE 1'
id='img1'
/>

My intention with this was to create a generalisable algorithm that would be relatively agnostic toward the nature of the material it was segmenting. I also wanted to avoid some the issues whereby  frame-by-frame approaches often only target short windows of time and do not respect the longer term implications of changes in the signal. By using a shifting window that merges while it slides across the segments, it gives the algorithm a "lookahead" and "look behind" that constantly updates as it progresses. This aspect is configurable

I evaluated the effectiveness of the algorithm by modifying each component of this two-phase segmentation algorithm and observing the results as a REAPER session. Overall, I found that the choice of segmentation algorithm to over-segment had little effect on the final result. What had the most significant impact in this aspect was the initial granularity of the segments which the clustering was applied to. If the segments were too large the algorithm would often deem contiguous segments to be similar as the data in each segment regressed to the mean. Thus, it was important to begin a high level of granularity and differentiation between segments to avoid this. The size of the sliding window and the number of classification clusters had a bigger effect on the results. In preliminary tests I found that they could diverge significantly in unexpected ways. Unlike changing a threshold for an amplitude-based slicer for example, which as it is lowered one expects to see more clusters emerge, the combination of window size and number of clusters was difficult to predict. [IMAGE 2]({m.em}#img1) depicts the REAPER session I used to inspect the results, where each track is the result of a different combination of these two parameters. The session itself can also be found in the repository here: [session.rpp](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation_scripts/2020-06-18%2004-09-54.718681/session.rpp) and the code that performed the segmentation can be found here: [clustered_segmentation.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation_scripts/clustered_segmentation.py).

<ImageMedia2 
url='/inter/clusterd-segmentation.jpg'.
figure='IMAGE 2'
caption='Clustered segmentation results rendered as a REAPER session.'
id='img2'
/>

Testing different parameters produced a variety of results, both clearly unsatisfactory from over-segmentation while others were borderline and harder to evaluate. Between different parameter settings, slice points I would deem as straightforward and easy to discern changes in texture and morphology had been detected, but rarely did one configuration agree on the same results. Given this disparity it was difficult to determine if there a clear "best" configuration. Moving forward I turned this algorithm into a [FTIS]({m.ftis}) analyser so that I could integrate it with the technical exploration which followed this.

I then ran the algorithm on all of the induction recordings using this script: [segmentation.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/segmentation.py). The results from this were satisfactory to start, but not necessarily perfect. This was not an issue though as I planned to do further segmentation later to further separate the material. At this point it only had to be a general division of the sounds to facilitate clustering them into *static* and *active* categories.

#### Classification Through Clustering
I then classified each of these newly generated segments into two distinct groups by calculating the statistics of spectral flux across windowed frames in each sample, and then clustering each sample according to this data using the Agglomerative Clustering algorithm. The code for this can be found here: [split_by_activity.py](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/split_by_activity.py). The results were acceptable for dividing sounds into static and active groups. A selection of different results from this process are presented in [AUDIO 5]({m.em}#aud5).

<Album 
tracks={classifier}
id='aud5'
title='Several classification results.'
figure='AUDIO 5' 
/>

### Pathways Through The Corpus
Once I had classified the segmented sound files I wanted to begin operating on this material as two distinct compositional materials through further segmentation, clustering and audition. This section describes different *pathways* through the corpus of segmented and classified sounds I created, and experiments which aided in formulating concepts and ideas for the two pieces through audition and structured listening.

I wanted to begin by further deconstructing sounds classified into the *active* label into individual impulses, utterances segments. For example, I imagined that [AUDIO 6]({m.em}#aud6), a segment from *Kindle_04_08.wav*, might be restructured to be less chaotic and more ordered — or perhaps a rhythmic structure could be imposed on to individual segments extracted from this sound.

<Waveform 
file='/inter/kindle-04-8-seg.mp3'
peaks='/inter/kindle-04-8-seg.dat'
id='aud6'
figure='AUDIO 6'
title='Kindle_04_08.wav segment'
/>

Following this intuitive desire, I used [FTIS]({m.ftis}) to segment the *active* sounds and then clustered these segments based on their perceptual similarity. This was orchestrated with two separate [FTIS]({m.ftis}) scripts. This [segmentation script](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/micro_segmentation.py) uses the clustered segmentation algorithm to create granular slices and is followed by this [clustering script](https://github.com/jamesb93/interferences/blob/0773d279b98925cfa7a8b4008ab901d19af5c45b/micro_clustering.py) hich calculates MFCCs for each segment and clusters them using the HDBSCAN algorithm. I explored the results aurally in Max and found that this process rendered perceptually homogenous clusters in terms of texture and morphology. [VIDEO 4]({m.em}#vid4) captures this audition process and demonstrates the perceptual homogeneity.

<VideoMedia2 
url='/inter/exploring-clusters.webm'
caption='Exploring segmented and clustered active material'
figure='VIDEO 4'
id='vid4'
/>

This Max patch looped a selected sample from a cluster. As I navigated between clusters and samples within them, the intricacy and fragility of the sounds within specific clusters was foregrounded. Curiously too, some material that I would have perceived as static had been included in what was the *active* classification performed earlier. The nature of the looping playback, combined with already mechanised and artificial nature of these sounds was evocative and I was drawn to different groups — imagining composing with the segments to form my own artificial machine-like sound composites. Clusters 1, 2, and 37, for example, produced structured rhythmic sounds articulated by sparse and subtle transients. When I was changing the sample the rhythmic patterns of these articulations could be altered. I imagined that combining several of these could be used to build layers of  intricate quasi-rhythmic patterns. Clusters 0, 11 and 18 contained a large amount of drone-like static material with a prominent pitch content. To an extent, I was thankful this material had been poorly grouped into the *active* classification causing it to be discovered by me in this process. Looping short segments of this material highlighted how it could be transformed from its originally glacial and static form into a more musically driven behaviour.

For me this this was a revelatory listening experience in the compositional process. I imagined those sounds with more rhythmic and iterated qualities could form a piece in this EP, while those with drone-like qualities might form another. This elicited my further utilisation of [FTIS]({m.ftis}) to hone my exploration and deconstruction in terms of these two compositional aims. This next next phase of exploration and computer-aided composition did not directly lead to the creation of the two works in the EP, rather, I would characterise it as a series of experiments that functioned as stepping stones in formulating my compositional intent.

#### Ad-Hoc Experiments

One such experiment was to run similar clustering processes on a subset of the corpus, derived by filtering each item according to its average loudness. The aim of this was to hone in on quiet, drone-like sounds and to curate the corpus towards only those sounds and away the active rhythmic sounds. This was facilitated by extending [FTIS]({m.ftis}) and the implementation of a *Corpus* object, a programming construct which contains both the data (a collection of samples and audio descriptor analysis), and a series of methods to operate on that data. Up to this point, *Corpus* objects only held the names of files in their data structure as a way of establishing where a corpus resided on a hard disk. I then added a number of filtering methods which allowed me to sieve the list of files contained in the corpus. These methods were composable, and could be combined in order to carve away samples from the overall corpus and to return a new corpus. [CODE 1]({m.em}#code1), for example, demonstrates filtering a corpus by the loudness of each item so that only the quietest 10% of samples would remain. Any number of those methods can be chained to increasingly filter what is returned by each filter method.

<Code2
caption='Corpus filtering by loudness example using the loudness() method of a Corpus() object.'
figure='CODE 1'
id='code1'
>

```py
from ftis.corpus import Corpus
corpus = Corpus('path/to/corpus')
corpus.loudness(max_loudness=10)
```

</Code2>

The extension to the *Corpus* object was effective for filtering toward the quietest and most delicate sounds in a collection, although it did not specifically isolate those that were more drone-like as I had intended. I embraced this, and combined the materials into a short sketch which can be heard in [AUDIO 7]({m.em}#aud7). Reflecting on this sketch, there are noticeable sonic connections to [{m.anchors}]({m.em}#manchors) in terms of the type of material and the focus on static sounds.

<Waveform 
file='/inter/short-quiet-idea.mp3'
peaks='/inter/short-quiet-idea.dat'
title='Short draft of a piece composed with sounds derived by corpus filtering'
caption='AUDIO 7'
id='aud7'
/>

Another ad-hoc experiment incorporated cluster-based exploring technique by which samples explored aurally in Max are rapidly exported to REAPER to be composed with intuitively. Using samples from clusters such as 1, 2, and 37 I constructed a series of different loop-based palettes manually in the DAW timeline. I aimed to create several iterated structures that were cyclical in nature, but also could have been perceived as relatively static at the same time. 

Several of these experiments can be heard in [AUDIO 8]({m.em}#aud8). These experiments were less evocative for me, and I felt like the underlying concept of what I was trying to do was not explored in as much detail as it demanded. That said, it did inform some of the way that material was constructed in [{m.eee}]({m.em}#{m.eee}), in terms of how sounds were deconstructed and restructured through intuitive compositional decision making.

<Album 
tracks={loopExperiments}
id='aud8'
title='Various loop experiments using clusters such as 1, 2, and 37'
figure='AUDIO 8'
/>

#### Mixing Sounds From [Reconstruction Error]({m.re})
These experiments were useful in acquainting me with the broader nature of the corpus and were meaningful as a way of stepping back into a compositional mindset after much technical work had taken place. While I was performing them though, I had a constant impression that the amount of variation in the corpus was not enough for me to carry forward initial compositional sketches into something more fully fledged as a piece. Specifically, the more I worked with short segments, the more they seemed to lose their significance when pulled apart from the original temporal structures. This was unlike [Reconstruction Error]({m.re}), in which I found a lot of value in dissecting sounds into atomic units and composing with them in that deconstructed state. 

In response to this I estimated that *combining* the [Reconstruction Error]({m.re}) databending corpus and this electromagnetic induction recording one might expand the possibilities for working with small segments of sounds while maintaining a sense of coherency in the sonic identity of the project. Technically though, this was not simple to do at this point, as [FTIS]({m.ftis}) was built in such a way that a corpus represents files container inside a single folder. I did not want to "pollute" each corpus by copying them around the file system or create copies which would be operated on differently. My ideal interface would instead to point [FTIS]({m.ftis}) to the location of several different corpora and be able to programmatically combine them at the script level.

This catalysed another new development to the [FTIS]({m.ftis}) *Corpus* object to service these emerging compositional aims. Up till now, a *Corpus* represented a single folder of samples or a single sample on the computer. Using [operator overloading](https://docs.python.org/3/reference/datamodel.html#special-method-names), I made it possible to combine corpora programmatically. This functionality made it possible for me to combine multiple corpora and treat them as a single corpus, or as I termed it internally a *multi-corpus*. [CODE 2]({m.em}#code2) shows an example of how this is orchestrated as [FTIS]({m.ftis}) code.

<Code2
id='code2'
caption='Adding two FTIS Corpus objects together using operator overloading. the variable multi_corpus is a new corpus as a result of adding corpus_one to corpus_two.'
figure='CODE 2'
>

```py
from ftis.corpus import Corpus
corpus_one = Corpus('path/to/corpus')
corpus_two = Corpus('path/to/corpus')
multi_corpus = corpus_one + corpus_two
```

</Code2>

While the notion of combining multiple sources of sonic materials is not novel to composition with digital samples, for me this change allowed me to use [FTIS]({m.ftis}) more flexibly  to overcome the mental pattern of one project pertainting to a single corpora. Instead of having to merge files from a number of different directories on disk to form a corpus, I could instead point a script to a number of places and deal with this in a fashion where the organisational house-keeping is separated from the creative workflow. This provoked another phase of exploration with FTIS that ultimately generated the two works presented as [Interferences]({m.em}). The next two sections, [{m.anchors}]({m.em}#manchors) and [{m.eee}]){m.em}#{m.eee}) discuss how these works emerged from this point onward.

### {m.anchors}
<Waveform 
file='/pieces/inter/anchors.mp3'
peaks='/pieces/inter/anchors.dat'
id=''
title={ m.anchors }
caption=''
/>

Developing the *multi-corpus* capabilities of [FTIS]({m.ftis}) prompted me to think how I might re-approach composing with the drone-like material I encountered in the earliest cluster auditioning process described in ["Pathways Through The Corpus"]({m.em}#pathways-through-the-corpus) and ["Ad-Hoc Experiments"]({m.em}#ad-hoc-experiments.)

The first step I took in addressing this was to investigate in more detail what level of variation existed between these *static* sounds. To facilitate this I created a [FTIS]({m.ftis}) script to take all of the samples initially classed as *static* (see ["Static And Active Material"]({m.em}#static-and-active-material) as a reminder), and to create three clusters from this. The number of clusters was an intuitive choice, and I imagined that by dividing *static* material into three rough groups I might be able to discern if there were any significant differences within this sub-section of the corpus. From this, I generated a REAPER session and auditioned the outputs. This script can be found here: [base_materials.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/base_materials.py).

Combing through this REAPER session confirmed my instinctive impression that there was not much variation in the static material. While three clusters were produced by the clustering process, the sounds grouped into clusters 1 and 2 possessed almost identical morphologies and textural qualities. The differences between clusters 1 and 2 compared to cluster 0 though were more perceptually obvious. I played with these differences and created a sketch based on hard panning clusters 1 and 2 to left and right channels respectively, and situating cluster 0 centrally. I arranged the material in order to explore different juxtapositions of these two sonic identities. This sketch can be heard in [AUDIO 9]({m.em}#aud9) alongside some annotations of my perception of different combinations between the clusters.

<Waveform 
id='aud9'
title='A sketch created from the output of base_materials.py'
caption='AUDIO 9'
file='/inter/base-materials-sketch.mp3'
peaks='/inter/base-materials-sketch.dat'
segments={baseSketchSegs}
/>

Following the composition of this sketch, I aimed to discover sounds which could be used for creating detailed superimpositions with sounds found in [AUDIO 9]({m.em}#aud9), as well as expand the palette of possible combinations between sounds. By utilising the *multi-corpus* techniques in [FTIS]({m.ftis}) further, I produced a two dimensional map, containing samples from both the [Reconstruction Error]({m.re}) corpus *and* the *static* materials I was already working with. This script can be found here: [find_tuned.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/find_tuned.py). This spatial representation was computed by reducing the dimensions of a Constant-Q Transform (CQT) analysis for each sample to two dimensions. I selected this audio descriptor based on the assumption that its logarithmic representation of the spectrum would be suitable for capturing differences and similarities in pitch as well as fine-grained spectral complexity in the high frequencies. From this spatial representation I computed a [k-d tree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) which would enable me to query for a number of neighbours to a specific point in that space. Using this data structure, I selected a sample ([AUDIO 10]({m.em}#aud10)) from the sketch heard in [AUDIO 9]({m.em}#aud9) with a prominent pitch component and queried for 200 samples closest to that sample in the spatial representation.

<KDTree />

<Waveform 
id='aud10'
title='Central query sample for find_tuned.py'
caption='AUDIO 10'
file='/inter/find-tuned-centre.mp3'
peaks='/inter/find-tuned-centre.dat'
/>

I auditioned the sounds curated from the [find_tuned.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/find_tuned.py) script and found that in the 200 samples there were many cohesive compositional materials. Many had a strong pitched component, while others were more similar texturally and were not similar in terms of the presence of a pitch. A short video capturing this listening process can be seen in [VIDEO 5]({m.em}#vid5)}. I imported all of these files into REAPER and created a longer-form sketch with them which can be heard in [AUDIO 11]({m.em}#aud11). 

<VideoMedia2 
url='/inter/exploring-tuned-anchors.webm'
caption='Auditioning the output of find_tuned.py'
figure='VIDEO 5'
id='vid5'
/>

<Waveform 
file='/inter/28-10-20.mp3'
peaks='/inter/28-10-20.dat'
title='Sketch using a combination of sounds from initial sketch (AUDIO 9) and from find_tuned.py output.'
caption='AUDIO 11'
id='aud11'
/>

These two sketches marked the beginning of a clearer idea for the final work. Throughout the computer-aided searching and the compositional process three particular samples persistently emerged, presenting characteristic textural qualities. I termed these three sounds "anchors". These three samples can be heard in [AUDIO 12]({m.em}#aud12).

<Album 
tracks={anchors}
title='Three anchor sounds'
caption='AUDIO 12'
id='aud12'
/>

I created a two dimensional spatial representation of a combined corpus containing [Reconstruction Error]({m.re}) samples, and the *static* samples which I had been sketching with. This script can be found here: [three_anchors.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/three_anchors.py). I then constructed a k-d tree from this representation and *branched* out from each of the three anchors in an attempt to structure similar *static* material within the frame of these three key sounds. This returned the same samples that [find_tuned.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/find_tuned.py) did roughly, but separated the results into three perceptually orientated groups, with each of the three anchors acting as templates, or markers for what those groups should contain.

My compositional decision making was largely a response to the division of material into these distinct groups. I focused on blending the groups in different ways to create composite textures with a distinct musical behaviours. Such combinations were based on forming chordal sounds from the summation of pitched material or layering dynamically striated and texturally "bumpy" sounds with those that were more static and inactive. This concept informed the high-level structure of the work, which is underpinned by a drone comprised of material belonging to a single one of the three groups. Longer form developments are orchestrated as interactions with this layer either emphasising and adding to it or assuming the role of a sonic antagonist. 

<!-- REVIEW: potential album of interactions here annotated -->

There is only one intentionally constructed dramatic moment that occurs at 6:20, while the rest of this piece is purposefully crafted to be less directed and non-narrative in nature. This emerged from a contrasting mindset to that which I embodied in [Reconstruction Error]({m.re}). Comparatively, I had much less material to work with in [Reconstruction Error]({m.re}) and so often I had to extract as much musical value from the samples I had found within perceptually homogenous clusters. This pushed me toward recycling the material as much as possible and composing several meso-scale structures which I then blended and arranged intuitively. By the time I had utilised those meso-structures as much as I felt I could, the form of a piece had crystallised and was less flexible or adaptable.

By having access to much more material in [{m.anchors}][{m.em}#manchors], and several different computational structures representing those materials, the conceptual goals and frame of the work was fostered by much more flexible forms of interaction with the computer. In reality, the final stage of the compositional process following on from the sketches was entrenched in a feeling that I was "firming up" the conceptual ideas of the piece and developing them iteratively. It also engendered an integration between my conceptual goal and realising that with the aid of the computer. I felt as if I did not have to compromise on the conceptual aims of the piece, rather something was catalysed and mediated by myself and the computer.

### {m.eee}
<Waveform 
file='/pieces/inter/eee.mp3'
peaks='/pieces/inter/eee.dat'
id='aud13'
title={m.eee}
caption='AUDIO 13'
segments={eeeSegments}
/>

This second piece of [Interferences]({m.em}) grew in response to the development of the [FTIS]({m.ftis}) *Corpus* object while creating {m.anchors}, and my desire to revisit some of the raw materials recorded with the Elektroslusch mini. While I was gathering these initial recordings, I had a strong aesthetic response to the recording of the e-reader. I wanted to return to this sound, and deconstruct it with the tools which at the start of this project were not yet developed or as sophisticated as they had become. The most striking aspect of this e-reader recording, in my perception, was the clarity and definition between *active* and *static* states as well as how the entire sample was structured around these two distinctive musical behaviours. The *static* component possessed a strong pitch element and was completely unwavering in dynamic, while the *active* was more gestural and had a varied morphology. A small segment of the longer recording demonstrates the antiphonal character of these two musical behaviours in [AUDIO 14]({m.em}#aud14). 

<Waveform 
title='E-reader sample section displaying active and static states (06-Kindle Off-200513_1547.wav)'
file='/inter/eee-seed.mp3'
peaks='/inter/eee-seed.dat'
id='aud14'
caption='AUDIO 14'
points={ [ { time: 18.3, editable: false, labelText: 'Interjection'} ] }
segments= {[
    { startTime: 0.0, endTime: 18.3, labelText: 'Static' },
    { startTime: 18.3, endTime: 136.0, labelText: 'Active' },
]}
/>

I wanted to separate these two behaviours and deconstruct their relationship — incorporating it as a formal aspect of this piece as well as allowing it inform my treatment and arrangement of compositional materials. [FTIS]({m.ftis}) was instrumental in evolving this concept and facilitated the necessary segmentation and deconstruction processes.

#### Deconstruction Of Active And Static States

Broadly speaking, the *active* and *static* states were treated differently and with a unique approach in mind. The static state is treated as a "base-layer" which is ornamented with other samples, while the active state is deconstructed in order to recycle that material and proliferate it in time. 

The first three and a half minutes of the piece are based around extending the antiphonal nature of the source material. To do this, I began by manually segmenting the unprocessed sample (06-Kindle Off-200513_1547.wav) in to regions of *active* and *static* behaviour. This way, I had access to those two different behaviours in isolation. I created various configurations in which *static* texture is interjected with the initial gesture found in the *active* state. Using [ReaCoMa]({m.reacoma}) to segment the *active* materials allowed me to create slight variations by reordering short granular segments and by removing and cutting this musical phrase in parts. The *static* material is mostly left untouched.

At 3:41, I refrain from introducing any new interjections and started developing the *static* musical behaviour. I used the k-d approach (the same one described and employed in {m.anchors}) to have the computer return perceptually similar sounds from the combined corpus containing[Reconstruction Error]({m.re}) and [Interferences]({m.em}) samples. The code that performed this can be found here: [isolate_sample.py](https://github.com/jamesb93/interferences/blob/c22b03b17930988fadfa792be28ec369d6584157/MultiCorpus/scripts/isolate_sample.py). Several complementary sounds were derived from this which I then incrementally superimposed onto the static texture. Some of the sounds returned were very short so I juxtaposed them contiguously in order to create textural material. These blended subtly, and merge with the the *static* material. These are progressively layered until 6:48 which begins a new section.

At 6:48 the texturally dense musical behaviour is interrupted by the return of a gesture extracted from the *active* material. This demarcates the beginning of a new section in which I deconstructed and reconstructed this gesture in a number of ways to temporally extend and iterate it. In order to achieve this, I used [ReaCoMa]({m.reacoma}) to first create short granular segments which I then rearrange into prolonged concatenated sequences. To avoid a sense of strong repetition I then used a Lua script to shuffle the order of those contiguous items randomly. [VIDEO 6]({m.em}#vid6) demonstrates how this is performed in practice.

<VideoMedia2 
url='/inter/reorganising-material.webm'
caption='Using ReaCoMa to segment and extend the active gesture '
figure='VIDEO 6'
id='vid6'
/>

Similarly, I processed the same granular segments using one of the [ReaCoMa]({m.reacoma}) "sorting" scripts, which takes a group of selected media items and arranges them in the timeline view according to audio descriptor values. I used this to sort the segments according to perceptual loudness. The outcome of this tends toward grouping segments with similar dynamic profiles, resulting in a glitch-like and artificial sonic outcome. This can be heard in [AUDIO 15]({m.em}#aud15). This particular type of gesture can be heard at 7:22, 7:36, 8:03 and 10:46.

<Waveform 
id='aud15'
title='ReaCoMa sorting used on small segments of active gesture.'
caption='AUDIO 15'
peaks='/inter/sorter-reacoma.dat'
file='/inter/sorter-reacoma.mp3'
/>

In the final section beginning at 8:32, the separation of *static* and *active* states becomes less discrete and the musical behaviours are merged and blended together. Rhythmicity is central to this seciton, as well as the sense that the structure of this is loose and off the grid. As such, this section is structured around carefully placing whole samples which themselves are inherently rhythmic or iterated, as well as my synthesis of such musical behaviours through segmentation and intuitive arrangement. For example, I returned to some of the original data produced by the process produced in earlier [ad-hoc experiments]({m.em}#ad-hoc-experiments). Several of the groups within this clustering output contained samples that were borderline between *active* and *static* states. They were relatively stationary in nature, but when observed over shorter time scales presented localised repeating and looping changes in texture and dynamics. These samples were drawn into this section as foundational layers upon which other rhythmic structures are composed. These can be heard in isolation in [AUDIO 16]({m.em}#aud16).

<Album 
title='Foundational layers'
figure='AUDIO 16'
tracks={foundations}
id='aud16'
/>

These foundational layers were left mostly untreated, while I performed surgical segmentation and rearrangement of short granular materials extracted from the *active* state in order to create my own synthetic rhythmic passages. The segmentation was again executed with [ReaCoMa]({m.reacoma}), and the organisation of the results were structured using the same Lua script described in [this section of Reconstruction Error](http://localhost:3001/projects/reconstruction-error#vid12). Two examples of this can be heard in [AUDIO 17]({m.em}#aud17).

<Album 
title='Intuitively composed rhythmic constructs.'
figure='AUDIO 17'
tracks={rhythmicConstructs}
id='aud17'
/>

## Reflection
Interferences was a highly successful project in terms of consolidating my computer-aided workflow towards a stable set of tools and creative coding interfaces. It was the first project where I was able to use [FTIS]({m.ftis}) and although I anticipated additional friction from having to support my a complex piece of bespoke software, this created a fruitful dialogue between myself and the computer. Compositional blockages and frustrations drove technological development, and those technological developments catalysed further compositional innovation and breakthroughs. {m.anchors}, for example, was significantly influenced improvements to the *Corpus* object, allowing me to seamlessly draw together multiple corpora together and to be treated as a single collection. This development itself was prompted by my feeling that there was not enough variety in the induction recordings and so I needed to create mechanisms for drawing together other sources in my computer-aided sample searching. Furthermore, working with the computer in this project was more often situated in *soundful* forms of exploration and experimentation. This can be attributed to the fact that [FTIS]({m.fits}) enabled me to fluidly move between REAPER, Max and scripting different machine learning and content-aware programs.

At the time, I was unable to foresee the ramifications that this would have musically on the EP as a whole. Enabling me to fluidly incorporate separate corpora into my creative coding, increased the variety of sounds in those processes while not disrupting the interaction and flow with [FTIS]({m.ftis}) through scripting. This was important to me in not feeling like I was diverging in the compositional process, but rather just *exploring more deeply* in an authentic way and without having to manage technical concerns once the implementation was complete. Combining different corpora sources at times improved the results of processes in which I aimed to find specific material morphological and textural material through matching. This was pivotal in {m.eee} for example, as the limited subset of materials contained in the *active* and *static* states were only proliferated by my finding appropriately similar and connected samples to compose with. Without this access to similarity, there would not have been enough novel material to compose the longer form of this work.

In addition to this, drawing together several sound sources into one computational object created aesthetic bridges between the unique qualities, behaviours and characteristics of samples from each source. For example, at the beginning of composing {m.anchors} , I aimed to use a constrained set of morphologically inactive and stationary sounds only from the induction recordings. Using the k-d tree to *branch* out through a spatial representation of samples within both corpora based on the CQT analysis, was like observing the connections between one corpus in another. Instead of conceiving of the two corpora as separate entities with their own history and conceptual backdrops, they became unified as a collection and this challenged the boundaries of the project. The piece became much more texturally variegated from this, and the tension between invariant static sounds, and lively dynamic sounds became a central compositional feature. As a result, the computer shaped the piece at its conceptual and aesthetic core.

### Formal Repercussions
In ["Time"]({m.preoc}#time), I describe my compositional approach and thinking toward form as the hierarchical superimposition of increasingly granular blocks of material. Treating form in this way is central to my engagement with the compositional process and the way that I build pieces from atomic building blocks such as audio segments and samples. [FTIS]({m.ftis}) and [ReaCoMa]({m.reacoma}), allowed me to fully explore this through the several methodologies of destructuring corpora and corpora items into perceptually relevant taxonomies and groupings. This fostered a workflow where the compositional process was led by engaging with gradually increasing detailed levels of structure in the source sounds. As a result, my compositional process reflected the level which I was currently engaging with at the time. For example, both pieces began with high-level interests in the structure of particular sounds. In {m.anchors}, this was situated in the conception of three material groups, derived from conceiving as the whole corpus being neatly divided into *static* and *active* archetypes. By deconstructing those groups into three key clusters and discovering connections between them and other sounds within the corpus, the identity of the piece emerged and crystallised. {m.eee} also developed as a direct result of deconstructive processes as the entire piece is predicated on the pulling apart of a single sample. This was performed in a similar way, at first dividing it into the *static* and *active* behaviours and then proliferating material from that point using the computer to aid intuitive compositional decision making.

Overall, deconstructing sounds and restructuring their constituent parts in response to the perceptually guided outputs of [FTIS]({m.ftis}) and [ReaCoMa]({m.reacoma}) led both my low and high-level compositional decision making. The computer became an extension my hierarchical thinking and was coupled to my compositional action in an inseparable manner. For me this was the most importing realisation that I made through reflection on this work piece, in that I had developed a combination of technologies which closely aligned with fundamental and core models which I conceive sound by. At the start of the other projects in this thesis, I have often shifted to a new approach and endeavoured to find a workflow that would leave the problems of the previous project behind. I did not have this experience from this project, and now only want to extend and improve these existing tools to continue composing in a similar way. 

The next section outlines the technical implementation for the software outputs included in this PhD thesis.

<NextSection 
next="Technical Implementation and Software"
link={m.ti}
/>